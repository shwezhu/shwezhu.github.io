[{"content":" # 1. Computers only know binary code Two ways to convert human-readable code to binary code:\nCompilation: The code is converted to binary code before it is executed. Interpretation: The code is converted to binary code as it is executed. # Why a compiled binary code cannot be run on a different OS? # ISA (Instruction Set Architecture) ISA: The CPU has a set of predefined actions called the instruction set or ISA (instruction set architecture). source\nPopular ISAs:\nx86: Intel and AMD processors, high performance. Not the case in nowadays, ARM is also used in high-performance devices. One reason os that some powerful GPUs, such as NVIDIA works better with x86 processors. ARM: Often used in the devices with small battery, such as smartphones, embedded systems, such as snapdragon, and Apple\u0026rsquo;s A-series chips, and Raspberry Pi, STM32. M-series chips. MIPS: used in switches and routers, and some embedded systems. # OS (Operating System) Different OSs have different system calls. We use standard libraries to make system calls, such as printf in C.\n# Cross-compilation Cross-compilation is the process of compiling code for a platform different from the one on which the compiler is running.\n# Why cross-compilation? Some machines are not powerful enough to compile the code. For example, compiling code for a micro-controller on a Raspberry Pi.\n# How to cross-compile? # Cross platform Java - Write once, run anywhere. Benefits from the JVM.\nPython - Benefits from the Python interpreter.\nC, C++ - Cross-compile the code.\n# References For many years, the typical answer to the \u0026ldquo;ARM CPU vs. x86?\u0026rdquo; question was that x86es are better suited to desktop and high-performance computing, while ARM chips were better suited for mobile devices. That perception changed when Apple released its ARM-based M1 chips in 2020, followed by the powerful M2 series in 2022. source\nx86 processors typically operate independently of peripheral components, such as RAM and GPUs. But ARM processors were designed to package these additional components into a central unit. That\u0026rsquo;s why ARM processors operate as part of a System on Chip (SOC). source\nWhen you communicate with GPUs, you typically use graphics libraries such as OpenGL, Vulkan, or DirectX. The company that sold you the graphics card also provides a graphics driver that translates commands from these graphics libraries into instructions that the GPU can understand. The instructions that your graphics driver generate are in a proprietary protocol known only to a few wizards at NVIDIA and AMD who actually understand how GPUs work on a low level. Us mere mortals don\u0026rsquo;t get to look under the hood at these instructions. Interestingly, many modern CPUs operate the same way. They take your x86 code and internally translate it into something better, faster, and chip specific. Adopted from source\n","date":"2024-04-25T21:30:17Z","permalink":"https://blog.yorforger.cc/p/compiled-and-interpreted-languages-video-draft/","title":"Compiled and Interpreted Languages - Video Draft"},{"content":" # Basic Theory MongoDB stores its information in documents rather than rows. Where relational databases have tables, MongoDB has collections.\nEvery MongoDB document requires an _id, and if one isn’t present when the document is created, a special MongoDB ObjectID will be generated and added to the document at that time. You can set your own _id by setting it in the document you insert, the ObjectID is just MongoDB’s default.\nIndexes don’t come for free; they take up some space and can make your inserts slightly more expensive, but they are an essential tool for query optimization.\nMongoDB store documents in a collection in no particular order. To get documents in a particular order, you must can use the sort() method or the $sort aggregation pipeline stage. Learn more: natural order — MongoDB Manual\n关于 foreign key 的概念常出现在 one to many 关系中, 比如评论表和用户, 一个用户可以有多个评论, 但一个评论只能有一个用户. 在传统数据库中, 评论表中会有一个 user_id 字段, 我们叫它外键, 每次查询时可以通过 join 操作将用户信息和评论信息关联起来 (即获取写了这个评论的用户具体信息和评论的具体内容).\nMongoDB 没有传统数据库中 join 操作和 foreign key, 但可以通过 embedded documents 或者 reference 来表示一对多关系, 也可使用聚合来实现类似 Join 的功能. 具体官方文档有解释: Model One-to-Many Relationships with Embedded Documents, Model One-to-Many Relationships with Document References\n关于一对多, 多对多关系参考: SQL Server Tutorial - One-to-many and many-to-many table relationships\n关于外键请参考: Learning MySQL - FOREIGN KEY CONSTRAINTS\n关于 join 操作请参考: SQL Joins Explained\n# Examples in the Book # Reviews - One to Many Relationship Each product can have many reviews, and you create this relationship by storing a product_id in each review, as shown in the sample document:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { _id: ObjectId(\u0026#34;4c4b1476238d3b4dd5000041\u0026#34;), product_id: ObjectId(\u0026#34;4c4b1476238d3b4dd5003981\u0026#34;), date: new Date(2010, 5, 7), title: \u0026#34;Amazing\u0026#34;, text: \u0026#34;Has a squeaky wheel, but still a darn good wheelbarrow.\u0026#34;, rating: 4, user_id: ObjectId(\u0026#34;4c4b1476238d3b4dd5000042\u0026#34;), username: \u0026#34;dgreenthumb\u0026#34;, helpful_votes: 3, voter_ids: [ ObjectId(\u0026#34;4c4b1476238d3b4dd5000033\u0026#34;), ObjectId(\u0026#34;7a4f0376238d3b4dd5000003\u0026#34;), ObjectId(\u0026#34;92c21476238d3b4dd5000032\u0026#34;) ] } But it may come as a surprise that you store the username as well. If this were an RDBMS, you’d be able to pull in the username with a join on the users table. Because you don’t have the join option with MongoDB, you can proceed in one of two ways: either query against the user collection for each review or accept some denormalization. Issuing a query for every review might be unnecessarily costly when username is extremely unlikely to change, so here we’ve chosen to optimize for query speed rather than normalization.\nAlso noteworthy is the decision to store votes in the review document itself. It’s common for users to be able to vote on reviews. Here, you store the object ID of each voting user in an array of voter IDs. This allows you to prevent users from voting on a review more than once, and it also gives you the ability to query for all the reviews a user has voted on. You cache the total number of helpful votes, which among other things allows you to sort reviews based on helpfulness. Caching is useful, a query to sort reviews by helpful votes, for example, is much easier if the size of the voting array is cached in the helpful_votes field.\n用户名信息原本可以通过用户ID (user_id) 在用户集合(users collection)中查到, 但这样每次刷新评论都要重新查一次用户名.\n这段提到了一个关键概念：缓存（Caching）, 这里的 “缓存” 是指在文档中直接存储一个额外的数据项（在这个例子中是“有帮助的投票数”），而不是每次查询时计算这个数值。\n# Student \u0026amp; Courses - Many to Many Relationship student:\n1 2 3 4 5 6 7 8 9 10 { _id: \u0026lt;number\u0026gt;, name: : \u0026lt;string\u0026gt;, otherDetails: { ... }, courses: [ { courseId: \u0026lt;number\u0026gt;, courseName: \u0026lt;string\u0026gt; }, { courseId: \u0026lt;number\u0026gt;, courseName: \u0026lt;string\u0026gt; }, ... ] } course:\n1 2 3 4 5 6 { _id: \u0026lt;number\u0026gt;, name: \u0026lt;string\u0026gt;, description: \u0026lt;string\u0026gt;, otherDetails: { ... } } Now, your application has some queries. To start with some queries I can think about is, get all students in a particular course and get all courses for a particular student. These are simple queries.\nTo get all courses for a specific student, the query would be:\n1 db.students.find( { name: \u0026#34;John Doe\u0026#34; }, { courses: 1, name: 1 } ) To get all students enrolled for a specific course, your query can be like this:\n1 db.students.find( { \u0026#34;courses.courseName\u0026#34;: \u0026#34;Database Design\u0026#34; } ) This example is from Many to many relationship and linked table/collection - Working with Data - MongoDB Developer Community Forums\n# Query Operators 前面介绍了find函数的基本用法和projection参数, 在实际查询中, 我们还需要使用一些操作符来构建更复杂的查询条件, 接下来一一介绍.\nLearn more: Query and Projection Operators — MongoDB Manual\n# Set operators $in, $nin, $all\n1 2 3 4 5 6 7 8 9 10 11 12 13 // $in: Matches any of the values specified in an array. db.products.find({ \u0026#39;main_cat_id\u0026#39;: { \u0026#39;$in\u0026#39;: [ ObjectId(\u0026#34;6a5b1476238d3b4dd5000048\u0026#34;), ObjectId(\u0026#34;6a5b1476238d3b4dd5000051\u0026#34;), ObjectId(\u0026#34;6a5b1476238d3b4dd5000057\u0026#34;) ] } }, null, null) // Query nested documents db.products.find({\u0026#39;details.color\u0026#39;: {$in: [\u0026#39;blue\u0026#39;, \u0026#39;Green\u0026#39;]}}) # Boolean operators $or, $and, $not, $nor, $exists\n1 2 3 4 5 6 7 8 // $or: matches if any of the supplied set of query terms is true // Finding all products that are either blue or made by Acme requires: db.products.find({ \u0026#39;$or\u0026#39;: [ {\u0026#39;details.color\u0026#39;: \u0026#39;blue\u0026#39;}, {\u0026#39;manufacturer\u0026#39;: \u0026#39;Acme\u0026#39;}, ] }) # Update, atomic operations, and delete - Action in MongoDB chapter 7 # Update You can either replace the document altogether, or you can use update operators to modify specific fields within the document.\nmodify by replacement:\n1 2 3 4 5 user_id = ObjectId(\u0026#34;4c4b1476238d3b4dd5003981\u0026#34;) doc = db.users.findOne({_id: user_id}) doc[\u0026#39;email\u0026#39;] = \u0026#39;mongodb-user@mongodb.com\u0026#39; print(\u0026#39;updating \u0026#39; + user_id) db.users.update({_id: user_id}, doc) The final line says, “Find the document in the users collection with the given _id, and replace that document with the one we’ve provided.”\nmodify by operator:\n1 2 user_id = ObjectId(\u0026#34;4c4b1476238d3b4dd5000001\u0026#34;) db.users.update({_id: user_id}, {$set: {email: \u0026#39;mongodb-user2@mongodb.com\u0026#39;}}) Performance-conscious users may balk at the idea of re-aggregating all product reviews for each update. Much of this depends on the ratio of reads to writes; it’s likely that more users will see product reviews than write their own, so it makes sense to re-aggregate on a write.\n# Standard update operators Certainly! Let\u0026rsquo;s go through each MongoDB update operator with an explanation followed by a real-world example:\n$set: Used to set the value of a field in a document. If the field does not exist, $set will add a new field with the specified value.\nExample: Updating a user\u0026rsquo;s email address.\n1 db.users.update({ username: \u0026#39;johndoe\u0026#39; }, { $set: { email: \u0026#39;johndoe@example.com\u0026#39; } }); $unset: Removes the specified field from a document.\nExample: Removing a phone number field from a user\u0026rsquo;s profile.\n1 db.users.update({ username: \u0026#39;johndoe\u0026#39; }, { $unset: { phoneNumber: \u0026#34;\u0026#34; } }); $inc: Increments the value of a field by the specified amount. If the field does not exist, it is set to the increment amount.\nExample: Incrementing a user\u0026rsquo;s reward points.\n1 db.users.update({ username: \u0026#39;johndoe\u0026#39; }, { $inc: { rewardPoints: 100 } }); $push: Adds an element to an array. If the field is not an array, this operator will create an array with one element.\nExample: Adding a new product to a user\u0026rsquo;s wishlist.\n1 db.users.update({ username: \u0026#39;johndoe\u0026#39; }, { $push: { wishlist: \u0026#39;productId1234\u0026#39; } }); $pull: Removes all instances of a value from an existing array.\nExample: Removing an item from a user\u0026rsquo;s shopping cart.\n1 db.users.update({ username: \u0026#39;johndoe\u0026#39; }, { $pull: { shoppingCart: \u0026#39;itemId5678\u0026#39; } }); $addToSet: Adds a value to an array unless the value is already present, in which case $addToSet does nothing to ensure uniqueness.\nExample: Adding a tag to a blog post without creating duplicates.\n1 db.blogPosts.update({ title: \u0026#39;MongoDB Tips\u0026#39; }, { $addToSet: { tags: \u0026#39;NoSQL\u0026#39; } }); $rename: Renames a field.\nExample: Changing a field name in a contact document.\n1 db.contacts.update({ name: \u0026#39;Jane Doe\u0026#39; }, { $rename: { \u0026#39;cellphone\u0026#39;: \u0026#39;mobileNumber\u0026#39; } }); $mul: Multiplies the value of the field by the specified amount. If the field does not exist, the operation sets the field to zero.\nExample: Updating the price of a product in inventory.\n1 db.products.update({ productId: \u0026#39;A123\u0026#39; }, { $mul: { price: 2 } }); # Slow queries - Chapter 8 值得反复阅读 Finding slow queries is easy with MongoDB’s profiler. Discovering why these queries are slow is trickier and may require some detective work. As mentioned, the causes of slow queries are manifold. If you’re lucky, resolving a slow query may be as easy as adding an index. In more difficult cases, you might have to rearrange indexes, restructure the data model, or upgrade hardware.\nMongoDB’s explain command provides detailed information about a given query’s path.\n1 2 3 4 5 6 7 8 9 10 11 12 db.values.find({}).sort({close: -1}).limit(1).explain() { \u0026#34;cursor\u0026#34; : \u0026#34;BasicCursor\u0026#34;, \u0026#34;isMultiKey\u0026#34; : false, “ \u0026#34;n\u0026#34; : 1, #A Number returned \u0026#34;nscannedObjects\u0026#34; : 4308303, \u0026#34;nscanned\u0026#34; : 4308303, #B Number scanned \u0026#34;nscannedObjectsAllPlans\u0026#34; : 4308303, \u0026#34;scanAndOrder\u0026#34; : true, \u0026#34;millis\u0026#34; : 10927, #C Time in milliseconds, 11 seconds this query took ... } The cursor field tells you that you’ve been using a BasicCursor, which only confirms that you’re scanning the collection itself and not an index. If you had used an index, the value would’ve been BTreeCursor.\nA second datum here further explains the slowness of the query: the scanAndOrder field. This indicator appears when the query optimizer can’t use an index to return a sorted result set. Therefore, in this case, not only does the query engine have to scan the collection, it also has to sort the result set manually.\nAvoid scanAndOrder. If the query includes a sort, attempt to sort using an index. Satisfy all fields with useful indexing constraints—attempt to use indexes for the fields in the query selector. If the query implies a range or includes a sort, choose an index where that last key used can help satisfy the range or sort. 当提到“last key used”这个术语，特别是在上下文中关于选择索引以优化范围查询或排序操作的讨论中，它指的是在复合索引中的最后一个字段。在复合索引中，字段的顺序是至关重要的，因为它决定了数据库如何组织和访问索引数据。让我们通过一个例子来解释这个概念。\n假设你有一个MongoDB集合，其中包含以下字段：a, b, 和 c。现在，假设你创建了一个复合索引 { a: 1, b: 1, c: 1 }。在这个索引中：\na 是第一个键， b 是第二个键， c 是“last key”或最后一个键。 在处理查询时，如果查询涉及到这三个字段中的任意一个的范围条件或排序要求，索引的效率将取决于这些条件是如何与索引中的键匹配的。在理想情况下，你希望查询中的范围或排序操作直接对应于复合索引中的最后一个键，因为这样可以最大化索引的效用。\n例如，考虑以下查询：\n1 db.collection.find({ a: 10, b: { $gt: 5 } }).sort({ c: 1 }) 在这个查询中：\na 是一个精确匹配条件， b 是一个范围查询条件， c 是一个排序条件。 索引 { a: 1, b: 1, c: 1 } 在这种情况下是高效的，因为：\n它首先使用 a 来快速定位数据（第一个键）， 接着，利用 b 来进一步过滤范围内的记录（第二个键）， 最后，使用 c 进行排序（“last key”或最后一个键）。 在这里，“last key” (c) 使得查询可以在使用索引的同时完成排序，从而避免了额外的排序步骤，提高了查询效率。所以，“last key”在复合索引中指的是最后一个被用来支持查询中的范围或排序条件的字段。\n","date":"2024-04-24T18:29:30Z","permalink":"https://blog.yorforger.cc/p/mongodb-in-action-reading-note/","title":"MongoDB in Action Reading Note"},{"content":" # Bson \u0026amp; Marshalling \u0026amp; Unmarshalling The process of converting a Go type to BSON is called marshalling, while the reverse process is called unmarshalling.\nWhy need convert Go type to BSON? MongoDB stores documents in BSON, when we store data (our struct) into MongoDB, MongoDB Go-driver convert our struct value into bson automatically.\nThe Go driver provides four main types for working with BSON data:\nD: An ordered representation of a BSON document (slice) M: An unordered representation of a BSON document (map) \u0026hellip; D is an ordered representation of a BSON document. This type should be used when the order of the elements matters, such as MongoDB command documents. If the order of the elements does not matter, an M should be used instead. This usually used as filter in a query operation.\nLike what we said before, our Go struct value needs to be converted to bson, there are Struct Tags, which is used to modify the default marshalling and unmarshalling behavior of a struct field.\nAfter we read data from MongoDB, we need to convert bson to Go struct, this is called unmarshalling. You can unmarshal BSON documents by using the Decode() method on the result of the FindOne method or any *mongo.Cursor instance.\nNote that when using FindOne() it returns a bson document, so you need decode it back to a Go struct value.\nMethod Find() returns a *mongo.Cursor, we usually iterate through it so get more than one (probably) result. And *mongo.Cursor needs to be closed usually to free resources.\nLearn more:\nmongodb - bson.D vs bson.M for find queries - Stack Overflow\nWork with BSON - Go Driver v1.15\n# Read Operation - Cursor To match a subset of documents, specify a query filter. In a query filter, you can match fields with literal values or with query operators. When you don\u0026rsquo;t know which method you should use, you can go to the mongo driver go API to check how to use them.\n# *mongo.Cursor A cursor is used to iterate over database results from read operations. A cursor is not goroutine safe. Do not use the same cursor in multiple goroutines at the same time. Method Find() returns a cursor.\nYou can retrieve results individually or get all results at a time, it depends on if the number of the result is very large.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cursor, _ := coll.Find(context.TODO(), bson.D{}) // retrieve results individually for cursor.Next(context.TODO()) { var result MyStruct if err := cursor.Decode(\u0026amp;result); err != nil { log.Fatal(err) } fmt.Printf(\u0026#34;%+v\\n\u0026#34;, result) } // get all results at a time var results []MyStruct if err = cursor.All(context.TODO(), \u0026amp;results); err != nil { panic(err) } If the number and size of documents returned by your query exceeds available application memory, your program will crash. If you except a large result set, you should consume your cursor iteratively.\nWhen your application no longer requires a cursor, close the cursor with the Close() method. This method frees the resources your cursor consumes in both the client application and the MongoDB server. Close the cursor when you retrieve documents individually because those methods make a cursor tailable.\n# Others If the necessary database and collection don\u0026rsquo;t exist when you perform a write operation, the server implicitly creates them. So you don\u0026rsquo;t need to create database explicitly when use MongoDB.\ncollection: you can use it concurrentlly\n1 2 3 4 5 6 7 8 9 10 11 12 type Repository struct { db *mongo.Client //Collection is safe for concurrent use by multiple goroutines. PokemonColl *mongo.Collection } func NewRepository(db *mongo.Client) *Repository { return \u0026amp;Repository{ db: db, PokemonColl: db.Database(\u0026#34;pokemon\u0026#34;).Collection(\u0026#34;pokemons\u0026#34;), } } modify behvavior when query: opts third parameter\n1 2 3 // Note that there is three parameters, last is optional. func (coll *Collection) Find(ctx context.Context, filter interface{}, opts ...*options.FindOptions) (cur *Cursor, err error) findByIdAndUpdate(): FindOneAndUpdate returns the original document before updating.\n1 2 3 opts := options.FindOneAndUpdate().SetReturnDocument(options.After) result := coll.FindOneAndUpdate(context.TODO(), filter, update, opts) ... You may wonder how do I know opts := options.FindOneAndUpdate().SetReturnDocument(options.After) this to create an operation? And how can I know the behavior of a method? The answer is the mongo-api, you can see there is the behavor and example and all the thing you need to know before you use it.\nAnd you can check the FindOneAndUpdateOptions just click, to check what is it:\nYou can see this option is in the options package, and you can create it with FindOneAndUpdate(), so it\u0026rsquo;s not difficult to write the codes like this:\n1 2 3 opts := options.FindOneAndUpdate().SetReturnDocument(options.After) result := coll.FindOneAndUpdate(context.TODO(), filter, update, opts) ... # References mongo package - go.mongodb.org/mongo-driver/mongo - Go Packages\nCRUD Operations - Go Driver v1.15\n","date":"2024-04-23T08:38:35Z","permalink":"https://blog.yorforger.cc/p/mongodb-docs-golang-basics/","title":"MongoDB Docs - Golang Basics"},{"content":" # 1. Virtual Memory What is virtual memory? Indirection between the program\u0026rsquo;s addresses and the RAM addresses. Mapping disk space into memory. Virtual memory addresses (VA) and Physical memory addresses (PA). VA is the address that the program uses. PA is the address in RAM. The MMU translates VA to PA. A memory management unit (MMU) is a computer hardware unit that examines all memory references on the memory bus, translating these requests, known as virtual memory addresses, into physical addresses in main memory. Memory management unit\n# 1.1. Example 1 ld r1, 400(r2) This instruction means to load data into register r1 from the address calculated by adding an offset of 400 to the content of register r2.\nCalculate the Virtual Address: Assume the register r2 currently holds the virtual address 0x1000 (4096 in decimal). The offset 400 (in decimal) is added to the content of r2. Therefore, the calculated virtual address is 0x1000 + 0x190 = 0x1190 (virtual address). Virtual to Physical Address Translation: The MMU receives the virtual address 0x1190 and begins to look up the page table for the current process. Suppose the page table entry shows that this virtual address maps to the physical address 0x5000. The page offset (offset within the page) remains 0x190. Thus, the complete physical address is 0x5000 + 0x190 = 0x5190. Access Physical Memory and Execute Instruction: Once the physical address is determined, the MMU instructs the system to load data from physical address 0x5190. The data is loaded into register r1. # 1.2. Other Concepts Both MIPS and ARM are types of RISC architectures. RISC is a type of microprocessor design. MIPS gives each program its own 32-bit address space. Programs can only access any byte in their own address space. What if we don\u0026rsquo;t have enough memory? Holes in our address space? Keeping programs secure with virtual memory. (1) Virtual Memory: 4 How Does Virtual Memory Work?\n# Segment Fault A segmentation fault is a specific type of error that occurs when a program tries to access a segment of memory that it doesn’t have the permissions to access or that doesn’t exist, leading to the program’s abrupt termination by the operating system.\nUnderstanding Segmentation Fault: What it is \u0026amp; How to Fix it\n# How does a game that is several hundred gigabytes run on a computer with only a few gigabytes of memory? # How does user mode switch to kernel mode? Reference: 莉莉丝后端服务器go开发实习生一面二面挂 - 知乎\n","date":"2024-04-20T09:58:10Z","permalink":"https://blog.yorforger.cc/p/os-related/","title":"OS Related"},{"content":" # 1. Test real time network IO and disk IO 1 2 dstat -d # check the disk speed at real time dsat -n # check the network speed at real time # 2. Check the network speed speedtest-cli and iperf3: speedtest-cli measures speeds to public servers, reflecting general internet performance to the user; iperf3 can perform tests between any two points, ideal for in-depth performance testing in private or internal networks.\n1 2 3 4 iperf3 -s -p 5202 # server iperf3 -c server_ip_address -p 5202 # test the upload speed (client upload to server) iperf3 -c server_ip_address -p 5202 -R # test the download speed (client download from server) # Why the result of iperf3 is different from speedtest-cli? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # run iperf3 on the client to test speed between client and server: ❯ iperf3 -c 104.152.xxx.122 Connecting to host 104.152.xxx.122, port 5201 [ 5] local 192.168.2.15 port 64062 connected to 104.152.xxx.122 port 5201 [ ID] Interval Transfer Bitrate [ 5] 0.00-1.00 sec 8.38 MBytes 70.1 Mbits/sec [ 5] 1.00-2.00 sec 15.0 MBytes 126 Mbits/sec [ 5] 2.00-3.00 sec 9.12 MBytes 76.6 Mbits/sec ❯ iperf3 -c 104.152.xxx.122 -R Connecting to host 104.152.xxx.122, port 5201 Reverse mode, remote host 104.152.xxx.122 is sending [ 5] local 192.168.2.15 port 64067 connected to 104.152.xxx.122 port 5201 [ ID] Interval Transfer Bitrate [ 5] 0.00-1.00 sec 128 KBytes 1.04 Mbits/sec [ 5] 1.00-2.00 sec 256 KBytes 2.10 Mbits/sec [ 5] 2.00-3.00 sec 256 KBytes 2.10 Mbits/sec [ 5] 3.00-4.00 sec 384 KBytes 3.14 Mbits/sec # run speedtest-cli on the server: ➜ ~ speedtest-cli Retrieving speedtest.net configuration... Testing from WebNX (104.152.50.122)... Retrieving speedtest.net server list... Selecting best server based on ping... Hosted by S\u0026amp;A Telephone (Allen, KS) [175.61 km]: 49.106 ms Testing download speed................................................................................ Download: 425.75 Mbit/s Testing upload speed...................................................................................................... Upload: 138.52 Mbit/s Test Server Differences:\nSpeedtest-cli connects to servers likely hosted within your ISP\u0026rsquo;s network or well-connected data centers. Iperf3 results depend on endpoints you specify, which might not have optimal network paths or server capabilities. Use of Parallel Connections:\nIperf3 can run multiple concurrent connections, potentially showing higher bandwidth capacities compared to speedtest-cli. # 3. Find which process is using the port 1 sudo lsof -i :8080 lsof: Stands for \u0026ldquo;list open files.\u0026rdquo; The lsof command is used to list currently open \u0026ldquo;files,\u0026rdquo; where in this context, a network port is also considered a \u0026ldquo;file.\u0026rdquo;\n-i: This is an option for the lsof command that restricts the output to only show information related to network connections.\n:8080: Specifies the port number to examine. Here, :8080 indicates port number 8080.\n","date":"2024-04-13T12:40:56Z","permalink":"https://blog.yorforger.cc/p/tools-commonly-used-on-linux/","title":"Tools Commonly Used on Linux"},{"content":" # 1. Problem description Considering the limitation of the RAM on my VPS, when user attempting to upload a large file, I want to handle the file in chunks, so that I can read the file in small pieces and write them to the disk. And this process is called streaming.\nThe code snippet is like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func (s *server) handleUpload(w http.ResponseWriter, r *http.Request, currentDir string) (errs []uploadError) { // limit the size of incoming request bodies. r.Body = \u0026amp;LimitedReader{r: r.Body, n: int64(s.maxFileSize * 1024 * 1024)} reader, err := r.MultipartReader() if err != nil {...} for { // reader.NextPart() will close the previous part automatically. part, err := reader.NextPart() if err != nil {...} // not a file, move to the next part. if part.FileName() == \u0026#34;\u0026#34; { continue } filename := getAvailableName(currentDir, part.FileName()) dstPath := filepath.Join(currentDir, filename) dst, err := os.Create(dstPath) if err != nil {...} // io.Copy() will stream the file to dst, multipart.Part is an io.Reader. _, err = io.Copy(dst, part) if err != nil {...} if err = dst.Close(); err != nil { ... } } return } But when I try to upload a file of 1.6GB, it takes a long time to finish (about 2 minutes), I thought it may caused by the CPU speed of my VPS or the I/O speed of the disk or the network speed.\nBut at the browser side, the uploading progress bar is always at 100%, and the browser is waiting for the response from the server, which indicates that the uploading process is finished, but the server is still processing the file.\nI use htop to check the CPU usage of my VPS, it\u0026rsquo;s around 20% when uploading the file (consists of network IO and disk IO), which indicates that the CPU is not the bottleneck.\nThen I use dd to test the I/O speed of my disk, it\u0026rsquo;s around 488MB/s, which is not bad.\nThen why the server is still processing the file so long?\nBesides, when my server is processing the file, if I refresh the page, the RAM and cpu usage of my VPS will increase to 100%, and the server will get killed by the OS. This is super weird.\n# 2. Solution for high CPU usage The error output (call stack) looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 goroutine 12 [running]: runtime.systemstack_switch() /Users/David/sdk/go1.20.4/src/runtime/asm_amd64.s:463 fp=0xc00004b7b8 sp=0xc00004b7b0 pc=0x4643e0 runtime.(*mheap).alloc(0x1c036000?, 0xe01b?, 0x78?) /Users/David/sdk/go1.20.4/src/runtime/mheap.go:955 +0x65 fp=0xc00004b800 sp=0xc00004b7b8 pc=0x427025 runtime.(*mcache).allocLarge(0x47047e?, 0x1c036000, 0x0) /Users/David/sdk/go1.20.4/src/runtime/mcache.go:234 +0x85 fp=0xc00004b848 sp=0xc00004b800 pc=0x4167e5 runtime.mallocgc(0x1c036000, 0x757080, 0x1) /Users/David/sdk/go1.20.4/src/runtime/malloc.go:1053 +0x4fe fp=0xc00004b8b0 sp=0xc00004b848 pc=0x40d6de runtime.growslice(0xc05ca3c000, 0x15?, 0xc00004b9c0?, 0x1?, 0x757080) /Users/David/sdk/go1.20.4/src/runtime/slice.go:274 +0x4e9 fp=0xc00004b910 sp=0xc00004b8b0 pc=0x44c3a9 main.(*server).handleUpload(0xc00005c240, {0x0?, 0x820a00?}, 0xc000138600, {0xc000098008, 0x8}) /Users/David/Codes/GoLand/file-server/handleFile.go:87 +0x2ad fp=0xc00004ba00 sp=0xc00004b910 pc=0x70290d main.(*server).taskDelegation(0xc00005c240, {0x823000, 0xc000012140}, 0xc000138600) /Users/David/Codes/GoLand/file-server/route.go:25 +0x252 fp=0xc00004bbc0 sp=0xc00004ba00 pc=0x704652 main.(*server).ServeHTTP(0xc00005c240, {0x823000, 0xc000012140}, 0xc000138600) /Users/David/Codes/GoLand/file-server/server.go:144 +0x4eb fp=0xc00004bca0 sp=0xc00004bbc0 pc=0x70608b net/http.serverHandler.ServeHTTP({0xc00001a0a8?}, {0x823000, 0xc000012140}, 0xc000138600) /Users/David/sdk/go1.20.4/src/net/http/server.go:2936 +0x316 fp=0xc00004bd50 sp=0xc00004bca0 pc=0x6a2ff6 net/http.initALPNRequest.ServeHTTP({{0x8232b8?, 0xc0001588a0?}, 0xc00010e700?, {0xc000146000?}}, {0x823000, 0xc000012140}, 0xc000138600) /Users/David/sdk/go1.20.4/src/net/http/server.go:3545 +0x245 fp=0xc00004bed8 sp=0xc00004bd50 pc=0x6a4ee5 net/http.(*initALPNRequest).ServeHTTP(0x40b5b6?, {0x823000?, 0xc000012140?}, 0x0?) \u0026lt;autogenerated\u0026gt;:1 +0x55 fp=0xc00004bf20 sp=0xc00004bed8 pc=0x6b3fb5 net/http.Handler.ServeHTTP-fm({0x823000?, 0xc000012140?}, 0xc00001c2e8?) \u0026lt;autogenerated\u0026gt;:1 +0x42 fp=0xc00004bf50 sp=0xc00004bf20 pc=0x6b4dc2 It seems that the server is trying to allocate memory to a slice, and the size of the slice is too large, so the server is killed by the OS.\nCheck the code snippet I provided before, there is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 for { // reader.NextPart() will close the previous part automatically. part, err := reader.NextPart() if err != nil { if err == io.EOF { // finish reading all parts, exit the loop break } errs = append(errs, uploadError{Message: fmt.Sprintf(\u0026#34;reader.NextPart(): %v\u0026#34;, err)}) continue } } What if the error is not io.EOF, but other errors? Then there will be a infinite loop, this is the reason why the CPU usage is high. What I do is to limit the size of the error message:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 if err != nil { if err == io.EOF { // finish reading all parts, exit the loop break } errs = append(errs, uploadError{Message: fmt.Sprintf(\u0026#34;reader.NextPart(): %v\u0026#34;, err)}) // too many errors, stop uploading, you must limit the number of errors in case of infinite loop. if len(errs) \u0026gt;= 10 { errs = append(errs, uploadError{Message: \u0026#34;Maximum error limit reached\u0026#34;}) return } continue } # 3. Solution for slow uploading speed I ask this question in Go community that I joined, and I got a suggestion:\nForm Upload(multipart/form-data) is not good for large files, when uploading a large file, we usually use Binary Stream Uploads(application/octet-stream) instead, use r.Body directly to read the file content, and write the content to the disk this is real streaming.\nActually, with multipart/form-data, the file content is read by block, not the real streaming, you can check the code I provided before: part, err := reader.NextPart().\nSo use PUT with application/octet-stream instead of multipart/form-data to upload large files.\nBut after I change the code to use POST with application/octet-stream, the uploading speed is still slow.\nThen I monitor the disk I/O speed and network speed and found the reason, the network speed is around 7MB/s, which is the bottleneck.\n1 2 dstat -d # check the disk speed at real time dsat -n # check the network speed at real time But why the speedtest-cli shows the network speed is fast but the real speed is slow?\n1 2 3 Download: 298.52 Mbit/s Testing upload speed...................................................................................................... Upload: 192.82 Mbit/s ","date":"2024-04-12T10:42:22Z","permalink":"https://blog.yorforger.cc/p/an-analysis-of-high-cpu-and-ram-usage-caused-by-go-program/","title":"An Analysis of high CPU and RAM usage caused by Go Program"},{"content":"我正使用 github pages 自定义域名功能, 然后我为我的域名yorforger.cc 添加了一个 CNAME 记录即: blog.yorforger.cc -\u0026gt; shwezhu.github.io\n然后我在 GitHub pages 的自定义域名页面填入 blog.yorforger.cc, 然后一切工作正常, 我可以通过 blog.yorforger.cc 来访问我部署在 GitHub pages 上的博客.\n我还有个域名 yorblogger.top 托管在了 Cloudflare, 我就想 是不是 我可以给它添加一个 CNAME 记录, 比如: blog.yorblogger.top -\u0026gt; blog.yorforger.cc\n这样当我访问 blog.yorblogger.top, DNS 服务器会去查询 blog.yorforger.cc 的地址, 最后找到 shwezhu.github.io 然后访问我的博客, 但是当我尝试在浏览器访问blog.yorblogger.top 的时候, github 提示 404: There isn\u0026rsquo;t a GitHub Pages site here.\n可以看到我确实通过 blog.yorblogger.top 访问到了 shwezhu.github.io, 但是 GitHub Pages 服务器返回了 404 错误, 这是为什么呢?\n我看到了类似的问题 Can github pages CNAME file contain more than one domain? - Stack Overflow\nNo, this is not possible. See the GitHub Help docs that explain this: Ensure you only have one domain listed in your CNAME file. If you wish to have multiple domains pointing to the same Pages, you will need to set up redirects for the other domains. Most domain registrars and DNS hosts offer this service to their customers.\n根据文档, 我们需要设置重定向, 刚好 Cloudflare 也提供了这个服务, 具体可参考: Redirect one domain to another\n根据文档, 随便为我的域名添加了个 A 记录, 然后设置重定向规则, 具体如下:\n此时, 当我访问 yorblogger.top 时, 会自动重定向到 blog.yorforger.cc, 一切工作正常.\n后来我想让 blog.yorblogger.top 重定向到 blog.yorforger.cc, 然后简单设置了一下:\n在 DNS Record 把 www 记录改为 blog 去 Rules -\u0026gt; Redirect Rules 添加一个规则, 与上图相同只是把 hostname=yorblogger.top 改为 hostname=blog.yorblogger.top ","date":"2024-04-07T21:43:22Z","permalink":"https://blog.yorforger.cc/p/cname-%E6%8C%87%E5%90%91-github-pages-%E9%97%AE%E9%A2%98/","title":"CNAME 指向 GitHub Pages 问题"},{"content":" # 1. DNS Hierarchy DNS服务器怎么会知道每个域名的IP地址呢？答案是分级查询, 仔细看下面DNS解析过程，每个域名的尾部都多了一个点.\n多出的那个.是Root Level Domain, 比如www.example.com真正的名字是www.example.com.root然后上图就简写为www.example.com. 因为根域名.root对于所有域名都是一样的，所以平时是省略的。\n域名的层级结构: hostname.SLD.TLD.root, hostname 也叫 subdomain.\n# 2. HOSTNAME vs SLD HOSTNAME 和二级域名(SLD)是不一样的, 二级域名是指example.com里面的example, 而HOSTNAME是指www.example.com里面的www.\nHOSTNAME 的作用是为了区分同一个域名下的不同服务, 比如www.example.com和blog.example.com是同一个域名下的不同服务, 现在都是采用微服务分布式架构, 即每个服务通常不在一个主机, 这样用户访问www.example.com和blog.example.com时就会访问到不同的服务器, 从而提供不同的服务. 如下图可以看出一个 HOSTNAME 可以对应一个 IP:\n# 3. DNS Records # 3.1. A Record 常见的 DNS Records 有 A, CNAME, TXT, 其中 A 记录是最常见的, 用于将域名指向一个 ipv4 IP地址, CNAME 记录用于将域名指向另一个域名.\n当你想给域名添加一个 ip 地址时, 只能添加 A 记录. 添加 A 记录时需要指定 HOSTNAME 和 IP, 其中 HOSTNAME 可以填@或者www或blog等, @代表空即不填,\n一般域名自带默认的 DNS Records, 所以买了域名之后做的第一件事就应该删除这些默认记录, 不然等你又添加了一个A记录, 这时候你的域名就会被解析到多个IP(默认的和你刚添加的), 那浏览器访问你域名的时候, 选择哪个呢? 我查了一下论坛, 有人说是choose randomly, 所以如果你不删除域名所有的默认DNS Records, 那浏览器访问你域名的时候就有可能选择“错误”的ip,\nYes you can. It is called round-robin DNS, and the browser just chooses one of them randomly. It is a well used method of getting cheap load balancing, but if one host goes down, users will still try to access it. https://serverfault.com/q/528742/761923\n# 3.2. CNAME Record Use a CNAME record instead of an A record when one domain or subdomain is just another name for a different domain. All CNAME records must point to a domain, never to an IP address.\ndomain: example.com, subdomain: blog.example.com\n假设你有个主网站 example.com，它有一个A记录指向IP地址 123.45.67.89。若你还想通过 www.example.com 访问这个网站，你可以为 www 设置一个CNAME记录，指向 example.com，而不是再次创建一个A记录指向 123.45.67.89。\n这样的设置如下：\nexample.com A记录 -\u0026gt; 123.45.67.89 www.example.com CNAME记录 -\u0026gt; example.com 当用户尝试访问 www.example.com 时，DNS解析流程如下：\nDNS查找 www.example.com 的记录。 找到CNAME记录，了解到 www.example.com 是 example.com 的别名。 接着，DNS会解析 example.com 的A记录，获取其IP地址 123.45.67.89。 用户的请求最终被定向到IP地址 123.45.67.89，也就是 example.com 所在的服务器。 你可能会好奇, 为什么不直接给 www.example.com 添加一个 A 记录, 指向 123.45.67.89 呢?\n再次创建一个A记录指向 123.45.67.89 以通过 www.example.com 访问网站是完全可行的。 然而，选择为 www 使用CNAME记录而不是另一个A记录主要是为了维护简便性：比如如果你的服务器IP地址发生变化，此时只需要更新 example.com 的A记录。所有指向 example.com 的CNAME记录（如 www.example.com）将自动指向新的IP地址。如果使用A记录，你需要手动更新 example.com 和 www.example.com 的A记录。\nGithub Pages 的 custom domain 就可以使用 CNAME 记录, 即只需简单给你的域名添加一个 CNAME 记录, 指向 username.github.io 即可. 注意添加 CNAME 记录时, 我的 HOSTNAME 填的是 blog, 即 blog.example.com 指向 username.github.io, 你也可以把 HOSTNAME 设置为空, 若为空则代表你的主域名 example.com 指向 username.github.io, 根据个人喜好来设置.\n# 3.3. TTL Field 另外 DNS Records 有一个字段叫 TTL, 这里介绍一下: Time to Live (TTL) is a field on DNS records that controls how long each record is valid and — as a result — how long it takes for record updates to reach your end users. Longer TTLs speed up DNS lookups by increasing the chance of cached results, but a longer TTL also means that updates to your records take longer to go into effect.\n# 4. 总结 买过来域名, 根据不同情况可能做的修改如下:\n无托管域名, 则直接去你的域名注册商那里添加 DNS Records, 一般是添加 A 记录, 指向你的服务器 IP 地址 即可. 若要将域名托管到其他地方 (如 Cloudflare), 则只需要修改域名的 Nameserver 为 Cloudflare 指定的的 Nameserver 即可. 之后在 Cloudflare 上即添加管理不同类似的 DNS Records. References:\nTime to Live (TTL) · Cloudflare DNS docs 什么是 DNS CNAME 记录？ | Cloudflare What is DNS Hierarchy? ","date":"2024-04-07T00:15:30Z","permalink":"https://blog.yorforger.cc/p/dns-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/","title":"DNS 基础概念"},{"content":" # R2 vs S3 Cloudflare R2 vs AWS S3 | Review Pricing \u0026amp; Features | Cloudflare\n# Install mc (MinIO Client) 1 2 3 4 # just install minio client not minio server brew install minio/stable/mc mc alias set r2 R2-URL AccessKey SecretKey The MinIO client provides identical commands to Unix file management commands, such as cp and ls, but is designed for both local and remote storage systems. It’s fully compatible with AWS S3.\n# Common Commands 1 2 3 4 5 mc ls \u0026lt;ALIAS\u0026gt;/\u0026lt;BucketName\u0026gt;: List all objects in a bucket. mc cp -r \u0026lt;LOCAL-FOLDER-PATH\u0026gt; \u0026lt;ALIAS\u0026gt;/\u0026lt;BUCKET\u0026gt;/\u0026lt;REMOTE-FOLDER-PATH\u0026gt;: Upload a folder to a bucket. mc rm \u0026lt;ALIAS\u0026gt;/\u0026lt;BUCKET\u0026gt;/\u0026lt;OBJECT\u0026gt;: Remove an object. # PicGo with R2 You need install a S3 plugin to use R2 as a storage service.\nHere is how to configure S3:\n自定义域名不一定非要填你自己买的域名, 你使用 R2 服务, CF 已经给你一个自家的 subdomain 了, 可以直接填在这. 需要注意的地方是: 填写自定义域名的的时候需要在后加一个 /blogs, 因为此插件上传文件时会自动创建一个与 bucket 同名的文件夹, 并不是直接在根目录上传文件, 然后每次上传完自动复制的 url 并不带 /blogs, 所以需要手动加上, 可以根据你的实际情况来添加路径,\nReleases · Molunerfinn/PicGo\nMacOS cannot open the app because it is from an unidentified developer, check the solution\nCloudFlare R2搭建个人图床\n","date":"2024-04-06T18:28:22Z","permalink":"https://blog.yorforger.cc/p/cloudflare-r2/","title":"Cloudflare R2"},{"content":" # 1. Issue When I use wget to download some files, I got the following error:\n1 2 $sh -c \u0026#34;$(wget https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)\u0026#34; Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... You can use curl to replace wget, or you can get the ip address of the domain and add it to /etc/hosts file.\n1 185.199.108.133 raw.githubusercontent.com You can find the ip address with dig/ping command or on the some online tool website. ping probably won\u0026rsquo;t work.\n","date":"2024-03-29T20:28:22Z","permalink":"https://blog.yorforger.cc/p/wget-connecting-timeout-issue/","title":"Wget Connecting Timeout Issue"},{"content":" # 1. Minor tricks # 1.1. array It is useful to remember which operations on arrays mutate them, and which don’t. For example, push, pop, reverse, and sort will mutate the original array, but slice, filter, and map will create a new one.\nfilter() creates a shallow copy of a portion of a given array.\n1 2 3 4 5 6 7 8 if (isEmpty) { postList = \u0026lt;h1\u0026gt;No posts found.\u0026lt;/h1\u0026gt; } else { // return a new array postList = posts.map(post =\u0026gt; ( \u0026lt;SimplePostCard post={post} key={post._id} onDelete={() =\u0026gt; handleDeletePost(post._id)}/\u0026gt; )); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // state 后期可以用数据库代替 const UsersState = { users: [], setUsers: function (newUsersArray) { this.users = newUsersArray } } // 从UsersState中删除指定的用户 function userLeavesApp(id) { UsersState.setUsers( // filter 返回一个新数组 浅拷贝 shallow copy // filter() creates a shallow copy of a portion of a given array UsersState.users.filter(user =\u0026gt; user.id !== id) ) } // 在用户加入聊天室时激活用户，并确保 UsersState 中没有重复的用户 function activateUser(id, name, room) { const user = { id, name, room } UsersState.setUsers([ ...UsersState.users.filter(user =\u0026gt; user.id !== id), user ]) return user } # 1.2. string length The length of a String value is the length of the string in UTF-16 code units not the number of characters. learn more: String: length - JavaScript | MDN\n1 2 3 console.log(\u0026#39;a\u0026#39;.length); // 1 console.log(\u0026#39;汉\u0026#39;.length); // 1 console.log(\u0026#39;😀\u0026#39;.length); // 2 1 UTF-16 code unit = 16 bits = 2 bytes\n# 1.3. encding string to utf-8 in JS TextEncoder: TextEncoder - Web APIs | MDN\n# 1.4. localStorage localStorage calculates its size limit based on the number of UTF-16 code units, not bytes. You can use the length property to get the number of code units in a string.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 function setLocalStorageSize() { localStorage.clear(); if (localStorage \u0026amp;\u0026amp; !localStorage.getItem(\u0026#39;size\u0026#39;)) { let i = 0; try { // Roughly 10240000 UTF-16 code units. for (i = 250; i \u0026lt;= 10000; i += 250) { // A character is 1B, (i * 1024) = i * 1KB localStorage.setItem(\u0026#39;test\u0026#39;, new Array((i * 1024) + 1).join(\u0026#39;a\u0026#39;)); } } catch (e) { localStorage.removeItem(\u0026#39;test\u0026#39;); // size in utf-16 code units, not bytes. localStorage.setItem(\u0026#39;size\u0026#39;, String(i - 250)); console.log(\u0026#39;localStorage size: \u0026#39; + (i - 250) + \u0026#39;*1024 code units\u0026#39;); } } } // will print: 5000*1024 code units If you change the code above to:\n1 localStorage.setItem(\u0026#39;test\u0026#39;, new Array((i * 1024) + 1).join(\u0026#39;汉\u0026#39;)); Stil will print: 5000*1024 code units, because 汉 is 1 UTF-16 code unit same as a.\nBut if you change the code above to:\n1 localStorage.setItem(\u0026#39;test\u0026#39;, new Array((i * 1024) + 1).join(\u0026#39;😀\u0026#39;)); Will print: 2500*1024 code units, because 😀 is 2 UTF-16 code units.\n# 2. Spread operator Spread operator ...:\nFunction arguments list (myFunction(a, \u0026hellip;iterableObj, b)) Array literals ([1, \u0026hellip;iterableObj, \u0026lsquo;4\u0026rsquo;, \u0026lsquo;five\u0026rsquo;, 6]) Object literals ({ \u0026hellip;obj, key: \u0026lsquo;value\u0026rsquo; }) 1 2 3 4 5 6 7 8 9 10 // We pass a function as argument to setPost(), like a callback, which will return a new state object. // React will call this callback with the previous state `post` as argument. setPost(prevPost =\u0026gt; ({ ...prevPost, // object spread syntax comments: [data, ...prevPost.comments], // array spread syntax engagement: { ...prevPost.engagement, // object spread syntax numComments: prevPost.engagement.numComments + 1, } })); Arrow function will return the value of the expression by default, so we don\u0026rsquo;t need to use return keyword.\n# 3. Falsy values In JavaScript, we have 6 falsy values:\nfalse 0 (zero) ‘’ or “” (empty string) null undefined NaN All these return false when they are evaluated.\n1 2 3 4 // this just for simplicity, no this syntax if(false/0/\u0026#39;\u0026#39;/null/undefined/NaN) { console.log(\u0026#34;never executed\u0026#34;) } # 4. Catching errors # 4.1. Catching errors in async functions In JavaScript, try...catch blocks are designed to handle errors in synchronous code. However, they do not work as expected with asynchronous code, unless used in conjunction with async/await.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 function fetchData() { return new Promise((resolve, reject) =\u0026gt; { // Simulate an asynchronous operation that fails setTimeout(() =\u0026gt; reject(new Error(\u0026#34;Failed to fetch data\u0026#34;)), 1000); }); } // The catch block here does not catch the error from fetchData() function main() { try { fetchData().then((data) =\u0026gt; { console.log(data); }); } catch (error) { console.error(\u0026#39;Error:\u0026#39;); } } Correct approach:\n1 2 3 fetchData() .then(data =\u0026gt; console.log(data)) .catch(error =\u0026gt; console.error(\u0026#39;Error:\u0026#39;, error)); 1 2 3 4 5 6 7 8 async function loadData() { try { const data = await fetchData(); console.log(data); } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); } } # 4.2. Forget catching errors in neasted promises 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 function fetchPosts() { fetch(`/posts`, { method: \u0026#39;GET\u0026#39;, }) .then((res) =\u0026gt; { if (res.status === 401) { return redirect(\u0026#39;/login\u0026#39;); } // Catch block here does not catch errors from res.json() res.json().then(data =\u0026gt; setPosts(data)); }) .catch((err) =\u0026gt; { console.error(\u0026#39;error fetching post:\u0026#39;, err); }); } Your current code handles errors from the fetchcall itself but does not handle potential errors that may occur during the parsing of the response withres.json(). This could be improved by adding a .catch` block specifically for this parsing stage.\nSince you are using .then() for promise resolution, it\u0026rsquo;s fine. However, consider using an async function with await for better readability and easier error handling. This is more of a stylistic choice but can improve the clarity of your code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 async function fetchPosts() { try { const res = await fetch(`/posts/${props.id}/`, { method: \u0026#39;GET\u0026#39;, credentials: \u0026#39;include\u0026#39;, }); if (!res.ok) { if (res.status === 401) { // Handle unauthorized access return redirectToLogin(); // Assuming redirectToLogin is a defined function } throw new Error(\u0026#39;Network response was not ok.\u0026#39;); } const data = await res.json(); setPosts(data); } catch (err) { console.error(\u0026#39;Error fetching posts:\u0026#39;, err); // Handle the error gracefully here } } # 5. await xxxx won\u0026rsquo;t return a promise but the actual result of the promise 1 2 3 4 5 6 7 8 9 async function handleGetPosts(req, res) { const posts = await fetchPosts(10); posts .then(...) .catch(...); } // You will get TypeError: posts.then is not a function The issue in your code is that you\u0026rsquo;re using await incorrectly with the fetchPosts function. When you use await, it waits for the promise to resolve and returns the result directly. Therefore, posts in your code is not a promise but the actual result of the promise.\n1 2 3 4 5 6 7 8 async function handleGetPosts(req, res) { try { const posts = await fetchPosts(10); ... } catch (err) { ... } } ","date":"2024-03-12T10:55:20Z","permalink":"https://blog.yorforger.cc/p/tricks-mistakes-in-javascript/","title":"Tricks \u0026 Mistakes in Javascript"},{"content":" # e.preventDefault() 1 2 3 4 5 6 7 8 9 10 11 function sendMessage(e) { e.preventDefault() // Submitting form would refresh the page, so we prevent page reload const input = document.querySelector(\u0026#39;input\u0026#39;) if (input.value) { ws.send(input.value) input.value = \u0026#34;\u0026#34; } input.focus() // Focus the input after sending the message } document.querySelector(\u0026#39;form\u0026#39;).addEventListener(\u0026#39;submit\u0026#39;, sendMessage) Usually used to prevent page reload when submit form.\n# defer attribute of \u0026lt;script\u0026gt; 1 2 3 4 5 6 7 \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Chat APP\u0026lt;/title\u0026gt; \u0026lt;script defer src=\u0026#34;js/app.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; ... A boolean value used to execute the script only after the browser completes the rendering activity.\nNote: The defer attribute can only be used when JavaScript is externally linked to the HTML file using the src attribute of the \u0026lt;script\u0026gt; tag.\n# innerHTML property innerHTML is a property of every element. It tells you what is between the starting and ending tags of the element, and it also let you sets the content of the element.\n1 2 3 4 5 6 \u0026lt;p class=\u0026#34;myp\u0026#34;\u0026gt;\u0026lt;a\u0026gt;hello hi ni\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;script\u0026gt; const myp = document.querySelector(\u0026#39;.myp\u0026#39;) console.log(myp.innerHTML) \u0026lt;/script\u0026gt; This will print a stirng:\n1 \u0026lt;a\u0026gt;hello hi ni\u0026lt;/a\u0026gt; The innerHTML property is part of the Document Object Model (DOM) that allows Javascript code to manipulate a website being displayed. Specifically, it allows reading and replacing everything within a given DOM element (HTML tag).\nReferences:\nWhat is the use of the defer attribute in the HTML ","date":"2024-03-07T20:02:10Z","permalink":"https://blog.yorforger.cc/p/html-basics/","title":"HTML Basics"},{"content":" 1 2 3 4 5 6 7 8 9 # make sure npm and node.js are install npm -v node -v # create folder and init node.js project mkdir ws-chat \u0026amp;\u0026amp; cd ws-chat npm init # execute js file touch index.js node xxx.js # or use \u0026#39;npm start\u0026#39; predefined in package.json # 1. Intro Node. js is not a programming language. Rather, it\u0026rsquo;s a runtime environment that\u0026rsquo;s used to run JavaScript outside the browser. For example on the server or in the command line.\nRecall a paragraph which introduces js engine:\nThe use of JavaScript engines is not limited to browsers. For example, the V8 engine is a core component of the Node.js. V8 is the Javascript engine inside of node.js that parses and runs your Javascript. The same V8 engine is used inside of Chrome to run javascript in the Chrome browser. Both chrome browser and node.js have v8 inside.\nA Node.js app runs in a single process, without creating a new thread for every request. Node.js provides a set of asynchronous I/O primitives in its standard library that prevent JavaScript code from blocking and generally, libraries in Node.js are written using non-blocking paradigms, making blocking behavior the exception rather than the norm.\n# 2. Install 1 2 3 4 # which will insatll both nodejs and npm brew install node # uninstall brew uninstall node Another way: go to Download | Node.js, note that when you install, the install pragram will reminds you:\n1 2 3 This package will install: •\tNode.js v16.17.0 to /usr/local/bin/node •\tnpm v8.15.0 to /usr/local/bin/npm # 3. npm npm is the package manager for JavaScript, commonly used with Node.js. It is widely used for managing dependencies in JavaScript projects and for running scripts defined in a project\u0026rsquo;s package.json file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 npm init: This command initializes a new Node.js project. It creates a package.json file which holds various metadata relevant to the project. npm install: This command is used to install dependencies listed in the package.json file. When used with the --save flag, it also updates the package.json file with the newly installed package. npm install \u0026lt;package-name\u0026gt;: This command installs a specific package. For example, npm install express installs the Express framework. npm install -g \u0026lt;package-name\u0026gt;: The -g flag installs the package globally, making it available as a command line tool. npm uninstall \u0026lt;package-name\u0026gt;: This command removes a package from the node_modules directory and updates the package.json file. npm update: This command updates the installed packages to their latest versions based on the version range specified in the package.json file. npm list: This command displays a tree of every package installed in the current project. References:\nAbout npm | npm Docs https://nodejs.dev/en/learn/ ","date":"2024-03-07T14:06:27Z","permalink":"https://blog.yorforger.cc/p/nodejs-intro/","title":"Nodejs Intro"},{"content":" # 浏览器 JS 异步执行的原理 JS 是单线程的，也就是同一个时刻只能做一件事情，为什么浏览器可以同时执行异步任务呢？\n因为浏览器是多线程的，当 JS 需要执行异步任务时，浏览器会另外启动一个线程去执行该任务。也就是说，“JS 是单线程的”指的是执行 JS 代码的线程只有一个，是浏览器提供的 JS 引擎线程（主线程）。浏览器中还有定时器线程和 HTTP 请求线程等，这些线程主要不是来跑 JS 代码的。\n以 Chrome 为例，浏览器不仅有多个线程，还有多个进程，如渲染进程、GPU 进程和插件进程等。而每个 tab 标签页都是一个独立的渲染进程，所以一个 tab 异常崩溃后，其他 tab 基本不会被影响。作为前端开发者，主要重点关注其渲染进程，渲染进程下包含了 JS 引擎线程、HTTP 请求线程和定时器线程等，这些线程为 JS 在浏览器中完成异步任务提供了基础。\n# 事件驱动浅析 浏览器异步任务的执行原理背后其实是一套事件驱动的机制。NodeJS 和浏览器的设计都是基于事件驱动的，简而言之就是由特定的事件来触发特定的任务，这里的事件可以是用户的操作触发的，如 click 事件；也可以是程序自动触发的，比如浏览器中定时器线程在计时结束后会触发定时器事件。\n了解更多: 面试必问之 JS 事件循环(Event Loop)，看这一篇足够 - 知乎\n# JavaScript Event Loop 浏览器 nodes.js 的事件循环与浏览器的事件循环有所不同，这里只讨论浏览器的事件循环。\n调用栈（Call Stack）：\n调用栈是一个LIFO（后进先出）结构，用于跟踪程序中的函数调用。 当一个函数被执行时，它被添加到调用栈的顶部；当函数执行完毕时，它被从栈顶移除。 事件队列（Event Queue）：\n当异步事件（如用户输入、文件读取等）发生时，相关的回调函数被添加到事件队列中。 事件队列是一种FIFO（先进先出）结构。 事件队列分为两种：宏任务（Macro Tasks）和微任务（Micro Tasks）队列. 宏任务（Macro Tasks）与微任务（Micro Tasks）：\n宏任务包括脚本执行、setTimeout、setInterval等。 微任务通常来自Promise、MutationObserver等。 在每次宏任务执行完毕后，会处理所有的微任务队列，然后再执行下一个宏任务。 也有人这样去理解：微任务是在当前事件循环的尾部去执行；宏任务是在下一次事件循环的开始去执行。 事件循环的作用是监控 Call Stack 和 Event Queue. 如果 Call Stack 为空，事件循环会查看 Event Queue。如果事件队列中有等待的回调函数，事件循环会将它们依次移动到 Call Stack 中进行执行。这个循环过程是持续不断的，这就是为什么它被称为“事件循环”。\nReferences:\n面试必问之 JS 事件循环(Event Loop)，看这一篇足够 - 知乎 Difference between the Event Loop in Browser and Node Js? - DEV Community The event loop - JavaScript | MDN The Node.js Event Loop, Timers, and process.nextTick() | Node.js ","date":"2024-01-27T22:54:02Z","permalink":"https://blog.yorforger.cc/p/event-loop-in-browser/","title":"Event Loop in Browser"},{"content":" # Definition In simple words, a loop invariant is some predicate (condition) that holds for every iteration of the loop. For example, let\u0026rsquo;s look at a simple for loop that looks like this:\n1 2 3 int j = 9; for(int i=0; i\u0026lt;10; i++) j--; In this example it is true (for every iteration) that i + j == 9. A weaker invariant that is also true is that i \u0026gt;= 0 \u0026amp;\u0026amp; i \u0026lt;= 10.\nAs people point out, the loop invariant must be true:\nbefore the loop starts before each iteration of the loop after the loop terminates 就是正确的算法在循环的各个阶段, 总是存在一个固定不变的特性, 找出这个特性并且你写的代码可以证明其固定不变, 则可推断出你写的代码是正确的. 如何证明那个特性(循环不变式)固定不变呢? 具体的说就是证明它满足上面的三个条件.\n( although it can temporarily be false during the body of the loop ). On the other hand the loop conditional must be false after the loop terminates, otherwise the loop would never terminate. Thus the loop invariant and the loop conditional must be different conditions.\n# Example # Binary Search 第一步确定循环不变量：在每次迭代开始时以及循环结束时，如果目标元素存在于数组中，则它必定位于数组的 [low, high] 索引范围内 (左闭右闭区间)\n第二步根据循环不变量来验证自己写好的代码 (或者写代码的时候就开始验证):\n1 2 3 4 5 6 7 8 9 10 11 12 def search(self, nums, target): left = 0 right = len(nums) - 1 # 记得减1, 因为我们循环不变量为左闭右闭区间 while left \u0026lt; right: m = (left + right) // 2 if target \u0026gt; nums[m]: left = m + 1 elif target \u0026lt; nums[m]: right = m - 1 else: return m return -1 第三步验证：\nbefore the loop starts：若目标在数组中, 则其必在 [left, high] 区间内, 因此循环不变量成立 before each iteration of the loop: 每次迭代开始时, 我们都会将搜索区间减半, left = m + 1 或 right = m - 1, 排除了当前中间元素, 因此循环不变量成立 after the loop terminates: 循环结束后, 有可能是 left == right, 比如 [3, 3], 若元素的index就是3, 则错过了, 循环不变量不成立, 因此要 while 结束的条件要改为 left \u0026lt;= right 而不是 left \u0026lt; right 注意, 循环不变量本身用于确保算法逻辑的正确性，而不直接涉及到如何避免死循环, 若上面代码改为如下, 则会死循环:\n1 2 3 4 if target \u0026gt; nums[m]: left = m elif target \u0026lt; nums[m]: right = m 而此时循环不变量仍然成立: 若存在, 目标元素在 [left, right] 区间内,\n〉 left = m + 1, 或 right = m - 1 保证了每次迭代都会排除至少一个元素, 避免了死循环的发生\n结合以上各个算法，可以找出根据需要写二分查找的规律和具体步骤，比死记硬背要强不少，万变不离其宗嘛：\n(1)大体框架必然是二分，那么循环的key与array[mid]的比较必不可少，这是基本框架;\n(2)循环的条件可以先写一个粗略的，比如原始的while(left\u0026lt;=right)就行，这个循环条件在后面可能需要修改；\n(3)确定每次二分的过程，要保证所求的元素必然不在被排除的元素中，换句话说，所求的元素要么在保留的其余元素中，要么可能从一开始就不存在于原始的元素中；\n(4)检查每次排除是否会导致保留的候选元素个数的减少？如果没有，分析这个边界条件，如果它能导致循环的结束，那么没有问题；否则，就会陷入死循环。为了避免死循环，需要修改循环条件，使这些情况能够终结。新的循环条件可能有多种选择：while(left\u0026lt; right)、while(left\u0026lt; right-1)等等，这些循环条件的变种同时意味着循环终止时候选数组的形态。\n(5)结合新的循环条件，分析终止时的候选元素的形态，并对分析要查找的下标是否它们之中、同时是如何表示的。\n对于(3)，有一些二分算法实现不是这样的，它会使left或right在最后一次循环时越界，相应的left或right是查找的目标的最终候选，这一点在理解时需要注意。当然，不利用这个思路也可以写出能完成功能的二分查找，而且易于理解。\nReferences:\nalgorithm - What is a loop invariant? - Stack Overflow 利用循环不变式写出正确的二分查找及其衍生算法 – KelvinMao Blog ","date":"2024-01-19T20:32:25Z","permalink":"https://blog.yorforger.cc/p/loop-invariant/","title":"Loop Invariant"},{"content":" # 1. Quick start Execute the following commands at your terminal, you don\u0026rsquo;t need to execute them under your project folder, just execute them anywhere, it will install the protoc compiler and the protoc-gen-go and protoc-gen-go-grpc plugins into your $GOPATH/bin folder.\n1 2 3 4 5 6 7 # install protoc compiler $ brew install protobuf $ protoc --version # Ensure compiler version is 3+ # install plugins for $ go install google.golang.org/protobuf/cmd/protoc-gen-go@latest $ go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest Update your $PATH so that the protoc compiler can find the plugins: export PATH=\u0026quot;$PATH:$(go env GOPATH)/bin\u0026quot;\n# 2. Usage protoc 命令的基本用法是 protoc [OPTION] PROTO_FILES, 其中 OPTION 包括搜索路径参数, 语言插件参数等, PROTO_FILES 是要编译的 .proto 文件. 可以通过 protoc --help 查看所有参数. 下面主要介绍 [OPTION] 参数.\n# 2.1. 搜索路径参数 -IPATH 或 --proto_path=PATH: 指定搜索 .proto 文件的目录 如: -I. 表示在当前目录下搜索 如果不指定该参数，则默认在当前路径下进行搜索 # 2.2. 语言插件 protoc-gen-go protoc 本身支持多种语言, \u0026ndash;cpp_out=，\u0026ndash;python_out=等, 但不支持 Go 由google维护，需要安装 protoc 插件：\n1 $ go install google.golang.org/protobuf/cmd/protoc-gen-go@latest 之前介绍了 go install会把 protoc-gen-go 安装到 $GOPATH/bin 目录下, 之后就可以在 protoc 命令中使用 --go_out 等参数了.\n注意: protoc-gen-go 要求 pb (.proto) 文件中必须指定 go 包的路径, 例如 option go_package = \u0026quot;github.com/shwezhu/consignment-service/proto/consignment\u0026quot;. 文件结构: consignment-service/proto/consignment/consignment.proto\n1 2 3 4 5 6 7 8 $ protoc I. --go_out=. --go_opt=paths=source_relative \\ --go-grpc_out=. --go-grpc_opt=paths=source_relative \\ chat.proto # or $ protoc I. --go_out=. --go_opt=paths=import \\ --go-grpc_out=. --go-grpc_opt=paths=import \\ chat.proto 解释:\n--go_out=. 作用: 指定go代码生成的基本路径 属于哪个插件：这个参数是为 protoc-gen-go 插件设置的 --go_opt=paths=source_relative 属于哪个插件：这同样是为 protoc-gen-go 插件设置的 一般有两个值: paths=source_relative 和 paths=import paths=source_relative: 例如你的 .proto 文件位于 proto/myapp/myproto.proto, 若使用 paths=source_relative 选项，生成的 Go 文件将会在类似 myapp/myproto.pb.go 的路径下 paths=import: 例如你在 myproto.proto 文件中指定了 option go_package = \u0026quot;github.com/example/project/proto\u0026quot;;，那么无论 .proto 文件的实际位置在哪里，生成的 Go 文件都将使用这个路径作为包路径 --go-grpc_out=. 属于哪个插件：这个参数是为 protoc-gen-go-grpc 插件设置的 --go-grpc_opt=paths=source_relative 属于哪个插件：这是为 protoc-gen-go-grpc 插件设置的 protoc-gen-go vs protoc-gen-go-grpc: The old-way is using protoc-gen-go that generates both serialization of the protobuf messages and grpc code. Now protoc-gen-go no longer supports generating gRPC service definitions. For gRPC code, a new plugin called protoc-gen-go-grpc was developed by Go gRPC project. source\n# 2.3. grpc go 插件 protoc-gen-go 纯粹用来生成pb(.proto)序列化相关的文件, 不再承载 gRPC 代码生成功能, 生成gRPC相关代码需要安装 grpc-go 相关的插件 protoc-gen-go-grpc, 安装命令如下:\n1 $ go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest 插件 protoc-gen-go-grpc 也有两个参数, 类似 protoc-gen-go 插件的 --go_out, --go_opt 参数, 分别是 --go-grpc_out, --go-grpc_opt, 用法也是一样的,\n# 3. Example 文件结构如下:\n1 2 3 4 5 6 ❯ tree -L 4 . ├── Makefile ├── go.mod ├── proto │ └── calculator.proto calculator.proto:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 syntax = \u0026#34;proto3\u0026#34;; // \u0026#34;./pb\u0026#34; 指定了生成的 Go 文件将被放置在相对路径 ./pb 的目录下 // 这里的相对路径是相对于 protoc 命令执行的目录, 而不是相对于 .proto 文件的目录 // protoc-gen-go 要求 `.proto` 文件必须指定 go 包的路径 option go_package = \u0026#34;./pb\u0026#34;; service Calculator { rpc Add(CalculationRequest) returns (CalculationResponse); rpc Subtract(CalculationRequest) returns (CalculationResponse); rpc Multiply(CalculationRequest) returns (CalculationResponse); rpc Divide(CalculationRequest) returns (CalculationResponse); rpc Sum(NumbersRequest) returns (CalculationResponse); } message CalculationRequest { int64 a = 1; int64 b = 2; } message CalculationResponse { int64 result = 1; } message NumbersRequest { repeated int64 numbers = 1; } Makefile:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # run: make generate generate: protoc --proto_path=proto proto/*.proto --go_out=. --go-grpc_out=. # 1. 上面使用的所有路径都是相对 protoc 命令执行时所在的目录 # 2. --proto_path=proto: 指定 .proto 文件的搜索路径, 在这，编译器会在名为 proto 的目录下查找 .proto 文件, # 你也可以使用 -IPATH 命令行参数来指定搜索路径 如 protoc -I. --go_out=. --go-grpc_out=. proto/*.proto # proto/*.proto : 指定要编译的 .proto 文件，这里我们指定了 proto 目录下的所有 .proto 文件, # 不指定会报错, 因为 protoc 需要知道要编译哪个文件, 你可以使用如 proto/xxx.proto # 3. --go_out=.: 还记得我们在 .proto 文件中指定了 go_package=\u0026#34;./pb\u0026#34; 吗, # --go_out=. 为 go_package 指定了所在的工作目录, 举例: # 若你使用 --go_out=abc, go_package=\u0026#34;./pb\u0026#34;, 则生成的文件会放在 abc/pb 目录下, 前提是你得先创建 abc 目录 # 我们在这指定的是 --go_out=., 所以最终生成的文件会放到 ./pb 目录下 执行 make generate 命令, 文件结构如下:\n1 2 3 4 5 6 7 8 9 ❯ tree -L 4 . ├── Makefile ├── go.mod ├── pb │ ├── calculator.pb.go │ └── calculator_grpc.pb.go ├── proto │ └── calculator.proto 安装 grpc 包, 编写程序:\n1 ❯ go get google.golang.org/grpc 创建文件 server/main.go (我省略了一些代码):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package main type server struct { pb.UnimplementedCalculatorServer } func (s *server) Add( ctx context.Context, in *pb.CalculationRequest, ) (*pb.CalculationResponse, error) { return \u0026amp;pb.CalculationResponse{ Result: in.GetA() + in.GetB(), }, nil } func main() { listener, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:8080\u0026#34;) if err != nil { log.Fatalln(\u0026#34;failed to create listener:\u0026#34;, err) } s := grpc.NewServer() // binds the Calculator service implementation to the gRPC server s. pb.RegisterCalculatorServer(s, \u0026amp;server{}) if err := s.Serve(listener); err != nil { log.Fatalln(\u0026#34;failed to serve:\u0026#34;, err) } } 最后文件结构如下:\n1 2 3 4 5 6 7 8 9 10 11 12 ❯ tree -L 4 . ├── Makefile ├── go.mod ├── go.sum ├── pb │ ├── calculator.pb.go │ └── calculator_grpc.pb.go ├── proto │ └── calculator.proto └── server └── main.go 运行服务器:\n1 ❯ go run server/main.go # 4. References (1) When RESTful architecture isn\u0026rsquo;t enough\u0026hellip;\n","date":"2024-01-19T00:01:08Z","permalink":"https://blog.yorforger.cc/p/ptotocol-buffers/","title":"Ptotocol Buffers"},{"content":" # go build vs go install The go install command builds and installs the packages named by the paths on the command line. Executables (main packages) are installed to the directory named by the GOBIN environment variable, which defaults to $GOPATH/bin or $HOME/go/bin if the GOPATH environment variable is not set.\nThe go install command behaves almost identically to go build, but instead of leaving the executable in the current directory, or a directory specified by the -o flag, it places the executable into the $GOPATH/bin directory.\n1 2 3 # check the $GOPATH ❯ go env GOPATH /Users/David/go Since go install will place generated executables into $GOPATH/bin, this directory must be added to the $PATH environment variable so that you can run the executables from the command line anywhere.\n# Commands commonly used in Go Modules go mod init github.com/your-username/your-repo-name (enabling dependency tracking in your code) To track and manage the dependencies you add, you begin by putting your code in its own module. This creates a go.mod file at the root of your source tree. Dependencies you add will be listed in that file.\ngo get (adding a dependency) The following describes a few examples.\nTo add all dependencies for a package in your module, run a command like the one below (\u0026quot;.\u0026quot; refers to the package in the current directory):\n1 $go get . To add a specific dependency, specify its module path as an argument to the command:\n1 $ go get github.com/example/xxmodule go mod tidy go mod tidy ensures that the go.mod file matches the source code in the module. It adds any missing module requirements necessary to build the current module’s packages and dependencies, and it removes requirements on modules that don’t provide any relevant packages. It also adds any missing entries to go.sum and removes unnecessary entries.\nReferences:\nGo Modules Reference - The Go Programming Language Managing dependencies - The Go Programming Language ","date":"2024-01-18T22:12:20Z","permalink":"https://blog.yorforger.cc/p/go-modules-and-environment/","title":"Go Modules and Environment"},{"content":" # Store string The most commonly used string data types in the context of databases are CHAR and VARCHAR. TEXT and LONGTEXT are also commonly used string data types.\nchar(10) vs varchar(10) When you define a column as CHAR(10), it will always occupy 10 characters of storage, regardless of the actual data length. If you store a string shorter than 10 characters, it will be padded with spaces to fill up the remaining space.\nWhen you define a column as VARCHAR(10), you store a string shorter than 10 characters, it will use only the necessary amount of storage, without any padding.\nIn general, the performance difference between CHAR and VARCHAR is usually negligible unless you\u0026rsquo;re dealing with extremely large datasets or have specific performance requirements.\nWhen the length of strings to be written to the field is explicitly specified choose CHAR as the data type. When the number of strings that users will input is not fixed, but there is a limit based on the number of characters, use VARCHAR as the data type. For example, for a username that can vary in length, VARCHAR is used as the data type.\n# 存储时间 # 不要用字符串存储日期 字符串占用的空间更大 字符串存储的日期比较效率比较低（逐个字符进行比对），无法用日期相关的 API 进行计算和比较 # Datetime and Timestamp Datetime 和 Timestamp 是 MySQL 提供的两种比较相似的保存时间的数据类型, 通常我们都会首选 Timestamp. 因为DateTime类型没有时区信息的, 而Timestamp可以存储time zone信息, 并且做转换.\nTimestamp 只需要使用 4 个字节的存储空间，但是 DateTime 需要耗费 8 个字节的存储空间。但是，这样同样造成了一个问题，Timestamp 表示的时间范围更小。\nDateTime ：1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 Timestamp： 1970-01-01 00:00:01 ~ 2037-12-31 23:59:59 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 CREATE TABLE `time_zone_test` ( `id` int NOT NULL AUTO_INCREMENT, `date_time` datetime DEFAULT NULL, `time_stamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ); INSERT INTO time_zone_test(date_time) VALUES(NOW()); select * from time_zone_test; +----+---------------------+---------------------+ | id | date_time | time_stamp | +----+---------------------+---------------------+ | 1 | 2023-04-01 10:45:04 | 2023-04-01 10:45:04 | +----+---------------------+---------------------+ 这也说明了一个问题, 就是我们插入数据的时候, 没必要在逻辑上获取时间再加入, 我们只需要在创建表的时候设置一个time column并为其设置default值, 即可, 每次只用在Java代码中插入其他column, 然后时间会被mysql自动加上去.\n对于上面的数据, 我们修改会话的时区, 可以看到时间就变了:\n1 2 3 4 5 6 7 set time_zone=\u0026#39;+8:00\u0026#39;; +----+---------------------+---------------------+ | id | date_time | time_stamp | +----+---------------------+---------------------+ | 1 | 2023-04-01 10:45:04 | 2023-04-01 21:45:04 | +----+---------------------+---------------------+ # 查看 MySQL Warning 有时候我们创建表的时候或者执行SQL语句, 虽然执行成功了但是会显示有警告,但是还不告诉你警告内容, 这时候你需要立刻执行SHOW WARNINGS;语句, 否则你执行了其他语句再执行这个show, 那现实的就不是上一个语句的warnings了, 如下图:\n# Default vs NOT NULL 有没有想过, 建表的时候default 和 not null一起使用, 是不是有点redundant? 因为比如你不插入值的时候mysql会帮你插入默认值,\n其实这么想你就错了, 你想的是我不插入, mysql就会帮我插入个默认值, 所以似乎not null没起作用, 但是你有没有想过如果你只设置了default而没有设置not null限制, 那这时候我插入null个呢, 显然可以插入成功, 但有时候为null, 比如一个日期, 当我们在写Java或者其他代码的时候查询数据然后把date转为string, 如果数据为null可能就会发生异常~\n# Naming conventions General\nUsing lowercase will help speed typing, avoid mistakes as MYSQL is case sensitive.\nSpace replaced with Underscore — Using space between words is not advised.\nNumbers are not for names — While naming, it is essential that it contains only Alpha English alphabets.\nTable\nTable names are lower case, uses underscores to separate words, and are singular (e.g. foo, foo_bar, etc. Columns\nAlways use the singular name. Always use lowercase except where it may make sense not to such as proper nouns. Where possible avoid simply using id as the primary identifier for the table. Do not add a column with the same name as its table and vice versa. I generally (not always) have a auto increment PK. I use the following convention: tablename_id (e.g. foo_id, foo_bar_id, etc.). 参考:\nMYSQL Naming Conventions. What is MYSQL? | by Centizen Nationwide | Medium Is there a naming convention for MySQL? - Stack Overflow SQL style guide by Simon Holywell MySQL数据库中常见的几种表字段数据类型 - 掘金 老生常谈！数据库如何存储时间？你真的知道吗？ - 掘金 ","date":"2024-01-10T23:06:36Z","permalink":"https://blog.yorforger.cc/p/practice-of-mysql/","title":"Practice of MySQL"},{"content":" # Denormalization # Normalization 首先，了解什么是 Normalization 很重要。在关系型数据库设计中，Normalization 是一个组织数据的过程，目的是减少数据冗余和提高数据完整性。它涉及将数据分解成多个相互关联的表。这种设计减少了数据的重复，但通常会导致更复杂的查询，因为需要多个JOIN操作来重建原始信息。\n# Denormalization Denormalization 是 Normalization 的对立面。它涉及将数据从多个表合并到一个表中，有时通过添加冗余数据来实现。在非关系型数据库，如MongoDB中，Denormalization 通常表现为：\n嵌入子文档：将相关的数据直接嵌入到一个文档中，而不是将它们分散到多个集合（表）中。例如，而不是在单独的集合中维护用户地址，可以将地址作为子文档直接嵌入到用户文档中。\n使用数组：在文档中使用数组来存储相关项的列表。例如，一个产品文档可能包含一个评论的数组，而不是将评论存储在一个单独的集合中。\n# Denormalization 的优点和缺点 优点：\n提高查询性能：由于相关数据更紧密地存储在一起，因此减少了查询所需的JOIN操作或跨文档查找。 简化查询逻辑：数据模型更直观，容易理解和操作，特别是对于文档数据库。 缺点：\n数据冗余(Data Redundancy)：相同的数据在多个地方重复，可能导致数据同步问题。 更新操作复杂化(Complexity in Updates)：当需要更新冗余数据时，可能需要在多个地方进行更新，增加了复杂性。 # 结论 在MongoDB这样的非关系型数据库中，反规范化是一种常见的数据建模技术，特别适用于读取操作远多于写入操作的场景。它通过牺牲一定程度的数据冗余来换取读取性能的提升和查询逻辑的简化。然而，设计时需要平衡冗余带来的管理复杂性和性能优势。\n# Join \u0026amp; Foreign Key Join: MySQL: JOINS are easy (INNER, LEFT, RIGHT) Foreign Key is used to ensure the consistency and integrity of data. 既然有了 join, 为什么还需要 foreign key: https://chat.openai.com/share/419afe95-279b-477f-8afa-8f75ab76edad MongoDB 没有 join 和 foreign key 的的概念, 但是可以通过嵌套文档来实现类似 Join 的功能, 以及使用 Reference 来实现类似 Foreign Key 的功能.\nJOIN操作经常利用外键来连接两个表, 虽然JOIN操作不一定要求存在外键约束, 但外键为JOIN提供了自然的连接点, 如下例子:\nUsers表 存储用户信息：UserID (用户ID，主键), UserName (用户名) Orders表 存储订单信息：OrderID (订单ID，主键) OrderDate (订单日期) UserID (用户ID，外键) 在这个情况下，Orders.UserID 是一个外键，它指向Users.UserID。这意味着每个订单都与一个特定的用户相关联，外键保证了每个订单中的UserID都对应于一个有效的用户。假设我们想获取订单信息以及下单的用户的名称。我们可以使用以下SQL查询：\n1 2 3 SELECT Users.UserName, Orders.OrderID, Orders.OrderDate FROM Orders JOIN Users ON Orders.UserID = Users.UserID; 这个查询中：\nJOIN Users ON Orders.UserID = Users.UserID这一句是JOIN的核心，它说明了如何连接这两个表。我们通过Orders表中的UserID（外键）与Users表中的UserID（主键）进行匹配。 由于使用了JOIN，我们可以同时从Orders表和Users表中选择数据。因此，我们能够在同一个查询结果中同时看到用户的名字和他们的订单信息。 # One to Many \u0026amp; Many to Many 一对多关系：通过在“多”的一方表中设置外键指向“一”的一方的主键来实现。 多对多关系：通过创建一个额外的关联表，其中包含指向两个相关表主键的外键来实现。 在一对多关系中，一个记录在一个表中对应着多个记录在另一个表中。这种关系通常通过在“多”的一方使用外键（Foreign Key）来实现。如 Posts 表和 Comments 表, 此时 Comments 表中的 PostID 就是一个外键, 它指向 Posts 表中的 PostID. 通过这个外键, 我们可以追踪哪个评论属于哪个帖子, 从而实现一对多关系。\n多对多关系是指两个表中的记录可以相互关联, 在关系型数据库中, 这种关系通常通过第三个表（称为关联表或连接表）来实现，这个表包含了两个表的外键. 假设有两个表：Students 和 Courses:\nStudents表 存储学生的信息，如：\nStudentID (主键) StudentName Courses表 存储课程的信息，如：\nCourseID (主键) CourseName Enrollments表 作为关联表，存储学生和课程之间的关系，如：\nEnrollmentID (主键) StudentID (外键，指向Students表) CourseID (外键，指向Courses表) 在这个例子中，每个学生可以选修多个课程，同时每个课程也可以被多个学生选修。通过Enrollments表，我们可以追踪哪个学生选修了哪个课程，从而实现多对多关系。\n# One-to-Many and Many-to-Many in NoSQL MongoDB # One-to-Many 一对多关系在MongoDB中通常有两种表示方式：\n嵌入文档（Embedded Documents）:\n在这种方法中，\u0026lsquo;多\u0026rsquo;的部分作为子文档嵌入到\u0026rsquo;一\u0026rsquo;的部分中。 例如，如果一个用户有多个地址，那么地址可以直接嵌入到用户文档中。 示例:\n1 2 3 4 5 6 7 8 { \u0026#34;_id\u0026#34;: \u0026#34;userId123\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;addresses\u0026#34;: [ {\u0026#34;street\u0026#34;: \u0026#34;123 Apple St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;New York\u0026#34;}, {\u0026#34;street\u0026#34;: \u0026#34;456 Orange Ave\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Boston\u0026#34;} ] } 引用（References）:\n在这种方法中，\u0026lsquo;多\u0026rsquo;的一方会包含指向\u0026rsquo;一\u0026rsquo;方的引用（通常是ID）。 这类似于关系型数据库中的外键 示例:\n1 2 3 4 5 6 7 8 9 10 11 // User document { \u0026#34;_id\u0026#34;: \u0026#34;userId123\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34; } // Address documents [ {\u0026#34;_id\u0026#34;: \u0026#34;addressId1\u0026#34;, \u0026#34;userId\u0026#34;: ObjectID(\u0026#34;userId123\u0026#34;), \u0026#34;street\u0026#34;: \u0026#34;123 Apple St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;New York\u0026#34;}, {\u0026#34;_id\u0026#34;: \u0026#34;addressId2\u0026#34;, \u0026#34;userId\u0026#34;: ObjectID(\u0026#34;userId123\u0026#34;), \u0026#34;street\u0026#34;: \u0026#34;456 Orange Ave\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Boston\u0026#34;} ] Looking at how you model users and orders illustrates another common relationship: one-to-many. That is, every user has many orders. In an RDBMS, you’d use a foreign key in your orders table; here, the convention is similar. Learn more: Action in MongoDB: 4.2.2 Users and orders (one-to-many)\n# Many-to-Many 多对多关系在MongoDB中通常通过引用来表示, 每个文档存储与之相关联的其他文档的ID, an array of object IDs.\n了解更多: MongoDB in Action: 4.2.1 Many-to-many relationships\n# 示例 假设有学生和课程，每个学生可以选修多门课程，每门课程也可以由多个学生选修。\n学生文档可能包含它们所选课程的ID列表。\n1 2 3 4 5 { \u0026#34;_id\u0026#34;: \u0026#34;studentId1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;courses\u0026#34;: [\u0026#34;courseId1\u0026#34;, \u0026#34;courseId2\u0026#34;] } 课程文档可能包含选修该课程的学生ID列表。\n1 2 3 4 5 { \u0026#34;_id\u0026#34;: \u0026#34;courseId1\u0026#34;, \u0026#34;courseName\u0026#34;: \u0026#34;Mathematics\u0026#34;, \u0026#34;students\u0026#34;: [\u0026#34;studentId1\u0026#34;, \u0026#34;studentId3\u0026#34;] } 嵌入文档可以提高读取性能，因为所有相关数据都在一个文档内；而引用更灵活，可以更容易地维护大量动态关联数据。\n","date":"2024-01-10T11:51:35Z","permalink":"https://blog.yorforger.cc/p/basic-mysql/","title":"Basic MySQL"},{"content":" # setState 同步 OR 异步 我们首先需要明确, 从 API 层面上说, 它就是普通的调用执行的函数, 自然是同步的. 这里所说的同步和异步指的是 API 调用后更新 DOM 是同步还是异步的.\n如果 setState 在 React 能够控制的范围被调用，它就是异步的。 例如：合成事件处理函数, 生命周期函数, 此时会进行批量更新, 也就是将状态合并后再进行 DOM 更新。 如果 setState 在原生 JavaScript 控制的范围被调用，它就是同步的。 例如：原生事件处理函数中, 定时器回调函数中, Ajax 回调函数中, 此时 setState 被调用后会立即更新 DOM 。 一文彻底搞懂React的setState是同步还是异步 - 掘金\nReact 通过异步更新状态，可以批量处理多个状态更新，而不是对每个状态更新都重新渲染组件。这种批处理方式减少了不必要的渲染次数，提高了应用的性能。(batch multiple state updates together, instead of re-rendering components for each individual state update.)\n# setState 的坑 刚开始很容易写出以下错误代码:\n1 2 3 4 5 const [age, setAge] = useState(0); function handleClick() { setAge(() =\u0026gt; age + 1); // ... 这样访问准确来说并不是上一次的state, 而是组件在它当时状态值, 因为有可能发生多次连续的状态更新, 比如其它 event handler 也调用了 setAge 函数, 不要忘了这些 setXXX() 是异步的, 会被React集中到一起执行, 所以要确保 setAge() 更新的是上一次的值, 要使用（即 setXXX(previousState =\u0026gt; newState)），这样可以确保你的更新基于最新的状态值。\n正确方式如下:\n1 2 3 4 5 const [age, setAge] = useState(0); function handleClick() { setAge(age =\u0026gt; age + 1); // ... 注意第二个 setXXX() 的参数是一个匿名函数, 这个匿名函数的参数是上一次的 state, 由 React 负责传入, 返回值更新后的 state,\n# 组件渲染过程 This process of requesting and serving UI has three steps:\nTriggering a render A render is triggered when a component’s props or state changes. A render can also be triggered by its parent component re-rendering. 这里注意, 使用自定义 hook 时, 其返回的 state 变化时, 也会触发组件的重新渲染. 并不是自定义 hook 的状态改变而导致的重新渲染, 而是自定义 hook 返回的 state 改变而导致的重新渲染. 自定义 Hooks 的主要优势在于它们允许你重用状态逻辑，而不是创建全新的状态机制。每次在组件中使用自定义 Hook 时 (若该 hook 中定义了 state const [xxx, setXXX] = useState(xx)) ，都相当于在该组件内部创建了一个新的独立状态 xxx, 因此当 hook 中调用 setXXX 时, 会触发该组件的重新渲染.\nRendering the component\nThe process of rendering is recursive. React commits changes to the DOM\nReact will apply the minimal necessary operations (calculated while rendering!) to make the DOM match the latest rendering output. Think the clock and input example. The default behavior of rendering all components nested within the updated component is not optimal for performance if the updated component is very high in the tree. If you run into a performance issue, there are several opt-in ways to solve it described in the Performance section. Don’t optimize prematurely!\nWhen developing in “Strict Mode”, React calls each component’s function twice, which can help surface mistakes caused by impure functions. Source\nLearn more: Render and Commit – React\n# 组件渲染会导致异步函数暂停吗 React组件的重新渲染不会导致其中正在执行的异步函数暂停或中断,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 function MyComponent() { console.log(\u0026#39;组件开始渲染\u0026#39;); // 异步函数 setTimeout(() =\u0026gt; { console.log(\u0026#39;异步函数执行\u0026#39;); }, 0); console.log(\u0026#39;组件渲染中\u0026#39;); // 组件渲染的返回 return \u0026lt;div\u0026gt;Hello World\u0026lt;/div\u0026gt;; } // 当MyComponent被渲染时 ReactDOM.render(\u0026lt;MyComponent /\u0026gt;, document.getElementById(\u0026#39;root\u0026#39;)); 在这个例子中，当MyComponent组件开始渲染时，它首先打印\u0026quot;组件开始渲染\u0026quot;。接下来，尽管有一个setTimeout异步函数，它不会立即执行。相反，组件会继续执行下一行同步代码，打印\u0026quot;组件渲染中\u0026quot;。只有在所有同步代码执行完毕后，即组件渲染完成后，事件循环才会处理setTimeout中的异步回调，打印\u0026quot;异步函数执行\u0026quot;。\n因此，在React中，组件的渲染过程是不会被其中的异步函数所打断的。同步代码总是在异步代码之前执行完毕。这保证了组件的渲染逻辑的一致性和可预测性。\n","date":"2024-01-07T19:27:22Z","permalink":"https://blog.yorforger.cc/p/rendering-react/","title":"Rendering - React"},{"content":" # State When you call useState, you are telling React that you want this component to remember something:\n1 const [index, setIndex] = useState(0); In this case, you want React to remember index.\nState is tied to a position in the render tree: When you give a component state, you might think the state “lives” inside the component. But the state is actually held inside React. React associates each piece of state it’s holding with the correct component by where that component sits in the render tree. source\nHere’s how that happens in action:\n1 const [index, setIndex] = useState(0); Your component renders the first time. Because you passed 0 to useState as the initial value for index, it will return [0, setIndex]. React remembers 0 is the latest state value. You update the state. When a user clicks the button, it calls setIndex(index + 1). index is 0, so it’s setIndex(1). This tells React to remember index is 1 now and triggers another render. Your component’s second render. React still sees useState(0), but because React remembers that you set index to 1, it returns [1, setIndex] instead. State is local to a component instance on the screen. In other words, if you render the same component twice, each copy will have completely isolated state! Changing one of them will not affect the other.\nLearn more:\nState as a Snapshot – React Queueing a Series of State Updates – React # 什么是副作用 Anything that results in changes that can be observed when the calculation is completed, beside the return value of the calculation itself, is a side effect. Calculation should return just result of the calculation. Anything else is side-effect. source\nComponents should only return their JSX, and not change any objects or variables that existed before rendering—that would make them impure!\nWhile functional programming relies heavily on purity, at some point, somewhere, something has to change. That’s kind of the point of programming! These changes—updating the screen, starting an animation, changing the data (call setState)—are called side effects. They’re things that happen “on the side”, not during rendering.\n所以严格来说 console.log 也是副作用.\nIn React, side effects usually belong inside event handlers. Event handlers are functions that React runs when you perform some action—for example, when you click a button. Even though event handlers are defined inside your component, they don’t run during rendering! So event handlers don’t need to be pure.\nIf you’ve exhausted all other options and can’t find the right event handler for your side effect, you can still attach it to your returned JSX with a useEffect call in your component. This tells React to execute it later, after rendering, when side effects are allowed. However, this approach should be your last resort.\n修改或读取外部变量, 修改 props 是不被允许的, 在哪都不行, 你可以把 外部变量通过 props 传进来, 然后在组件内部读取.\nLearn more: Keeping Components Pure – React\n# 函数组件无副作用 # Render logic must not Same inputs, same output: 函数组件的输出（即渲染的UI）仅依赖于它的props和state。换句话说，给定相同的props和state，组件将始终渲染相同的输出。\n不改变外部状态: 在理想情况下，函数组件不应该改变外部状态，不应该直接修改传入的props或外部的全局变量。\n注意 props, 和 外部的变量 就是不允许被修改的, 即使在 event handler 中, 也不能修改, 这是 React 的设计哲学 state 是可以被修改的, 但是只能通过 setState 在 event handler 或一些 hooks中修改, 不能在渲染逻辑中修改, 否则会导致死循环. 官方文档有例子, 在文章最后: https://react.dev/learn/keeping-components-pure The React philosophy is that props should be immutable and top-down. This means that a parent can send whatever prop values it likes to a child, but the child cannot modify its own props. What you do is react to the incoming props and then, if you want to, modify your child\u0026rsquo;s state based on incoming props. source\n无副作用操作: 函数组件在其主体内(渲染逻辑中)不应该执行有副作用的操作，比如直接进行网络请求、订阅事件、直接操作DOM等。这些操作应该放在 event handlers 或钩子（如useEffect）中。 Can event handlers have side effects? Absolutely! Event handlers are the best place for side effects. Unlike rendering functions, event handlers don’t need to be pure, so it’s a great place to change something—for example, change an input’s value in response to typing, or change a list in response to a button press. However, in order to change some information, you first need some way to store it. In React, this is done by using state, a component’s memory. You will learn all about it on the next page. source\n# Render logic may Mutate objects that were newly created while rendering 1 2 3 4 5 const MyComponent = () =\u0026gt; { const user = { name: \u0026#39;Alice\u0026#39; }; user.name = \u0026#39;Bob\u0026#39;; // Mutating a local object is fine return \u0026lt;div\u0026gt;{user.name}\u0026lt;/div\u0026gt;; }; Throw errors 1 2 3 4 5 6 const MyComponent = ({ id }) =\u0026gt; { if (!id) { throw new Error(\u0026#39;ID is required\u0026#39;); } return \u0026lt;div\u0026gt;ID: {id}\u0026lt;/div\u0026gt;; }; Strive to express your component’s logic in the JSX you return. When you need to “change things”, you’ll usually want to do it in an event handler. As a last resort, you can useEffect. source\nIt is useful to remember which operations on arrays mutate them, and which don’t. For example, push, pop, reverse, and sort will mutate the original array, but slice, filter, and map will create a new one. source\n","date":"2024-01-07T10:27:22Z","permalink":"https://blog.yorforger.cc/p/pure-function-state-event-loop-react/","title":"Pure Function \u0026 State \u0026 Event Loop - React"},{"content":" # 1. flex-none 由于 flex items 三属性 flex-grow, flex-shrink, flex-basis 的默认值分别是 0, 1, auto, 即 flex items 默认允许在必要情况下缩小, 比如有左右两个 flex items, 当左边的 flex items 的内容很多, 而右边的 flex items 的内容很少, 此时右边的 flex items 会被压缩, 以适应 flex container 的宽度, 即使右边的 flex items 设置了宽度, 也会被压缩.\n这就导致有时候会遇到明明设置了宽度, 却“不起作用”的情况, 其实就是被压缩了, 这时候就需要使用 flex-none (0 0 auto) 来禁止 flex items 在必要情况下缩小, 从而保证 flex items 的宽度不会被压缩.\n如下, 若没有 flex-none, 因为段落内容较多, 会导致image被压缩, 即使设置了宽度,\n1 2 3 4 5 6 7 8 \u0026lt;div className={\u0026#39;flex items-start p-3 max-h-80 border-b-2\u0026#39;}\u0026gt; \u0026lt;a href={`#`} className=\u0026#34;flex-none\u0026#34;\u0026gt; \u0026lt;img src={\u0026#39;#\u0026#39;} alt=\u0026#34;User avatar\u0026#34; className=\u0026#34;rounded-full h-11 w-11\u0026#34;/\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;p className={\u0026#39;mt-2\u0026#39;}\u0026gt; Compound indexes collect and sort data from two or more fields in each document in a collection. Data is grouped by the first field in the index and then by each subsequent field. \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ","date":"2024-01-06T22:35:10Z","permalink":"https://blog.yorforger.cc/p/tailwind-css-tricks/","title":"Tailwind CSS Tricks"},{"content":" # Block and Inline block 元素独占一行, 宽度默认是父元素的 100%, 高度由内容决定\n特殊情况: button 默认是 inline-block, 支持设置宽高, 即使把 button 的 display 设置为 block, 其宽度依然不会是父元素的 100%, 而是由内容决定的. 其他的 inline 元素, 如 \u0026lt;span\u0026gt;, \u0026lt;a\u0026gt;, \u0026lt;strong\u0026gt;, 如果设置为 block, 则其宽度会自动变为父元素的 100%. inline 元素不会独占一行, 宽高度由内容决定\n不能设置宽高, 若要设置宽和高, 需要将其转换为块级元素. 如 display: block 或 display: inline-block. 不可以设置上下 padding 和 margin, 可以设置左右 padding 和 margin. 特殊情况: margin-left: auto, margin-right: auto 无法适用 inline 元素. 这也是为什么 tailwind css 中 mx-auto 使 inline 元素水平居中的原因. inline-block, 既可以设置宽高, 也可以设置所有 padding 和 margin, 其他与 inline 元素一样.\nflex 子元素既不是块级元素, 也不是内联元素, 而是 flex 元素, 有自己的特性. 若同在 flex box内, 则 \u0026lt;a\u0026gt; 与 \u0026lt;div\u0026gt; 在显示上是完全一样的, 因为他们都是 flex items. 注意 flex 子元素的默认值是 flex: 0 1 auto, 即 flex-grow: 0, flex-shrink: 1, flex-basis: auto, 即默认情况下, flex 子元素是允许在必要情况下缩小的, 而不会增长, 从而导致 flex 子元素的宽度不是父元素的 100%, 而是由内容决定的.\n# Flex Flex 有很多性质, 要区分哪些性质是用到 Flex Box 上的, 哪些是用到 Flex Items 上的. 可直接观看: https://youtu.be/fYq5PXgSsbE?si=yeeW9PDx-Als9CWX\n# flex box flex-direction: row | row-reverse | column | column-reverse;\nTailwind CSS: flex-row, flex-col justify-content: flex-start | flex-end | center | space-between | space-around | space-evenly;\nTailwind CSS: justify-start, justify-end, justify-center, justify-between, align-items: flex-start | flex-end | center\nTailwind CSS: items-start, items-end, items-center 常用技巧:\njustify-between 主要是用来设置 flex items 之间的间距, 一般是让两个元素分布在容器左右或上下两端, 聊天软件, 自己信息靠右, 对方信息靠左, 不可以使用 justify-self 或 align-self, 前者是用于 Grid 布局的, 后者是用于 Flex 布局的交叉轴上的对齐方式. 更不可以单独对一个元素设置 justify-start 或 justify-end, 因为这是 flex box 的属性, 会影响所有的 flex items, 而不是单个元素. 你可以使用 margin-left: auto 或 margin-right: auto 来实现这个效果. mx-auto 用于水平居中, my-auto 用于垂直居中, 也可使用 order 属性来调整元素的顺序, 配合 justify-between 使用 (控制在两端, 一左一右), 只有两个元素这种简单情况下推荐使用. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 export default function Message({role, text, time}) { const messageClass = role === \u0026#39;bot\u0026#39; ? \u0026#39;mr-auto bg-gray-300 text-black\u0026#39; : \u0026#39;ml-auto bg-blue-300 text-white\u0026#39; const timePos = role === \u0026#39;bot\u0026#39; ? \u0026#39;order-2\u0026#39; : \u0026#39;order-1\u0026#39; const rolePos = role === \u0026#39;bot\u0026#39; ? \u0026#39;order-1\u0026#39; : \u0026#39;order-2\u0026#39; const sentTime = new Date(time).toLocaleString() return ( \u0026lt;div className={messageClass} \u0026gt; \u0026lt;div className={\u0026#39;flex flex-row justify-between\u0026#39;}\u0026gt; \u0026lt;span className={rolePos}\u0026gt;{role}\u0026lt;/span\u0026gt; \u0026lt;span className={timePos}\u0026gt;{sentTime}\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;p\u0026gt;{text}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ) } # flex items flex-grow: 用来指定一个 flex 子项（flex item）相对于其他子项在可用空间中的扩展比例, 沿主轴方向。 Tailwind CSS 类名:flex-grow, flex-grow-0 align-self: 用来指定flex子项在交叉轴上的对齐方式, 会覆盖父容器的 align-items 属性。 很好理解, align-self 是用来指定单个 flex item 在交叉轴上的对齐方式, 而 align-items 是用来指定所有 flex items 在交叉轴上的对齐方式. 前者用在 item上, 后者用在 flex box 上, 前者会覆盖后者. Tailwind CSS 类名: self-auto, self-start, self-center, self-end, self-stretch 了解更多: Align Self - Tailwind CSS 常用技巧-1: 把 navigator 和 footer 设置为 flex-grow: 0, 然后中间的内容设置为 flex-grow: 1, 这样中间的内容会占据剩余的空间, 从而实现中间内容自动填充剩余空间的效果. (前提是他们都是 flex items, 即他们都在 flex box 内)\n常用技巧-2: 通常把元素设置为 flex-grow 后, 会自动填充剩余空间, 但如果该元素的内容很多, 就会导致溢出, 比如对话框, 此时应该会想到加个 overflow 属性, 但是这样并不会起作用, 原因是你没设置该元素的高度, 此时可以给个高度0, 这样即使内容为空因为 flex-grow 会自动填充剩余空间, 若内容溢出因为定义了高度, overflow 属性就会起作用了. 了解更多: https://stackoverflow.com/a/14964944/16317008\n易错点-1: flex-grow 是 flex items 的属性, 用来控制 flex tiems 如何在主轴方向上增长, 不要简单的以为是用来控制在水平方向上增长的. 比如在 flex-col 的情况下，flex-grow 影响的是items的高度，而不是宽度，因为主轴是垂直的.\n易错点-2: justify-items 和 justify-self 专门用于Grid布局, 不用于Flex布局, 了解更多: Justify Items - Tailwind CSS\n# Position # absolute 最近写菜单栏的时候, 菜单栏展开后会将下面的内容挤下去, 像下面这样:\n基本上菜单栏都很窄, 所以当菜单栏展开时, 我希望的是不影响下面的内容, 但是正常情况下两个元素要么是 inline 那种可以在同一行, 要么是 block 那种独占一行, 无法实现我想要的效果. 此时就可以使用 absolute 来实现, 如下:\nAbsolute positioned elements are removed from the normal flow, and can overlap elements.\n此时也应该注意覆盖的问题, 若对话框宽度为 100vw, 可能会覆盖掉对话框, 因此要将 z-index 设置大一些.\n","date":"2024-01-06T20:46:20Z","permalink":"https://blog.yorforger.cc/p/tailwind-css-flex/","title":"Tailwind CSS Flex"},{"content":" # 1. Main axis and cross axis - Flex 1 2 3 4 5 6 7 \u0026lt;div className=\u0026#34;flex flex-col\u0026#34;\u0026gt; ... \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;flex flex-row\u0026#34;\u0026gt; ... \u0026lt;/div\u0026gt; 为什么使用 class=\u0026ldquo;flex flex-row\u0026rdquo; 只是用 class=\u0026ldquo;flex-row\u0026rdquo; 不行吗? flex 确保元素变成一个 Flex 容器。flex-row 确保该容器内的项目沿着水平轴排列。\n另外注意, flex items 会自动填充满其 flex container 的沿着交叉轴方向的空间, 也就是说, flex items 的宽度或高度会等于交叉轴的长度, 具体是高度还是宽度, 取决于主轴方向, 因为交叉轴方向就是主轴方向的垂直方向.\n根据下面这个例子, 因为主轴方向是水平, 因此 flex items 的高度会等于 flex container 的高度, 至于宽度, 则是由其内容决定的 (flex-grow 默认值为 0).\n1 2 3 4 \u0026lt;div className=\u0026#34;flex flex-row h-64 border-2\u0026#34;\u0026gt; \u0026lt;button className={\u0026#39;border-2\u0026#39;}\u0026gt;hello\u0026lt;/button\u0026gt; \u0026lt;button className={\u0026#39;border-2\u0026#39;}\u0026gt;hello\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; flex items 的三个属性 flex-grow, flex-shrink, flex-basis 的默认值是 flex-grow: 0, flex-shrink: 1, flex-basis: auto. 注意这是 flex items 的属性, 不是 flex container 的属性.\n# 2. 水平和垂直居中 - Flex # 2.1 水平居中 对于 flex-row 主轴就是水平方向, 简单使用 justify-center (沿主轴方向操作 flex items) 即可使 flex items 水平居中.\n1 2 3 \u0026lt;div className=\u0026#34;flex flex-row justify-center\u0026#34;\u0026gt; ... \u0026lt;/div\u0026gt; 对于 flex-col 主轴就是垂直方向, 使用 items-center (沿交叉轴方向操作 flex items) 即可使 flex items 水平居中.\n1 2 3 4 \u0026lt;div className=\u0026#34;flex flex-col items-center h-64 border-2\u0026#34;\u0026gt; \u0026lt;button className={\u0026#39;border-2\u0026#39;}\u0026gt;hello\u0026lt;/button\u0026gt; \u0026lt;button className={\u0026#39;border-2\u0026#39;}\u0026gt;hello\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; 你也可以使用 mx-auto, 但 mx-auto 只作用于块级元素, 因此你可能需要把 flex items 改为块级元素, 别忘了 flex items 既不是块级元素也不是行内元素, 你需要将其父的 display: flex 去掉.\n# 2.2 垂直居中 与上面类似, 不再赘述\n# 3. 宽度高度内边距外边距 宽度（Width）:\nTailwind 使用 w-{size} 的模式来设置宽度，其中 {size} 可以是具体的数值或者百分比。例如： w-1/2：元素宽度为容器宽度的 50%。 w-4：元素宽度为 1rem（默认情况下，所有的尺寸都基于 4 倍体系）。 w-full：元素宽度为 100%。 高度（Height）:\n高度的设置方式与宽度类似，使用 h-{size} 的格式。例如： h-10：高度为 2.5rem。 h-screen：高度为视口的高度。 内边距（Padding）:\n内边距使用 p-{size} 的格式，针对所有四个方向，或者使用 px-、py-、pt-、pb-、pl-、pr- 分别设置水平、垂直、上、下、左、右的内边距。例如： p-4：所有方向的内边距都是 1rem。 px-2：水平方向（左右）的内边距是 0.5rem。 外边距（Margin）:\n外边距的设置与内边距类似，使用 m-{size}，或者 mx-、my-、mt-、mb-、ml-、mr- 来分别设置。例如： m-5：所有方向的外边距都是 1.25rem。 mt-3：上方外边距是 0.75rem。 # 4. Breakpoints 在 Tailwind CSS 中，断点（breakpoints）用于创建响应式设计，允许在不同屏幕尺寸下应用不同的样式规则。以下是 Tailwind CSS 中一些常见断点的用法及其解释：\nsm（Small）: sm: 前缀用于小屏幕设备。例如，w-full md:w-1/2 : 小屏幕都会自动应用 w-full, 宽度大于 md 的, 宽度自动变为 w-1/2 ","date":"2024-01-06T12:46:22Z","permalink":"https://blog.yorforger.cc/p/tailwind-css-commonly-used-class/","title":"Tailwind CSS Commonly Used Class"},{"content":" # 1. Post Schema # 1.1. Likes Field The limit of the document in MongoDB is 16mb, which is pretty huge. Unless you\u0026rsquo;re trying to create the next Facebook, putting the IDs in the array won\u0026rsquo;t be an issue. I have production workflows with 10k+ IDs in a single document array, have no issues. source\n","date":"2024-01-04T17:51:35Z","permalink":"https://blog.yorforger.cc/p/design-of-database-schema/","title":"Design of Database Schema"},{"content":" # Aggregate framework The aggregation framework is MongoDB’s advanced query language, and it allows you to transform and combine data from multiple documents to generate new information not available in any single document.\n# Common aggregation stages Below is a list of common MongoDB aggregation pipeline stages and their typical order:\n$match Stage\nUsually at the beginning of the pipeline. Filters the documents as early as possible, which reduces the amount of data processed by subsequent stages. $sort Stage\nAfter $match and before $limit if sorting the entire dataset is necessary. Be aware that $sort can be memory-intensive. If used before $limit, it sorts the entire dataset filtered by $match. $limit Stage\nUse $limit as early as possible, but after $match and $sort if sorting is needed. $project Stage\nReduces the number of fields in the documents, decreasing the amount of data processed in later stages. Keep in mind that some fields might be needed in subsequent stages. You can use $project more than once in a pipeline, you have to speficify all the fields you want to keep in each $project stage, even you have already specified them in the previous $project stage. If you need to add new fields, consider using $addFields instead wich has more flexibility. $group Stage\nAfter $match, $sort, $limit, and $project. Groups documents by specified fields after they have been filtered and shaped. $unwind Stage\nUsed to expand array fields into separate documents. 1 2 3 4 5 6 7 db.inventory.insertOne({ \u0026#34;_id\u0026#34; : 1, \u0026#34;item\u0026#34; : \u0026#34;ABC1\u0026#34;, sizes: [ \u0026#34;S\u0026#34;, \u0026#34;M\u0026#34;, \u0026#34;L\u0026#34;] }) db.inventory.aggregate( [ { $unwind : \u0026#34;$sizes\u0026#34; } ] ) // The operation returns the following results: { \u0026#34;_id\u0026#34; : 1, \u0026#34;item\u0026#34; : \u0026#34;ABC1\u0026#34;, \u0026#34;sizes\u0026#34; : \u0026#34;S\u0026#34; } { \u0026#34;_id\u0026#34; : 1, \u0026#34;item\u0026#34; : \u0026#34;ABC1\u0026#34;, \u0026#34;sizes\u0026#34; : \u0026#34;M\u0026#34; } { \u0026#34;_id\u0026#34; : 1, \u0026#34;item\u0026#34; : \u0026#34;ABC1\u0026#34;, \u0026#34;sizes\u0026#34; : \u0026#34;L\u0026#34; } $lookup Stage\nOrder: After filtering and limiting stages like $match and $limit. Reason: Performs a join to another collection, which can be data-intensive. Considerations: Can significantly increase data volume, so use after reducing the dataset size. $addFields / $set Stage\nOrder: After $project, $unwind, or $group. Reason: Adds new fields or modifies existing ones, often based on existing fields. Considerations: Useful for adding calculated fields. Similar to $project, but adds fields without removing existing ones. The code below will result in an error \u0026ldquo;$size cannot be used for not array type\u0026rdquo;, because during the project stage filters out the likes and comments fields, which results the likes and comments fields to be null instead of an array.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { $project: { text: 1, images: 1, comments: 1, createdAt: 1, } }, { $addFields: { engagement: { numLikes: { $size: \u0026#34;$likes\u0026#34; }, numComments: { $size: \u0026#34;$comments\u0026#34; }, isLiked: { $in: [new mongoose.Types.ObjectId(userId), \u0026#34;$likes\u0026#34;] }, } } } So in this case, you should put the $addFields stage before the $project stage.\n","date":"2024-01-04T17:51:35Z","permalink":"https://blog.yorforger.cc/p/mongodb-aggregate/","title":"MongoDB Aggregate"},{"content":" # 1. Common used commands 1 2 3 4 5 6 $apt install nginx # ubuntu $apk add nginx # Alpine Linux # start $sudo systemctl start/stop/restart/status nginx $sudo service nginx start/stop/restart/status # 2. Basic configurations # 2.1. Configuration file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $vi /etc/nginx/http.d/default.conf # Alpine Linux server { listen 2087 ssl; server_name shaowenzhu.top www.shaowenzhu.top; ssl_certificate /root/tls/cert.pem; ssl_certificate_key /root/tls/cert.key; location / { root /var/www/html; try_files $uri $uri/ /index.html; } # You may need this to prevent return 404 recursion. location = /404.html { internal; } } You can get free TLS certificate from Cloudflare but you need get a domain first. Learn more: Coudflare TLS encryption 520 Error Code \u0026amp; too Many Redirections - David\u0026rsquo;s Blog\nBecause the 443 port is used by my file server, so I use 2087 port for my gbtbot server. I use Cloudflare for reverse proxy, Cloudflare only allows the following ports: 443, 2053, 2083, 2087, 2096, 8443. So I choose 2087 port for this app.\nDon\u0026rsquo;t forget allow 2087 port on your server firewall with command sudo ufw allow 2087.\n# 2.2. Static website Then access my website by https://shaowenzhu.top:2087/ will get the static html file under /var/www/html.\nWith Vite, you can use vite build to build your project, then copy the dist folder to /var/www/html.\n1 2 3 4 5 6 ➜ /etc tree -L 3 /var/www/html /var/www/html ├── assets │ └── index-r4STm7R-.js ├── index.html └── vite.svg ","date":"2023-12-30T14:01:50Z","permalink":"https://blog.yorforger.cc/p/nginx/","title":"Nginx"},{"content":" # 为什么 useEffect 不能是异步函数 异步函数隐式地返回一个Promise，而useEffect的设计是期望返回一个清理函数或者什么都不返回（undefined）。这个设计原则确保了React能够正确地处理副作用和它们的清理逻辑。\n如果你需要在useEffect中执行异步操作，正确的做法是在useEffect的函数体内定义一个异步函数，并在该函数内执行异步操作，然后立即调用这个异步函数。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 useEffect(() =\u0026gt; { async function fetchData() { // 异步操作 const data = await fetchSomeData(); // 使用数据更新状态 setState(data); } fetchData(); // 清理函数（如果需要） return () =\u0026gt; { // 清理逻辑 }; }, [/* 依赖列表 */]); 这种模式允许在useEffect内部使用异步操作，同时遵循React的副作用处理规则。\n函数组件在其主体内不应该执行有副作用的操作，比如直接进行网络请求、订阅事件、直接操作DOM等。这些操作应该放在特定的生命周期方法或钩子（如useEffect）\n# 2. When useEffect is called What does useEffect do? By using this Hook, you tell React that your component needs to do something after render. React will remember the function setup you passed, and call it later after performing the DOM updates.\nuseEffect 是在渲染逻辑之后执行的, 是可以有副作用的. 可以看出, useEffect 天生就是为了解决这个问题.\n1 useEffect(setup, dependencies?) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 function MyComponent() { const [count, setCount] = useState(0); console.log(\u0026#34;1. Component function is running (render phase)\u0026#34;); useEffect(() =\u0026gt; { setTimeout(() =\u0026gt; { console.log(\u0026#34;3. useEffect is called (after DOM updates)\u0026#34;); }, 2000); }); return ( \u0026lt;div\u0026gt; \u0026lt;p\u0026gt;You clicked {count} times\u0026lt;/p\u0026gt; \u0026lt;button onClick={() =\u0026gt; setCount(count + 1)}\u0026gt; Click me \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; ); } When you reload the page, you will see the following in the console (Don\u0026rsquo;t use StrictMode):\n1 2 1. Component function is running (render phase) 2. useEffect is called (after DOM updates) # 2 seconds later will print When you click the button, you will see the following in the console:\n1 2 3 1. Component function is running (render phase) You clicked 1 times 3. useEffect is called (after DOM updates) # 2 seconds later will print Here\u0026rsquo;s a simplified sequence of what happens:\nComponent Function Executes: The function component runs. During this execution, it can render JSX and call hooks like useState. (Reload page, in example above)\nRender to DOM: React updates the DOM and the screen based on the returned JSX from the function. (\u0026lsquo;You click x times\u0026rsquo;, in example above)\nAfter Render: Once the rendering is complete and the DOM has been updated, useEffect is called. This is done asynchronously; it doesn\u0026rsquo;t block the browser from updating the screen. (2 seconds later, in example above)\n# 3. 副作用操作实例 fetch data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import { useState, useEffect } from \u0026#39;react\u0026#39;; import { fetchBio } from \u0026#39;./api.js\u0026#39;; export default function Page() { const [person, setPerson] = useState(\u0026#39;Alice\u0026#39;); const [bio, setBio] = useState(null); useEffect(() =\u0026gt; { fetchBio(person).then(result =\u0026gt; { setBio(result); }); }, [person]); return ( \u0026lt;\u0026gt; \u0026lt;select value={person} onChange={e =\u0026gt; { setPerson(e.target.value); }}\u0026gt; \u0026lt;option value=\u0026#34;Alice\u0026#34;\u0026gt;Alice\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;Bob\u0026#34;\u0026gt;Bob\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;Taylor\u0026#34;\u0026gt;Taylor\u0026lt;/option\u0026gt; \u0026lt;/select\u0026gt; \u0026lt;hr /\u0026gt; \u0026lt;p\u0026gt;\u0026lt;i\u0026gt;{bio ?? \u0026#39;Loading...\u0026#39;}\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/\u0026gt; ); } There is a trick, \u0026lt;p\u0026gt;\u0026lt;i\u0026gt;{bio ?? 'Loading...'}\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;, \u0026lsquo;Loading\u0026hellip;\u0026rsquo; will be used if bio is null or undefined. We write it in this way because useEffect is called after the initial rendering, so bio will be null or undefined at the first render.\n# 4. Race conditions in useEffect You would typically notice a race condition (in React) when two slightly different requests for data have been made, and the application displays a different result depending on which request completes first.\nWith cleanup function, we can \u0026ldquo;cancel\u0026rdquo; the previous request (because active is false, setData(newData); in the previous useEffect won\u0026rsquo;t be called), and only use the result of the latest request.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 useEffect(() =\u0026gt; { let active = true; const fetchData = async () =\u0026gt; { setTimeout(async () =\u0026gt; { const response = await fetch(`https://swapi.dev/api/people/${props.id}/`); const newData = await response.json(); if (active) { setFetchedId(props.id); setData(newData); } }, Math.round(Math.random() * 12000)); }; fetchData(); return () =\u0026gt; { active = false; }; }, [props.id]); Your setup code runs when your component is added to the page (mounts), this is useEffect gets called after the render phase. After every re-render of your component where the dependencies have changed: First, your cleanup code runs with the old props and state. Then, your setup code runs with the new props and state. Your cleanup code runs one final time after your component is removed from the page (unmounts). So, in the above example, the cleanup function of the previous useEffect call is called before the setup function of the current useEffect is called.\nLearn more: Race Condition useEffect\n","date":"2023-12-26T19:49:22Z","permalink":"https://blog.yorforger.cc/p/useeffect-hook-in-react/","title":"useEffect Hook in React"},{"content":" # 1. 对角线构图 动景比较适合, 比如沙滩的海浪, 海上一群海鸥连成线,\n(PS. 下面这张是网图\n另外也有对称线构图, 也可以找其他物体衬托, 主要看感觉,\n# 2. 留白构图 还没实践\u0026hellip;\n# 3. 其他分类 参考:\n6分钟让你学会构图，拍照萌新变摄影大师！（手机摄影和相机摄影构图技巧）_哔哩哔哩_bilibili ","date":"2023-12-20T22:50:37Z","permalink":"https://blog.yorforger.cc/p/%E6%8B%8D%E7%85%A7%E6%9E%84%E5%9B%BE%E6%8A%80%E5%B7%A7/","title":"拍照构图技巧"},{"content":" # 1. Create project with Vite 1 2 3 4 npm create vite@latest name-of-your-project -- --template react cd \u0026lt;your new project directory\u0026gt; npm install react-router-dom localforage match-sorter sort-by npm run dev/npm run build The Proxy in vite.config.js just works for development mode. In production mode it doesn\u0026rsquo;t work.\nThe production mode means you run npm run build which will generate a dist folder. Then you can deploy that folder into your server.\n# 2. Deploy to server 1 scp -rp dist/* root@129.18.30.20:/var/www/html/ Then follow this: Nginx - David\u0026rsquo;s Blog\n","date":"2023-12-20T19:44:22Z","permalink":"https://blog.yorforger.cc/p/vite-deployment-react/","title":"Vite \u0026 Deployment - React"},{"content":" # 1. Error handling with promise objects # 1.1. Rejection handler In JavaScript, when a promise is rejected, the control is passed to the nearest rejection handler. This is typically managed through .catch() blocks or through the rejection parameter in .then() method.\nThe error will be catght by the rejection parameter of the then() method: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 const promise1 = new Promise((resolve, reject) =\u0026gt; { setTimeout(() =\u0026gt; { reject(\u0026#39;Error in promise1\u0026#39;); }, 1000); }); const promise2 = promise1.then( value =\u0026gt; console.log(value), error =\u0026gt; { console.error(\u0026#39;Caught in first handler:\u0026#39;, error); // not re-throwing or returning another rejected promise, so flow goes to next then\u0026#39;s fulfillment handler } ).then( () =\u0026gt; console.log(\u0026#39;This will still execute.\u0026#39;), error =\u0026gt; console.error(\u0026#39;Caught in second handler:\u0026#39;, error) ).catch( error =\u0026gt; console.error(\u0026#39;Caught in catch:\u0026#39;, error) ); Output:\n1 2 Caught in first handler: Error in promise1 This will still execute. The error will be catght through .catch() blocks 1 2 3 4 5 6 7 8 9 10 11 12 13 14 const promise1 = new Promise((resolve, reject) =\u0026gt; { setTimeout(() =\u0026gt; { reject(\u0026#39;Error in promise1\u0026#39;); }, 1000); }); const promise2 = promise1.then( value =\u0026gt; console.log(value), ).then( () =\u0026gt; console.log(\u0026#39;This will still execute.\u0026#39;), error =\u0026gt; console.error(\u0026#39;Caught in second handler:\u0026#39;, error) ).catch( error =\u0026gt; console.error(\u0026#39;Caught in catch:\u0026#39;, error) ); Output:\n1 Caught in second handler: Error in promise1 # 1.2. Any errors will be caught before catch() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // any errors happen in main block, will be caught main().catch(err =\u0026gt; console.log(err)); async function main() { await simulateDatabaseConnection(); console.log(\u0026#39;Connected to the database successfully\u0026#39;) } async function simulateDatabaseConnection() { // Simulating a database connection attempt that could fail let isConnected = false; // Simulating an asynchronous operation using a Promise return new Promise((resolve, reject) =\u0026gt; { setTimeout(() =\u0026gt; { if (isConnected) { resolve(\u0026#39;Connected to the database successfully\u0026#39;); } else { reject(\u0026#39;Failed to connect to the database\u0026#39;); } }, 1000); // Simulating a delay of 1 second }); } // print: Failed to connect to the database. Another example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 fetch(\u0026#39;/article/promise-chaining/user.json\u0026#39;) .then(response =\u0026gt; response.json()) .then(user =\u0026gt; fetch(`https://api.github.com/users/${user.name}`)) .then(response =\u0026gt; response.json()) .then(githubUser =\u0026gt; new Promise((resolve, reject) =\u0026gt; { let img = document.createElement(\u0026#39;img\u0026#39;); img.src = githubUser.avatar_url; img.className = \u0026#34;promise-avatar-example\u0026#34;; document.body.append(img); setTimeout(() =\u0026gt; { img.remove(); resolve(githubUser); }, 3000); })) .catch(error =\u0026gt; alert(error.message)); Normally, such .catch doesn’t trigger at all. But if any of the promises above rejects (a network problem or invalid json or whatever), then it would catch it.\n# 2. try \u0026amp; catch This is similar to other languages, any errors happen in the try block will be caught by catch:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 const promise = new Promise((resolve, reject) =\u0026gt; { setTimeout(() =\u0026gt; { reject(\u0026#39;Error in promise\u0026#39;); }, 1000); }); async function exampleFunction() { try { await promise; console.log(\u0026#39;This line will not execute if promise rejects\u0026#39;); } catch (error) { console.error(\u0026#39;Error caught in async function:\u0026#39;, error); } } (async () =\u0026gt; { await exampleFunction() })() // Error caught in async function: Error in promise Error handling with promises\n","date":"2023-12-17T23:30:29Z","permalink":"https://blog.yorforger.cc/p/error-handling-javascript/","title":"Error Handling Javascript"},{"content":"Google Cloud Page: https://console.cloud.google.com/welcome/new?organizationId=0\nSelect your project or create a new one, then enable some APIs \u0026amp; Services for this project:\nI selected a already existed project Gemini-Test, now you can see its details as below:\nCheck enabled API \u0026amp; Services, you can click the enabled one then disable it:\nOr you can search some new APIs \u0026amp; Services and eanble it:\n","date":"2023-12-15T11:28:22Z","permalink":"https://blog.yorforger.cc/p/google-cloud-get-started/","title":"Google Cloud Get Started"},{"content":" # 1. Type Admin isn\u0026rsquo;t *Adimin I found an interesting question on stackoverflow, this is the code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type User interface { Name() string SetName(name string) } type Admin struct { name string } func (a *Admin) Name() string { return a.name } func (a *Admin) SetName(name string) { a.name = name } OP tries to copy user1\u0026rsquo;s value below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 func main() { var user1 User user1 = \u0026amp;Admin{name:\u0026#34;user1\u0026#34;} fmt.Printf(\u0026#34;User1\u0026#39;s name: %s\\n\u0026#34;, user1.Name()) var user2 User user2 = user1 user2.SetName(\u0026#34;user2\u0026#34;) fmt.Printf(\u0026#34;User2\u0026#39;s name: %s\\n\u0026#34;, user2.Name()) // print: \u0026#34;user2\u0026#34; fmt.Printf(\u0026#34;User1\u0026#39;s name: %s\\n\u0026#34;, user1.Name()) // print: \u0026#34;user2\u0026#34; too, How to make the user1 name does not change？ } Everything pass by value in Golang, even assignment will make a copy. Therefore, I tried something like this:\n1 2 3 4 5 var user1 User user1 = \u0026amp;Admin{name:\u0026#34;user1\u0026#34;} fmt.Printf(\u0026#34;User1\u0026#39;s name: %s\\n\u0026#34;, user1.Name()) var user2 User user2 = *user1 // got an error: Invalid indirect of \u0026#39;user1\u0026#39; (type \u0026#39;User\u0026#39;) The erorr says that user\u0026rsquo;s type is User not *User, so cannot dereference it. Why user1 = \u0026amp;Admin{name:\u0026quot;user1\u0026quot;} works fine?\nAn error occurred again below:\n1 2 3 var user1 User user1 = Admin{name:\u0026#34;user1\u0026#34;} // error: Cannot use \u0026#39;Admin{name:\u0026#34;user1\u0026#34;}\u0026#39; (type Admin) as the type User Type does not implement \u0026#39;User\u0026#39; as the \u0026#39;Name\u0026#39; method has a pointer receiver. This is confusing, it says type Admin doesn\u0026rsquo;t implement interface User. You may wonder, you lied, Admin has implemented all methods of User, but wait, really?\nIn fact, it\u0026rsquo;s not type Admin implementing interface User but type *Admin. Checking the definition of Name() and SetName() above, both of these two method have a pointer receiver, not a value receiver. This means the Name() and GetName() method are in the method set of the *Admin type, but not in that of Admin.\n# 2. Any types can implement an interface \u0026ldquo;Any types can implement an interface.\u0026rdquo; is not accurate.\nAny type can have its method set, and if there are all the methods of an interfacce in that type\u0026rsquo;s method set, we say this type implements this interface. This type can be a struct, function, map, array even an integer.\n# 2.1. Array Get multiple parameters from cli with flag package:\n1 func Var(value Value, name string, usage string) According to the docs, custom a type userFlag to implement interface flag.Value:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 type usersFlag []user func (u *usersFlag) String() string { return fmt.Sprintf(\u0026#34;%v\u0026#34;, *u) } func (u *usersFlag) Set(value string) error { values := strings.Split(value, \u0026#34;:\u0026#34;) if len(values) != 3 { return fmt.Errorf(\u0026#34;wrong format of auth, format: path:username:password\u0026#34;) } *u = append( *u, user{ username: values[1], password: values[2], path: formatPath(values[0]), }, ) return nil } Then you can use as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 type Param struct { users []user } func (p *Param) init() { var users usersFlag flag.Var(\u0026amp;users, \u0026#34;auth\u0026#34;, fmt.Sprintf(\u0026#34;-auth \u0026lt;path:username:password\u0026gt;\\n\u0026#34;+ \u0026#34;specify user for HTTP Basic Auth\u0026#34;)) flag.Parse() p.users = users } # 2.2. function There is another function called http.Handle() can be used to set routing info and callback, which has a different parameter compared with function http.HandleFunc():\n1 func Handle(pattern string, handler Handler) The second pamrameter is a Handler which is an interface type defined in http package:\n1 2 3 type Handler interface { ServeHTTP(ResponseWriter, *Request) } Function type customHandler implements interface http.Handler:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type customHandler func(http.ResponseWriter, *http.Request) func (mwh customHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { if r.Method != \u0026#34;POST\u0026#34; { http.Error(w, \u0026#34;method not supported\u0026#34;, http.StatusMethodNotAllowed) return } _, _ = fmt.Fprintf(w, \u0026#34;do something before custom handler mwh(w, r)\u0026#34;) mwh(w, r) } func main() { callback := customHandler(func(w http.ResponseWriter, r *http.Request) { _, _ = fmt.Fprintf(w, \u0026#34;hello there\u0026#34;) }) // customHandler implemented interface http.Handler, so its object can passed here http.Handle(\u0026#34;/hello\u0026#34;, callback) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } NOTE: for struct, when you gengerate a value (\u0026ldquo;object\u0026rdquo;) you should use curly brcket {} as below:\n1 2 3 4 5 6 type Cat struct { name string age int } cat := Cat{name: \u0026#34;Kitten\u0026#34;, age: 3} for a function type, we use parentheses to produce a value of that type:\n1 2 3 4 5 type myWebHandler func(http.ResponseWriter, *http.Request) callback := myWebHandler(func(w http.ResponseWriter, r *http.Request) { _, _ = fmt.Fprintf(w, \u0026#34;hello there\u0026#34;) }) # 3. Essence of interface variables An interface is conceptually a struct with two fields. If we were to describe an interface in Go, it would look something like this.\n1 2 3 4 type interface struct { Type uintptr // points to the type of the interface implementation Data uintptr // holds the data for the interface\u0026#39;s receiver } Type points to a structure that describes the type of the value that implements this interface. Data points to the value of the implementation itself. The contents of Data are passed as the receiver of any method called via the interface.\nThe Go memory model says that writes to a single machine word will be atomic, but interfaces are two word values. It is possible that another goroutine may observe the contents of the interface value while it is being changed.\nLearn more: https://dave.cheney.net/2014/06/27/ice-cream-makers-and-data-races\n","date":"2023-12-07T15:12:32Z","permalink":"https://blog.yorforger.cc/p/interfaces-basics-go/","title":"Interfaces Basics - Go"},{"content":"You should learn how to read the documentation provided Go, it\u0026rsquo;s very important:\n# 1. static language Go is statically typed. Every variable has only one static type, that is, exactly one type known and fixed at compile time: int, float32, *MyType, []byte, and so on. If we declare\n1 2 3 4 type MyInt int var i int var j MyInt then i has type int and j has type MyInt. The variables i and j have distinct static types and, although they have the same underlying type, they cannot be assigned to one another without a conversion.\nThis is also true for an interface:\n1 2 3 4 // Reader is an interface defined in io package type Reader interface { Read(p []byte) (n int, err error) } Statically typed means that before source code is compiled, the type associated with each and every single variable must be known.\n# 2. value \u0026amp; variable There is no object in Go, just variable and the value of a variable. We usually use variable and value as a same thing verbally.\nI think is true in Go/C++/C : A variable is just an adress location. When assignments happens str=\u0026quot;hello world\u0026quot; : Instead of telling the computer store this series of bits in 0x015c5c15c1c5, you tell him to store it in str. str is just a nicer name of a memory adress location.\nThe computer doesn\u0026rsquo;t care and will replace them when compiling, str won\u0026rsquo;t exists, it\u0026rsquo;s all 0x015c5c15c1c5.\nSource: Does operator := always cause a new copy to be created if assign without reference?\n# 3. reference-type vs value-type Keep in mind these two things:\nThere is just value-type in Go, no reference type, reference-type vs value-type is just for easy understanding and catagorizing, because reference is a very common concept in other languages like Java/Python.\nEverything passed by value in Go.\nAll copy is shallow copy, that is, only the value of the variable is copied, not the underlying data. Go’s arrays are values. An array variable denotes the entire array; And everything passed by value, so when you assign or pass around an array variable you will make a copy of its all elements.\nSlice is just a struct, it consists of three fields: a pointer to a underlying array, and its length and capacity. So When you assign or pass around a slice variable, the value of this variable is copied, but this is very cheap, just 3 words.\nDid you catch that? All passed by value.\nThe terminology reference type has been removed from Go specification since Apr 3rd, 2013 (with the commit message: spec: Go has no \u0026lsquo;reference types\u0026rsquo;), the terminology is still popularly used in Go community. https://github.com/go101/go101/wiki/About-the-terminology-%22reference-type%22-in-Go\n# 3.1. reference-type A slice does not store any data, it just describes a section of an underlying array. Therefore, your function can return a slice directly or accept a slice as a argument. In Go, a string is in effect a read-only slice of bytes. Only use *string if you have to distinguish an empty string from no strings. A map value is a pointer to a runtime.hmap structure. A map is just a pointer itself, therefore, you don\u0026rsquo;t need returen a pinter of a map value. Like maps, channels are allocated with make, and the resulting value acts as a reference to an underlying data structure. Interface, a value of interface type is a pointer actually, not just a pointer, but consists of it. A variable of interface type stores a pair: the concrete value assigned to the variable, and that value’s type descriptor. You don\u0026rsquo;t need to return a pointer to a reference-type for better performance.\n# 3.2. value-type Actually, all are value-type in Go, but I\u0026rsquo;ll list some you may mistake them as reference-type:\narray 1 2 3 4 5 6 7 8 9 10 11 12 func main() { arr_1 := [3]int{0, 0, 0} arr_2 := arr_1 arr_2[0] = 99 arr_2[1] = 99 fmt.Println(\u0026#34;arr_1:\u0026#34;, arr_1) fmt.Println(\u0026#34;arr_2:\u0026#34;, arr_2) } // output: arr_1: [0 0 0] arr_2: [99 99 0] struct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 type Cat struct { Name string Age int } func main() { cat_1 := Cat{ Name: \u0026#34;Coco\u0026#34;, Age: 1, } cat_2 := cat_1 cat_2.Name = \u0026#34;Bella\u0026#34; fmt.Println(\u0026#34;cat_1:\u0026#34;, cat_1) fmt.Println(\u0026#34;cat_2:\u0026#34;, cat_2) } // output: cat_1: {Coco 1} cat_2: {Bella 1} As you can see, when do modification on cat_2, cat_1 is not affected.\n# 4. value size Kinds of Types Value Size Required by Go Specification bool 1 byte not specified int8, uint8 (byte) 1 byte 1 byte int, uint 1 word architecture dependent, 4 bytes on 32-bit architectures and 8 bytes on 64-bit architectures string 2 words slice 3 words pointer 1 word map 1 word NOTE: Here I call it value size not type size, this word is important, var a int, a is a variable/value whose type is int, and the size of this variable/value is 1 word on my arm64 cpmputer its size is 8 bytes.\n# 5. how the size of a struct value is calculated? The size of a value means how many bytes the direct part of the value will occupy in memory. The indirect underlying parts of a value don\u0026rsquo;t contribute to the size of the value.\nI\u0026rsquo;ll give you an example,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func main() { cat := Cat{ name: \u0026#34;Coco\u0026#34;, age: 1, s: []int{1, 3, 4, 5, 6, 7}, } fmt.Printf(\u0026#34;cat: %T, %d\\n\u0026#34;, cat, unsafe.Sizeof(cat)) fmt.Printf(\u0026#34;a.age: %T, %d\\n\u0026#34;, cat.age, unsafe.Sizeof(cat.age)) fmt.Printf(\u0026#34;a.name: %T, %d\\n\u0026#34;, cat.name, unsafe.Sizeof(cat.name)) fmt.Printf(\u0026#34;cat.s: %T, %d\\n\u0026#34;, cat.s, unsafe.Sizeof(cat.s)) } cat: main.Cat, 48 cat.s: []int, 24 a.age: int, 8 a.name: string, 16 The sizes of two string values are always equal, same as string, the sizes of two slice values are also always equal. Values of a specified type always have the same value size.\nLearn more: Go Value Copy Costs -Go 101\n# 6. Variable declarations # 6.1. := vs var The := can only be used in inside a function, which is called short variable declarations.\nA var statement can be at package or function level, which is called regular variable declarations.\n# 6.2. Zero value Variables declared without an explicit initial value are given their zero value. The zero value is:\n0 for numeric types, false for the boolean type, and \u0026quot;\u0026quot; (the empty string) for strings. nil for pointer nil for map and slice 1 2 3 4 5 6 7 8 9 10 11 12 func main() { var i int var f float64 var b bool var s string var p *string var sl []int var m map[string]string fmt.Printf(\u0026#34;%v %v %v %q %v %v %v \\n\u0026#34;, i, f, b, s, p, sl==nil, m==nil) } // 0 0 false \u0026#34;\u0026#34; \u0026lt;nil\u0026gt; true true Dereferencing a nil pinter will cause panic, don\u0026rsquo;t do that.\n1 2 3 4 kitten := Cat{Name: \u0026#34;Coco\u0026#34;} // nil is zero value for a pointer var cat *Cat *cat = kitten // runtime error: invalid memory address or nil pointer dereference # 6.3. var vs new vs make It\u0026rsquo;s a little harder to justify new. The main thing it makes easier is creating pointers to non-composite types. The two functions below are equivalent. One\u0026rsquo;s just a little more concise:\n1 2 3 4 5 6 func newInt1() *int { return new(int) } func newInt2() *int { var i int return \u0026amp;i } new() returns a pointer to the value it created, a pointer to map, channel and slice is thought as useless, so don\u0026rsquo;t use new() with these types, just use it with non-composite types. And always use make() when create slice, map and channnel.\nWhen create a custom type, you can use literal:\n1 2 3 4 5 6 7 type ListNode struct { Val int Next *ListNode } res := ListNode{} res := \u0026amp;ListNode{} Conclusion is that don\u0026rsquo;t use new(), it\u0026rsquo;s a little confusing, just use make() and var, :=\nwhy new(): https://softwareengineering.stackexchange.com/a/216582\nnew vs make: https://stackoverflow.com/a/9322182/16317008\nvar vs make: https://davidzhu.xyz/post/golang/basics/003-collections/#5-var-vs-make\n# 7. type conversions The expression T(v) converts the value v to the type T. Some numeric conversions:\n1 2 3 var i int = 42 var f float64 = float64(i) var u uint = uint(f) Or, put more simply:\n1 2 3 i := 42 f := float64(i) u := uint(f) Unlike in C, in Go assignment between items of different type requires an explicit conversion, this enchances the type safety of Golang.\n# 8. Type assertions # 8.1. Basic syntax 1 str := value.(string) If it turns out that the value does not contain a string, the program will crash with a run-time error. To guard against that, use the \u0026ldquo;comma, ok\u0026rdquo; idiom to test, safely, whether the value is a string:\n1 2 3 4 5 6 str, ok := value.(string) if ok { fmt.Printf(\u0026#34;string value is: %q\\n\u0026#34;, str) } else { fmt.Printf(\u0026#34;value is not a string\\n\u0026#34;) } similar syntax - 1:\n1 2 3 4 5 6 7 8 9 10 users := make(map[string]int) users[\u0026#34;jack\u0026#34;] = 13 users[\u0026#34;john\u0026#34;] = 15 user, ok := users[\u0026#34;milo\u0026#34;] if ok { fmt.Println(user) } else { fmt.Println(\u0026#34;no such user\u0026#34;) } similar syntax - 2:\n1 ele, ok:= \u0026lt;-channel_name Type assertion provides access to an interface value\u0026rsquo;s underlying concrete value. So it only can be used when value\u0026rsquo;s type is interface:\n1 2 var m map[interface{}]interface{} _, isMap := m.(map[interface{}]interface{}) The snippet above will cause error: Invalid type assertion: m.(map[interface{}]interface{}) (non-interface type map[interface{}]interface{} on the left.\nThe code below works fine:\n1 2 3 4 5 6 var m map[interface{}]interface{} var t interface{} t = m _, isMap := t.(map[interface{}]interface{}) fmt.Println(isMap) // print: true # 8.2. use case 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func Copy(dst Writer, src Reader) (written int64, err error) { return copyBuffer(dst, src, nil) } func copyBuffer(dst Writer, src Reader, buf []byte) (written int64, err error) { // If the reader has a WriteTo method, use it to do the copy. // Avoids an allocation and a copy. if wt, ok := src.(WriterTo); ok { return wt.WriteTo(dst) } // Similarly, if the writer has a ReadFrom method, use it to do the copy. if rt, ok := dst.(ReaderFrom); ok { return rt.ReadFrom(src) } ... } bytes.Reader implements io.WriterTo interface and io.Copy uses that for optimized copying. source\n","date":"2023-12-07T10:00:20Z","permalink":"https://blog.yorforger.cc/p/value-variable-and-types-go/","title":"Value, Variable and Types - Go"},{"content":" # 1. Basic concepts A map value is a pointer to a runtime.hmap structure, when you write the statement:\n1 m := make(map[int]int) The compiler replaces it with a call to runtime.makemap, which has the signature\n1 2 3 4 5 6 // makemap implements a Go map creation make(map[k]v, hint) // If the compiler has determined that the map or the first bucket // can be created on the stack, h and/or bucket may be non-nil. // If h != nil, the map can be created directly in h. // If bucket != nil, bucket can be used as the first bucket. func makemap(t *maptype, hint int64, h *hmap, bucket unsafe.Pointer) *hmap 1 2 3 4 5 func main() { var m map[int]int var p uintptr fmt.Println(unsafe.Sizeof(m), unsafe.Sizeof(p)) // 8 8 (linux/amd64) } ⚠️ Map value just a pointer, therefore, we don\u0026rsquo;t need to pass a pointer to a map for better performance. This is similar to slices, slice just a struct that has a pointer element which pointer to an underlaying array, so we don\u0026rsquo;t need to set a pointer of slice as a paramter. This also applys function\u0026rsquo;s return type.\nMaps, like channels, but unlike slices, are just pointers to runtime types. As you saw above, a map is just a pointer to a runtime.hmap structure.\nLearn more: maps | Dave Cheney\n# 2. var vs make() In previous post we know that you can append a nil slice directly, but this is not the case in a map:\n1 2 3 var cats map[string]int // panic: assignment to entry in nil map cats[\u0026#34;Coco\u0026#34;] = 3 Therefore, for map you should use make or map literal\n1 2 3 // They are equivlent cats := map[string]int{} dogs := make(map[string]int) # 3. Commone usage of map # 3.1. Use map as a counter 1 2 3 4 5 6 7 8 // as a counter counts := make(map[string]int) for name := range users { counts[name]++ } // check if exist, O(1) ele, ok := m[\u0026#34;key\u0026#34;] if !ok {...} # 3.2. Copy map Find a blog talks copy map, share it here:\nLike slices, maps hold references to an underlying data structure. So by assigning its value to another variable, only the reference will be passed. To copy the map, it is necessary to create another map and copy each value:\n1 2 3 4 5 6 7 8 9 10 11 12 // Create the original map originalMap := make(map[string]int) originalMap[\u0026#34;one\u0026#34;] = 1 originalMap[\u0026#34;two\u0026#34;] = 2 // Create the target map targetMap := make(map[string]int) // Copy from the original map to the target map for key, value := range originalMap { targetMap[key] = value } Deep copy a map or slice: encoding/gob \u0026amp; encoding/json in golang - David\u0026rsquo;s Blog\n# 4. Map in concurrent programming First version:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func (s *memoryStore) monitorExpiredSessions() { for { // detect every seconds time.Sleep(time.Second) memoryMutex.Lock() if len(s.sessions) == 0 { memoryMutex.Unlock() continue } for k, info := range s.sessions { if info.expiresTimestamp \u0026lt;= time.Now().Unix() { delete(s.sessions, k) } } memoryMutex.Unlock() } } // in other place will call this method go s.monitorExpiredSessions() This is not good, as the size of s.sessions grows, the time to iterate through s.sessions will increase, leading to longer lock holding times. We can optimize lock usage, possibly by using more granular locks or lock-free structures.\nSecond version:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func (s *memoryStore) monitorExpiredSessions() { ticker := time.NewTicker(time.Second) defer ticker.Stop() for range ticker.C { if s.isEmpty() { continue } for k, info := range s.sessions { if info.expiresTimestamp \u0026lt;= time.Now().Unix() { s.delete(k) } } } } func (s *memoryStore) delete(k string) { memoryMutex.Lock() defer memoryMutex.Unlock() delete(s.sessions, k) } func (s *memoryStore) isEmpty() bool { memoryMutex.RLock() defer memoryMutex.RUnlock() return len(s.sessions) == 0 } Some problems:\nThere is no lock when ranging map s.sessions You cannot require a RLock or Lock before range, if you do that, the memoryMutex.Lock() residing in s.delete() will block forever. Because you have hold a lock before the loop, which havn\u0026rsquo;t been released yet, and you cannot acquire another lock in the loop. Third version:\nI find fiber memory storage on github which suits my condition provided by a gopher.\nI\u0026rsquo;ll share the code here:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 // Adopted from: https://github.com/gofiber/storage/blob/main/memory/memory.go func (s *cookieStore) gc() { ticker := time.NewTicker(s.gcInterval) defer ticker.Stop() var expired []string for range ticker.C { if s.isEmpty() { continue } mutex.RLock() for k, session := range s.sessions { if session.expiry \u0026lt;= time.Now().Unix() { expired = append(expired, k) } } mutex.RUnlock() mutex.Lock() // Double-checked locking. // User might have reset the max age of the session in the meantime. for i := range expired { v := s.sessions[expired[i]] if v.expiry \u0026lt;= time.Now().Unix() { delete(s.sessions, expired[i]) } } mutex.Unlock() } } func (s *cookieStore) isEmpty() bool { mutex.RLock() defer mutex.RUnlock() return len(s.sessions) == 0 } When I test this, the codes get panic sometimes:\nAt v.expiry \u0026lt;= time.Now().Unix() :\n1 2 3 4 v := s.sessions[expired[i]] if v.expiry \u0026lt;= time.Now().Unix() { ... } 1 panic: runtime error: invalid memory address or nil pointer dereference This means we get a nil session from s.sessions[expired[i]], so there is a problem with slice expired, I was thinking if its length is 0, the range still iterate it. Turns out the range won\u0026rsquo;t iterate an empty slice, just do nothing.\nThen I realize that I did\u0026rsquo;t update the slice, woops, we need drop the useless element in last round:\n1 2 3 4 5 6 ... for range ticker.C { // drop the useless elements. expired = expired[:0] ..... } ","date":"2023-12-06T23:43:50Z","permalink":"https://blog.yorforger.cc/p/map-go/","title":"map - Go"},{"content":" # 1. Array An array\u0026rsquo;s length is part of its type, so arrays cannot be resized. Its length is part of its type ([4]int and [5]int are distinct, incompatible types).\nGo’s arrays are values. An array variable denotes the entire array; it is not a pointer to the first array element (as would be the case in C). This means that when you assign or pass around an array value you will make a copy of its contents. (To avoid the copy you could pass a pointer to the array, but then that’s a pointer to an array, not an array.)\n# 2. Slice # 2.1. Slice is just a Struct A slice is just a struct. It consists of a pointer to the array, the length of the underlying array, its capacity (the maximum length of the underlying array).\n1 2 3 4 5 type slice struct { data uintptr len int cap int } So the overhead of assigning or passing a slice value, is just 3 words (12 bytes on 32bit, 24 bytes on 64bit), which is very cheap. This is different from assigning or passing an array value, which will copy the entire array.\n# 2.2. Slicing When you create a new slice from an existing array or slice (e.g., newSlice := oldSlice[start:end]), the new slice does not copy the elements. Instead, it refers to the same underlying array as the original slice. Slice is just a struct, as we have talked above.\n1 2 3 4 5 6 7 8 9 10 originalSlice := []int{1, 2, 3, 4, 5} newSlice := originalSlice[:3] originalSlice[0] = 99 // This also changes newSlice[0] newSlice[1] = 101 // This also changes originalSlice[1] fmt.Println(\u0026#34;Original slice:\u0026#34;, originalSlice) fmt.Println(\u0026#34;New slice:\u0026#34;, newSlice) // ouput Original slice: [99 101 3 4 5] New slice: [99 101 3] Note: if you append to the slice and the capacity of the underlying array is exceeded, Go will allocate a new array and copy the elements over, causing the slices to diverge.\nThere is a trick, we usually use d = d[:0] to generate a new slice d whose length is 0 but capacity not change, which can make program more efficient.\n# 2.3. var vs make() 1 2 3 var cats []string // or dogs := make([]string, 0) We have know that any varibles declared with var without an explicit initial value are given their zero value, nil is zero for map, slice and pointer.\nTherefore, the value of cats is nil for sure, then for make([]string, 0)it allocates a memory with 0 elements, which means dog != nil. But there probably no difference when you use, because you can append a nil slice directly:\n1 2 3 4 var expired []string // it\u0026#39;s totally fine to do this: expired = append(expired, \u0026#34;hello\u0026#34;) fmt.Println(expired) # 3. Commone usage of slice # 3.1. Remove elements by reslicing 1 2 3 4 5 6 7 8 9 func (s *MemoryStore) gc() { var expired []string for { // A new cycle begins, \u0026#34;remove\u0026#34; all elements from \u0026#39;expired\u0026#39; expired = expired[:0] // add elements to \u0026#39;expired\u0026#39;, and use them ... } } 1 2 3 4 5 6 7 8 9 10 // Stimulate stack stack := make([]rune, 0) for _, r := range s { if _, ok := pairs[r]; ok { stack = append(stack, r) } else { // reslice, \u0026#34;delete\u0026#34; the last one element stack = stack[:len(stack)-1] } } # 3.2. Passing a slice to channel 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func main() { ready := make(chan []string) go func() { var expired []string expired = append(expired, \u0026#34;Coco\u0026#34;) expired = append(expired, \u0026#34;Bella\u0026#34;) ready\u0026lt;- expired }() go func() { // this will block until there is a data sent to the channel \u0026#34;ready\u0026#34; for s := range ready { for _, name := range s{ fmt.Println(name) } } }() time.Sleep(time.Second) } Bacsuse a slice likes a pointer, the two goroutines above share a same underlying array. You have to consider if there is a data race, if yes, consider make a deep copy of the slice: Everything Passed by Value - Go - David\u0026rsquo;s Blog\nNote that iteration variable is re-used in each iteration.\n","date":"2023-12-06T23:40:59Z","permalink":"https://blog.yorforger.cc/p/slice-array-go/","title":"slice \u0026 array - Go"},{"content":" # 1. os.Open() loads entire file into memory? No, the os.Open() function in Golang does not load the entire file into RAM by default. It returns a file descriptor that allows you to read or write data from or to the file.\nI found this answer talks about the open() sys call Linux on StackOverflow, which may give you hints on how os.Open() works:\nNo, a file is not automatically read into memory by opening it. That would be awfully inefficient. You have to read it yourself.\nNo it doesn\u0026rsquo;t. It just gives file a descriptor called file descriptor which then you can use to do read / write and some other operations. Think file descriptor as an abstraction or an handle over what lays on disk.\nThe open() function shall establish the connection between a file and a file descriptor.\nSource: https://stackoverflow.com/a/20512890/16317008\n1 func Open(name string) (*File, error) Open opens the named file for reading. If successful, methods on the returned file can be used for reading;\nThe return type of os.Open() is *File, which is a pointer to a struct:\n1 2 3 4 5 6 7 8 9 10 11 12 13 // File represents an open file descriptor. type File struct { *file // os specific } type file struct { pfd poll.FD name string dirinfo *dirInfo nonblock bool stdoutOrErr bool appendMode bool } As you can see, os.Open() just returns a file descriptor, which is a pointer to a struct. It does not load the entire file into memory. After you get the file descriptor, you can use it to read or write data from or to the file. This is why we say \u0026ldquo;The open() function shall establish the connection between a file and a file descriptor.\u0026rdquo;\nIf you want to read the entire file into memory, you can use the ioutil.ReadFile() function from the io/ioutil package. This function reads the entire contents of a file into a byte slice. However, keep in mind that this approach may not be suitable for very large files, as it loads the entire file into memory at once.\nLet me demonstrate how to open a file and read its contents chunk by chunk in Golang:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func main() { // Open the file for reading file, _ := os.Open(\u0026#34;input.txt\u0026#34;) // Close the file later, otherwise it will cause memory leak defer file.Close() // Create a buffer to keep chunks that are read buffer := make([]byte, 1024) // Read from the file for { // Read a chunk // There is another way to read: file.ReadAt() n, _ := file.Read(buffer) if n == 0 { break } fmt.Print(string(buffer[:n])) } } Why if os.File() load the entire file into memory matters? Because if you are dealing with a very large file, and the RAM is quite limited, your program may get killed by the OS due to out of memory.\n# 2. io.Reader interface # 2.1. What is io.Reader The use io.Reader is quite common in Go. What is io.Reader?\n1 2 3 type Reader interface { Read(p []byte) (n int, err error) } As you can see, io.Reader is an interface that has a Read() method. Any type that implements the Read() method is a io.Reader. Read reads up to len(p) bytes into p. It returns the number of bytes read (0 \u0026lt;= n \u0026lt;= len(p)) and any error encountered. Learn more: https://golang.org/pkg/io/#Reader\n# 2.2. Common types implement io.Reader # 2.2.1. *os.File *os.File we mentioned above has the Read() method, which means it\u0026rsquo;s an io.Reader. Note that *os.File doesn\u0026rsquo;t equals to os.File, because they have different method set. Learn more: https://davidzhu.xyz/post/golang/basics/006-interfaces/\n1 func (f *File) Read(b []byte) (n int, err error) # 2.2.2. *bytes.Buffer 1 2 3 4 // A Buffer is a variable-sized buffer of bytes with Read and Write methods. The zero value for Buffer is an empty buffer ready to use. type Buffer struct { // contains filtered or unexported fields } 1 func (b *Buffer) Read(p []byte) (n int, err error) As you can see, *bytes.Buffer has the Read() method, not bytes.Buffer. So *bytes.Buffer is an io.Reader, but bytes.Buffer is not.\nIf you run code below, you will get an error Cannot use \u0026lsquo;bytes.Buffer{}\u0026rsquo; (type bytes.Buffer) as the type io.Reader Type does not implement \u0026lsquo;io.Reader\u0026rsquo; as the \u0026lsquo;Read\u0026rsquo; method has a pointer receiver:\n1 2 3 var r io.Reader r = bytes.Buffer{} // error r = \u0026amp;bytes.Buffer{} // this is ok You should learn how to read the documentation provided Go, it\u0026rsquo;s very important:\n# 2.2.3. net.Conn The net.Conn interface represents a network connection, such as a TCP or UDP connection. Many networking packages in Go, like net, net/http, and net/rpc, provide implementations of this interface. net.Conn interface has the Read() method, so it\u0026rsquo;s an io.Reader.\n1 2 3 4 5 type Conn interface { Read(b []byte) (n int, err error) Write(b []byte) (n int, err error) ... } # 2.2.4. Code example 1 2 3 4 5 6 file, _ := os.Open(\u0026#34;example.txt\u0026#34;) defer file.Close() buffer := make([]byte, 1024) n, _ := file.Read(buffer) fmt.Printf(\u0026#34;Read %d bytes from the file: %s\\n\u0026#34;, n, buffer[:n]) 1 2 3 4 5 6 data := []byte(\u0026#34;Hello, World!\u0026#34;) buffer := bytes.NewBuffer(data) readData := make([]byte, 5) n, _ := buffer.Read(readData) fmt.Printf(\u0026#34;Read %d bytes from the buffer: %s\\n\u0026#34;, n, readData) 1 2 3 4 5 6 conn, _ := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;example.com:80\u0026#34;) defer conn.Close() buffer := make([]byte, 1024) n, _ := conn.Read(buffer) fmt.Printf(\u0026#34;Read %d bytes from the network connection: %s\\n\u0026#34;, n, buffer[:n]) *os.File, *bytes.Buffer, and net.Conn all have the Read() method, so they are all io.Reader. And the use of them are quite similar which involves:\nget a io.Reader close the io.Reader later create a buffer for store chunk of data (not load entire data into memory) read data (chunk by chunk) to the buffer # 3. Reader wrapper # 3.1. Read user input from the console Sometimes we want to add some extra functionality to a io.Reader, for example, when read user input from the console, we want use a specific delimiter to indicate the end of input. We can use a bufio.Reader to wrap the os.Stdin:\n1 2 3 4 reader := bufio.NewReader(os.Stdin) fmt.Print(\u0026#34;Enter your input: \u0026#34;) input, _ := reader.ReadString(\u0026#39;\\n\u0026#39;) fmt.Println(\u0026#34;You entered:\u0026#34;, input) 1 2 3 4 5 // In bufio package, so the pointer receiver *Reader here is *bufio.Reader not io.Reader. func NewReader(rd io.Reader) *Reader // In bufio package, same as above. func (b *Reader) ReadString(delim byte) (string, error) *bufio.Reader and os.Stdin both have the Read() method, so they are both io.Reader. But bufio.Reader provides more functionality than os.Stdin, for example, ReadString(), ReadBytes(), ReadLine(), etc. So we can use bufio.Reader to wrap os.Stdin to add extra functionality.\nIf you use os.Stdin to read user input, would be a little inconvenient, for example, you need convert the byte slice to a string, and trim the unused part. And you cannot specify a delimiter to indicate the end of input:\n1 2 3 4 5 6 7 8 fmt.Print(\u0026#34;Enter your input: \u0026#34;) input := make([]byte, 1024) // Create a byte slice of a certain size n, _ := os.Stdin.Read(input) // Convert byte slice to a string, trimming the unused part inputStr := string(input[:n]) fmt.Println(\u0026#34;You entered:\u0026#34;, inputStr) # 3.2. Limit the size of HTTP request body When handle the user upload file, you may handle like this:\n1 2 3 4 5 6 7 8 func uploadHandler(w http.ResponseWriter, r *http.Request) { // Parse the form data from the request _ := r.ParseMultipartForm(10 \u0026lt;\u0026lt; 20) // Set a reasonable file size limit, e.g., 10MB // Get the file from the request file, _, _ := r.FormFile(\u0026#34;file\u0026#34;) ... } Accroding to the documentation of r.ParseMultipartForm():\nParseMultipartForm parses a request body as multipart/form-data. The whole request body is parsed and up to a total of maxMemory bytes of its file parts are stored in memory, with the remainder stored on disk in temporary files. http package - net/http - Go Packages\nThis means r.ParseMultipartForm() will parse the whole request body even if the uploaded file is larger than maxMemory. But think about another senario, if the client sends a vary large file, the server needs to handle all that data, which may consume the bandwith and CUP usage of the server.\nSo we need to limit the size of incoming request bodies. We can use http.MaxBytesReader() to wrap the http.Request.Body like this:\n1 2 3 4 5 6 7 8 9 10 11 func (s *server) handleUpload(w http.ResponseWriter, r *http.Request, currentDir string) (error, int) { r.Body = http.MaxBytesReader(w, r.Body, s.maxFileSize) if err := r.ParseMultipartForm(maxFileSize); err != nil { return fmt.Errorf(\u0026#34;file is too large:%v\u0026#34;, err), http.StatusBadRequest } // obtain file from parsed form. parsedFile, _, _ := r.FormFile(\u0026#34;file\u0026#34;) ... } Let\u0026rsquo;s see the documentation of http.MaxBytesReader():\n1 func MaxBytesReader(w ResponseWriter, r io.ReadCloser, n int64) io.ReadCloser http.MaxBytesReader() accepts a io.ReadCloser and returns a io.ReadCloser. To be specific, http.MaxBytesReader() wraps the http.Request.Body(io.ReadCloser) to a http.maxBytesReader which implements the io.ReadCloser interface.\n1 2 3 4 5 6 func MaxBytesReader(w ResponseWriter, r io.ReadCloser, n int64) io.ReadCloser { if n \u0026lt; 0 { // Treat negative limits as equivalent to 0. n = 0 } return \u0026amp;maxBytesReader{w: w, r: r, i: n, n: n} } As you can see, what http.MaxBytesReader() does is just to wrap the http.Request.Body to another io.ReadCloser.\nThen you call r.ParseMultipartForm(), this is where the actual parsing and loading of data into memory occur. According to the documentation of r.ParseMultipartForm(), it will parse the whole request body. But the wrapped http.Request.Body will limit the size of the request body, if the incoming data exceeds maxFileSize during the reading and parsing process, the http.MaxBytesReader will trigger an error, effectively preventing the server from reading any more data from the client. This helps to protect against clients sending excessively large requests.\nThen what do you think about how the r.ParseMultipartForm() parse file from request body? Don\u0026rsquo;t forget http.Request.Body is a io.Reader too, which means it has the Read() method. It actually calls the Read() method of http.Request.Body to read data from the request body.\nIn the Read() method of http.maxBytesReader, it will check if the number of bytes read exceeds the limit, if so, it will return an error. You can check the source code of the Read() method of http.maxBytesReader:\n1 2 3 4 5 6 7 8 func (l *maxBytesReader) Read(p []byte) (n int, err error) { ... if res, ok := l.w.(requestTooLarger); ok { res.requestTooLarge() } l.err = \u0026amp;MaxBytesError{l.i} return n, l.err } MaxBytesReader prevents clients from accidentally or maliciously sending a large request and wasting server resources. If possible, it tells the ResponseWriter to close the connection after the limit has been reached.\n# 4. The nature of io.Reader Readers in Go are used for reading data streams, and they have a crucial property: once you read data from a reader, that data is consumed. It\u0026rsquo;s not possible to \u0026lsquo;rewind\u0026rsquo; or read the same data from the reader again unless the reader specifically supports such an operation.\nAs what we did above, we read data from *os.File, *bytes.Buffer, and net.Conn which are all io.Reader. When we read the data into the buffer chunk by chunk, we didn\u0026rsquo;t record or mark where we stop, but in next time we read from the reader, it will start from the last stop point. This is because the data is consumed. Let me demonstrate this:\n1 2 3 4 5 6 7 8 9 func main() { reader := strings.NewReader(\u0026#34;HelloWorld\u0026#34;) buf := make([]byte, 5) _, _ = reader.Read(buf) fmt.Println(string(buf)) // Outputs: Hello _, _ = reader.Read(buf) fmt.Println(string(buf)) // Outputs: World } This applies to all readers, including *os.File, *bytes.Buffer, and net.Conn, etc.\n","date":"2023-12-04T14:25:04Z","permalink":"https://blog.yorforger.cc/p/io-in-golang/","title":"IO in Golang"},{"content":" # 1. Slicing makes shallow copy of a list I have talked this nature of slicing in previous post, currently I came across a weird behavior when improt a variable from another package.\nmessag.py:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 messages = [] def with_user(msg: str) -\u0026gt; None: messages.append(msg) def trim_last_two_messages() -\u0026gt; None: global messages print(\u0026#34;in trim func, before trim, len of messages:\u0026#34; + str(len(messages))) if len(messages) \u0026gt; 2: messages = messages[:-2] print(\u0026#34;in trim func, after trim, len of messages:\u0026#34; + str(len(messages))) if __name__ == \u0026#34;__main__\u0026#34;: pass main.py:\n1 2 3 4 5 6 7 8 9 10 11 from message import messages from message import with_user from message import trim_last_two_messages with_user(\u0026#39;hello\u0026#39;) with_user(\u0026#39;world\u0026#39;) with_user(\u0026#39;foo\u0026#39;) print(\u0026#34;main func, before trim:\u0026#34;, messages) trim_last_two_messages() print(\u0026#34;main func, after trim:\u0026#34;, messages) The output is:\n1 2 3 4 main func, before trim: [\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;, \u0026#39;foo\u0026#39;] in trim func, before trim, len of messages:3 in trim func, after trim, len of messages:1 main func, after trim: [\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;, \u0026#39;foo\u0026#39;] But what I expected is:\n1 2 3 4 main func, before trim: [\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;, \u0026#39;foo\u0026#39;] in trim func, before trim, len of messages:3 in trim func, after trim, len of messages:1 main func, after trim: [\u0026#39;hello\u0026#39;] Importing messages: When you import messages from message.py into main.py, you are importing a reference to the same list object that exists in message.py. Appending Messages: When you call with_user, it appends new messages to the messages list. Both main.py and message.py are still referring to the same list object. Trimming Messages: In trim_last_two_messages, you are slicing the messages list (messages = messages[:-2]). This is where the key behavior occurs. Slicing a list in Python creates a new list object. So after this slicing operation, message.py\u0026rsquo;s messages now points to a new list object, but main.py\u0026rsquo;s messages still points to the original list object. Because of this, changes made to the list in message.py (after the slicing operation) are not reflected in main.py, as they are now two different list objects.\nCorrect way (change the function in message.py):\n1 2 3 4 5 6 def trim_last_two_messages() -\u0026gt; None: global messages print(\u0026#34;in trim func, before trim, len of messages:\u0026#34; + str(len(messages))) if len(messages) \u0026gt; 2: del messages[-2:] print(\u0026#34;in trim func, after trim, len of messages:\u0026#34; + str(len(messages))) or\nChange the code in main.py:\n1 2 3 4 import message # use messages as this: print(message.messages) # 2. Performance of del vs relicing 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import timeit # Test with a large list large_list = list(range(1000000)) # a list with one million elements def test_del_method(): temp_list = large_list.copy() del temp_list[-2:] def test_slice_method(): temp_list = large_list.copy() temp_list = temp_list[:-2] time_del = timeit.timeit(test_del_method, number=100) time_slice = timeit.timeit(test_slice_method, number=100) print(f\u0026#34;Time taken by del method: {time_del} seconds\u0026#34;) print(f\u0026#34;Time taken by slice method: {time_slice} seconds\u0026#34;) The output:\n1 2 Time taken by del method: 0.312709417 seconds Time taken by slice method: 0.683257708 seconds ","date":"2023-12-03T09:01:25Z","permalink":"https://blog.yorforger.cc/p/common-mistakes-and-tricks-python/","title":"Common Mistakes and Tricks Python"},{"content":" # 1. Problem description I\u0026rsquo;ve discussed how to use Cloudflare as a reverse proxy for my web traffic in previous post.\nI write a file server in Go, when attempting to upload a file of approximately 200 MB, it becomes very slow (always uploading) and never succeed. Upon monitoring the server using tools like htop, it seems that the server might be unaware of the incoming large file upload. I come across a error by chance: 413 Request Entity Too Large.\n# 2. Cloudflare Then I found this is because the limitation of Couldflare proxy:\nThere’s a body size limit of 100 MB on Free and Pro, 200 MB on Business and 50074 MB on Enterprise. Only Enterprise customers can request to have the body size limit increased.\nEach user can only upload files with maximum size of 100MB at a time if your website is proxied by Cloudflare.\nDoes the 100 Mb limit apllies to all users on my website?\nSo this is the thing, then I try to access my website directly by IP address (bypass the Cloudflare proxy).\n# 3. VPS RAM Then I can see my file server handling the incoming request whose body larger than 100MB, I use htop to check the RAM usage status on my VPS, I can see the ram increase from 50MB to 100MB then 200MB, then my file server get killed by the OS.\nMy VPS RAM is 512MB, and there is 460MB left, I don\u0026rsquo;t know why the OS kill my file server program.\n# 4. Golang Besides, when I tried to upload some files whose size is around 60MB, the RAM usage of my VPS increase from 50MB (total) to around 100MB, this makes sense, because the file server needs load the whole file to the RAM and copy the content to the disk.\nBut after handling this request, the RAM usage is still around 100MB, if I upload another file (60MB), then the RAM usage increases to around 160MB. I thought there is memory leak on my program. But I was wrong. I found a post and share it here:\nGo does not release memory it allocated from the OS immediately. The reason is probably that allocating memory is costly (needs system calls) and the chance is high that it will be needed in the near future anyway again. But, if memory gets long enough unused it will be released eventually so that the RSS of the process decreases again.\nLearn more: https://stackoverflow.com/a/49843380/16317008\nRSS indicates the actual physical memory consumed by a process, while VSZ includes not only the physical memory but also the virtual memory.\nLearn more: https://stackoverflow.com/a/21049737/16317008\n","date":"2023-11-30T21:43:22Z","permalink":"https://blog.yorforger.cc/p/go-file-server-cannot-handle-large-files/","title":"Go File Server Cannot Handle Large Files"},{"content":" # 1. Objects in Python Everything in Python is an object. Classes, functions, and even simple data types, such as integers, floats, and strings, are objects in Python. When we define an integer in Python, CPython internally creates an object of type integer. These objects are stored in heap memory.\nEach Python object consists of three fields:\nValue Type Reference count Let\u0026rsquo;s consider a simple example:\n1 a = 100 When the above code is executed, CPython creates an object of type integer and allocates memory for this object on the heap memory.\nThe type indicates the type of the object in CPython, and the value field, as the name suggests, stores the value of the object (100 in this case). We will discuss the ref_count field later in the article.\nCPython is the default and most widely used implementation of the Python language. When we say Python, it essentially means we\u0026rsquo;re referring to CPython. When you download Python from python.org, you basically download the CPython code. Thus, CPython is a program written in C language that implements all the rules and specifications defined by the Python language. CPython can be defined as both an interpreter and a compiler as it compiles Python code into bytecode before interpreting it.\n# 2. Variables in Python Variables in Python are just references to the actual object in memory. They are like names or labels that point to the actual object in memory. They do not store any value.\nConsider the following example:\n1 a = 100 As discussed earlier, when the above code is executed, CPython internally creates an object of type integer. The variable a points to this integer object as shown below:\nWe can access the integer object in the Python program using the variable a.\n读到这可以看出Java还不是真正的万物皆对象 (如果想在heap上创建primitive对象 需要Integer声明创建), 但Python就是所有的东西都是对象, 即使是一个int类型, 甚至一个函数也是个对象, 另外Java与python的GC (Python中不叫GC叫Python memory manager)对对象的管理方式基本上是一样的, 即分为引用和对象两部分, 引用在stack, 对象在heap, 不像c++那种在函数里声明的就是局部对象(除malloc和new), 对于Java与python来说无论你在哪创建一个对象, 他们都会被创建到heap上, 且根据引用计数来判断对象是否reachable, 然后判断是否回收对象, 所以你完全可以返回一个“局部对象”的引用, 但C++中, 肯定就不行了, 这会造成野指针问题,\nLet\u0026rsquo;s assign this integer object to another variable b:\n1 b = a When the above code is executed, the variables a and b both point to the same integer object, as shown below:\nLet\u0026rsquo;s now increment the value of the integer object by 1:\nLet\u0026rsquo;s now increment the value of the integer object by 1:\n1 2 # Increment a by 1 a = a + 1 When the above code is executed, CPython creates a new integer object with the value 101 and makes variable a point to this new integer object. Variable b will continue to point to the integer object with the value 100, as shown below:\nHere, we can see that instead of overwriting the value of 100 with 101, CPython creates a new object with the value 101 because integers in Python are immutable. Once created, they cannot be modified. Please note that floats and string data types are also immutable in Python.\nLet\u0026rsquo;s consider a simple Python program to further explain this concept:\n1 2 3 i = 0 while i \u0026lt; 100: i = i + 1 The above code defines a simple while loop that increments the value of the variable i until it is less than 100. When this code is executed, for every increment of the variable i, CPython will create a new integer object with the incremented value, and the old integer object will be deleted (to be more precise, this object would become eligible for deletion) from memory.\nCPython calls the malloc method for each new object to allocate memory for that object. It calls the free method to delete the old object from memory.\n1 2 3 4 5 i = 0 # malloc(i) while i \u0026lt; 100: # malloc(i + 1) # free(i) i = i + 1 We can see that CPython creates and deletes a large number of objects, even for this simple program. If we call the malloc and free methods for each object creation and deletion, it will degrade the program’s execution performance and make the program slow.\nHence, CPython introduces various techniques to reduce the number of times we have to call malloc and free for each small object creation and deletion. Let’s now understand how CPython manages memory!\nLearn more: Memory Management in Python - Honeybadger Developer Blog\n","date":"2023-11-29T14:51:31Z","permalink":"https://blog.yorforger.cc/p/objects-variables-python/","title":"Objects \u0026 Variables Python"},{"content":" # 1. Python 1 2 3 4 5 6 7 8 9 10 11 12 13 def test_function(x, y): x += 1 y.append(4) def main(): a = 1 b = [2, 3] test_function(a, b) print(\u0026#34;a =\u0026#34;, a) print(\u0026#34;b =\u0026#34;, b) main() Output:\n1 2 a = 1 b = [2, 3, 4] If you pass a mutable object into a method, the method gets a reference to that same object and you can mutate it to your heart\u0026rsquo;s delight, but if you rebind the reference in the method, the outer scope will know nothing about it, and after you\u0026rsquo;re done, the outer reference will still point at the original object.\ninteger, str are immutable objects in Python If you pass an immutable object to a method, you still can\u0026rsquo;t rebind the outer reference, and you can\u0026rsquo;t even mutate the object.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026gt;\u0026gt;\u0026gt; def main(): ... n = 9001 ... print(f\u0026#34;Initial address of n: {id(n)}\u0026#34;) ... increment(n) ... print(f\u0026#34; Final address of n: {id(n)}\u0026#34;) ... \u0026gt;\u0026gt;\u0026gt; def increment(x): ... print(f\u0026#34;Initial address of x: {id(x)}\u0026#34;) ... x += 1 ... print(f\u0026#34; Final address of x: {id(x)}\u0026#34;) ... \u0026gt;\u0026gt;\u0026gt; main() Initial address of n: 140562586057840 Initial address of x: 140562586057840 Final address of x: 140562586057968 Final address of n: 140562586057840 # 2. Javascript 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function changeStuff(a, b, c) { a = a * 10; b.item = \u0026#34;changed\u0026#34;; c = {item: \u0026#34;changed\u0026#34;}; } var num = 10; var obj1 = {item: \u0026#34;unchanged\u0026#34;}; var obj2 = {item: \u0026#34;unchanged\u0026#34;}; changeStuff(num, obj1, obj2); console.log(num); console.log(obj1.item); console.log(obj2.item); This produces the output:\n1 2 3 10 changed unchanged # 3. Golang, Java, C++ \u0026amp; Javascript Python, Java, C++ and JS are all pass by value, but they do have references. When you assign or pass a reference-type, the copied value is still the value of the reference, namely the address, giving us the illusion of \u0026ldquo;pass by reference not value.\u0026rdquo;\nC++ has another application where passing const references is preferred to enhance efficiency. However, in Java, Python, and JS, references are passed every time, but they lack the concept of const references.\nActually Golang doesn\u0026rsquo;t have reference type (all are values), reference type often refers to the maps, slice, channels and strings in Golang which are pointers themselves or a struct holding a pointer to the underlying data structure.\n","date":"2023-11-28T23:02:06Z","permalink":"https://blog.yorforger.cc/p/pass-by-value-or-reference/","title":"Pass by Value or Reference"},{"content":" # Data Types in Programming Languages Programs are collections of instructions that manipulate data to produce a desired result.\n# 1. Javascript As you have know that javascript is a dynamic language from previous post, which means the values have types, not variables.\nJavascript value types can be categorized into two main categories: objects and primitives. Among these types, the objects are mutable, which means their values can be modified or changed after they are created. On the other hand, the primitive types are immutable, meaning their values cannot be changed once they are assigned.\n# 1.1. Primitive types Type typeof return value Object wrapper Null \u0026quot;object\u0026quot; N/A Undefined \u0026quot;undefined\u0026quot; N/A Boolean \u0026quot;boolean\u0026quot; Boolean Number \u0026quot;number\u0026quot; Number BigInt \u0026quot;bigint\u0026quot; BigInt String \u0026quot;string\u0026quot; String Symbol \u0026quot;symbol\u0026quot; Symbol All primitive types, except null and undefined, have their corresponding object wrapper types, which provide useful methods for working with the primitive values. For example, the Number object provides methods like toExponential().\nAll primitive types, except null, can be tested by the typeof operator. typeof null returns \u0026quot;object\u0026quot;, so one has to use === null to test for null.\n# 1.2. Object types Learn more:\nObjects \u0026amp; Collections in Javascript - David\u0026rsquo;s Blog\nJavaScript data types and data structures - JavaScript | MDN\n# 2. Python Python is a dynamic language which means the values have types, not variables.\nIn Python all values are objects, so doesn\u0026rsquo;t like Java, there is no primitives, all variable are references. (Variables are associated with values, values have types)\n# 2.1. Data types Python has the following data types built-in by default, in these categories:\nText Type: str Numeric Types: int, float, complex Sequence Types: list, tuple, range Mapping Type: dict Set Types: set, frozenset Boolean Type: bool Binary Types: bytes, bytearray, memoryview None Type: NoneType # 2.2. Mutable and immutable objects Everything in Python is an object. And all objects in Python can be either mutable or immutable. Mutable objects are those that allow you to change their value or data in place without affecting the object’s identity. In contrast, immutable objects don’t allow this kind of operation, you have to create a new objects of the same type with different values.\nObjects of built-in types like (int, float, bool, str, tuple, unicode) are immutable. Objects of built-in types like (list, set, dict) are mutable. Custom classes are generally mutable.\nFind a good explanation:\nThe integer is immutable. When you write x=5, x points to a memory location that holds 5. When you go on and code y=x, the variable y points to the same location as x.\nThen you type x+1=6, and now x points to a new location that holds 6, and not the previous location. ( Here, the integer still holds immutable because the original integer 5 still exists, but the variable x is not bound to it now. x is now bound to a new location. But y is still bound to the integer 5)\nBut y still points to the same location that holds 5. So, integers are still immutable and this is how it works. To see it better, use id(x) or id(y) after every step.\nThe variable is not immutable; the int object referred to by the variable is.\n# 3. Java Java is a static language which means the variable have types, not values have types.\nTypes of variables in Java are divided into two categories—primitive types and reference types.\n# 3.1. Primitive types The primitive types are boolean, byte, char, short, int, long, float and double. All other types are reference types.\n• A primitive-type variable can store exactly one value of its declared type at a time.\n• Primitive-type instance variables are initialized by default. Variables of types byte, char, short, int, long, float and double are initialized to 0. Variables of type boolean are initialized to false.\n# 3.2. Reference types Reference types in Java are non-primitive data types. It\u0026rsquo;s called reference ecause it holds the memory address (or reference) of the objects.\nIn Java, all objects are allocated on Heap. This is different from C++ where objects can be allocated memory either on Stack or on Heap.\nWhenever you use new, an object is created on the heap. Local variables are stored on the stack. That includes primitives (such as int) and the references to any objects created. The actual objects themselves aren\u0026rsquo;t created on the stack, as I mentioned when you use new they\u0026rsquo;ll be created on the heap. https://stackoverflow.com/a/8061692/16317008 1 2 3 4 5 // variable age: primitive, stored on stack int age = 77; // variable person: reference, stored on stack, // its value is the address of the object stored on heap Person person = new Person(); Just as men and women are fundamentally different (according to John Gray, author of Men Are from Mars, Women Are from Venus), primitive variables and object reference variables differ from each other in multiple ways. The basic difference is that primitive variables store the actual values, whereas reference variables store the addresses of the objects they refer to. Let’s assume that a class Person is already defined. If you create an int variable a, and an object reference variable person, they will store their values in memory as shown in figure 2.13. https://stackoverflow.com/a/32049775/16317008\nNote that similar to Python, strings in Java are objects, and they are immutable.\n# 4. Golang Golang is static language, golang has no concept of object, all are values. So you can think the variable have types or values have types, all are fine.\nGolang has no refernce-type, all are values. But there is a concept of pointer, some types like slice, map, channel, function which are a pointer or a struct that has a pointer element, we usually catagorize them as reference-type.\n# 5. C++ C++ is a static language which means the variable have types, not values have types.\nC++ has variable, reference and object. A variable is a named object that can hold a value of a specific data type. A reference is an alias for a variable, which means that it refers to the same memory location as the variable it is referencing. An object is used to store a value in memory.\nA variable is an object that has a name (identifier). Naming our objects let us refer to them again later in the program. Although objects in C++ can be unnamed (anonymous), more often we name our objects using an identifier. An object with a name is called a variable.\n1 int x; // define a variable named x, of type int In C++, we use objects to access memory. A named object is called a variable. Variables have an identifier, a type, and a value (and some other attributes that aren’t relevant here). A variable’s type is used to determine how the value in memory should be interpreted.\nYou can think of RAM as a series of numbered boxes that can be used to store data while the program is running. In some older programming languages (like Applesoft BASIC), you could directly access these boxes (e.g. you could write a statement to “go get the value stored in mailbox number 7532”). In C++, direct memory access is discouraged. Instead, we access memory indirectly through an object. An object is a region of storage (usually memory) that can store a value, and has other associated properties.\n1.3 — Introduction to objects and variables – Learn C++\n# 6. Conclusion Variables, values, types, the concepts of these terms may a little different. Don\u0026rsquo;t need to remember what exactly these terms mean in each language, our goal is to know the behavior of the language so that we can use the language correctly and efficiently. Such as pass by value or reference, and if can return a reference of a locla variable.\n","date":"2023-11-28T20:50:06Z","permalink":"https://blog.yorforger.cc/p/data-types-in-programming-languages/","title":"Data Types in Programming Languages"},{"content":" # 1. Compiled vs interpreted language Programming languages are for humans to read and understand. The program (source code) must be translated into machine language so that the computer can execute the program. The time when this translation occurs depends on whether the programming language is a compiled language or an interpreted language. Instead of translating the source code into machine language before the executable file is created, an interpreter converts the source code into machine language at the same time the program runs. So you can\u0026rsquo;t say a language doesn’t have compilation step, because any language needs to be translated to machine code.\n# 2. Statically vs dynamically typing Also know as statically/dynamically typed, static/dynamic language.\nStatic Typing:\nIn statically typed languages, the type of a variable is known at compile time.\nThe programmer must explicitly declare the data type of each variable.\nExamples of statically typed languages include Java, C, C++, and Go.\nStatic typing allows for early detection of type-related errors during the compilation process.\nDynamic Typing:\nIn dynamically typed languages, the type of a variable is determined at runtime. The programmer does not need to explicitly declare the data type of each variable. Examples of dynamically typed languages include Python, JavaScript, Ruby, and PHP. Type checking occurs during runtime, which means that type-related errors may only be discovered when the code is executed. For example in Java:\n1 2 String str = \u0026#34;Hello\u0026#34;; //statically typed as string str = 5; //would throw an error since java is statically typed Whereas in a dynamically typed language the type is dynamic, meaning after you set a variable to a type, you CAN change it. That is because typing is associated with the value rather than the variable. For example in Python:\n1 2 str = \u0026#34;Hello\u0026#34; # it is a string str = 5 # now it is an integer; perfectly OK # 3. Strong vs weak typing The strong/weak typing in a language is related to implicit type conversions (partly taken from @Dario\u0026rsquo;s answer):\nFor example in Python:\n1 2 str = 5 + \u0026#34;hello\u0026#34; # would throw an error since it does not want to cast one type to the other implicitly. whereas in PHP:\n1 2 $str = 5 + \u0026#34;hello\u0026#34;; // equals 5 because \u0026#34;hello\u0026#34; is implicitly casted to 0 // PHP is weakly typed, thus is a very forgiving language. Static typing allows for checking type correctness at compile time. Statically typed languages are usually compiled, and dynamically typed languages are interpreted. Therefore, dynamicly typed languages can check typing at run time.\nSource: https://stackoverflow.com/a/34004765/16317008\n# 4. Conclusion Dynamically typing languages (where type checking happens at run time) can also be strongly typed (Python for example).\nNote that in dynamically checking languages, values have types, not variables (have types). Whereas, in statically checking languages, variables have types.\nStatic/Dynamic Typing is about when type information is acquired (Either at compile time or at runtime)\nStrong/Weak Typing is about how strictly types are distinguished (e.g. whether the language tries to do an implicit conversion from strings to numbers).\n","date":"2023-11-28T20:30:17Z","permalink":"https://blog.yorforger.cc/p/typing-in-programming-language/","title":"Typing in Programming Language"},{"content":"Previous post: Typing in Programming Language - David\u0026rsquo;s Blog\n# 1. Javascript JavaScript is a dynamic language with dynamic types. Variables in JavaScript are not directly associated with any particular value type, and any variable can be assigned (and re-assigned) values of all types:\n1 2 3 let foo = 42; // foo is now a number foo = \u0026#34;bar\u0026#34;; // foo is now a string foo = true; // foo is now a boolean This is in dynamic languages (dynamically typing), values have types, not variables.\nYou don\u0026rsquo;t need know what is dynamic languages, you just need know values have types, not variables.\nJavaScript is also a weakly typed language, which means it allows implicit type conversion when an operation involves mismatched types, instead of throwing type errors.\n1 2 3 const foo = 42; // foo is a number const result = foo + \u0026#34;1\u0026#34;; // JavaScript coerces foo to a string, so it can be concatenated with the other operand console.log(result); // 421 As you can see, weakly typed language means it allows implicit type conversion when an operation involves mismatched types, instead of throwing type errors.\n# 2. Python Python is dynamic language, and is strongly typed.\n1 2 bob = 1 bob = \u0026#34;bob\u0026#34; This works because the variable does not have a type; it can name any object. After bob=1, you\u0026rsquo;ll find that type(bob) returns int, but after bob=\u0026quot;bob\u0026quot;, it returns str. (Note that type is a regular function, so it evaluates its argument, then returns the type of the value.)\n1 2 3 4 5 6 # Attempting to add a string and an integer string_var = \u0026#34;Hello\u0026#34; integer_var = 42 # This line will result in a TypeError result = string_var + integer_var # 3. C/C++ Bbviously, they are static language, but strong or weak?\nIt\u0026rsquo;s hard to classify every language into \u0026lsquo;weakly\u0026rsquo; or \u0026lsquo;strongly\u0026rsquo; typed \u0026ndash; it\u0026rsquo;s more of a continuum. But, in comparison to other languages, C is fairly strongly typed. Every object has a compile-time type, and the compiler will let you know (loudly) if you\u0026rsquo;re doing something with an object that its type doesn\u0026rsquo;t let you do. For example, you can\u0026rsquo;t call functions with the wrong types of parameters, access struct/union members which don\u0026rsquo;t exist, etc.\nBut there are a few weaknesses. One major weakness is typecasts - they essentially say that you\u0026rsquo;re going to be mucking around with the types of objects, and the compiler should be quiet (when it can). void* is also another weakness \u0026ndash; it\u0026rsquo;s a generic pointer to an unknown type, and when you use them, you have to be extra careful that you\u0026rsquo;re doing the right thing. The compiler can\u0026rsquo;t statically check most uses of void*. void* can also be converted to a pointer to any type without a cast (only in C, not in C++), which is another weakness.\nhttps://stackoverflow.com/a/430204/16317008\n# 4. Java Java is static and strongly typed.\nIn Java or C/C++, every variable must have a declared type, and the type is checked at compile-time. Once a variable is declared with a specific type, it cannot be assigned a value of a different type. Here\u0026rsquo;s an example:\n1 2 int num = 10; // The variable \u0026#39;num\u0026#39; is declared as an integer num = \u0026#34;Hello\u0026#34;; // This will result in a compilation error because we are trying to assign a string to an integer variable Strongly Typed: Java is also a strongly typed language, which means that type conversions or implicit type coercion are limited. In Java, you cannot perform operations between incompatible types without explicitly converting them. Here\u0026rsquo;s an example:\n1 2 3 int num1 = 10; String str = \u0026#34;20\u0026#34;; int sum = num1 + str; // This will result in a compilation error because we are trying to add an integer and a string without explicit conversion # 4. Golang Golang is indeed a static and strongly typed language:\n1 2 3 a := 2 b := 3.2 fmt.Println(a + b) // invalid operation: a + b (mismatched types int and float64) ","date":"2023-11-28T19:20:17Z","permalink":"https://blog.yorforger.cc/p/typing-in-programming-language-example/","title":"Typing in Programming Language - Example"},{"content":" # 1. Concurrency models Processes Threads (system or green) Futures Asynchronous operations, non-blocking, single-threaded Coroutines CSP Actor Learn more: java - What\u0026rsquo;s the difference between a Future and a Promise? - Stack Overflow\n# 2. Promise object A wrapper for a resolved value or a reason that it\u0026rsquo;s not resolved yet. But with some powerful methods, allow you handle different situations. Like the then chain, the catch Non-blocking: enable you to write asynchronous code in a synchronous manner. Most important feature. There are more complicated methods, The Promise object has several fields and methods, including:\nstate: A private field that represents the current state of the promise (pending, fulfilled or rejected). result: A private field that holds the result value if the promise is fulfilled or the reason if it is rejected. then(), catch(), finally() methods, only get executed when the state of Promise object is fulfilled or rejected. # 3 Use await/async with Promise object 1 2 3 4 5 6 7 8 9 10 11 12 13 14 async function main() { let response try { // \u0026#39;response\u0026#39; is not a Promise object, // it\u0026#39;s the resolved value the in the Promise object response = await fetch(\u0026#39;https://www.google.com\u0026#39;); } catch (err) { console.error(\u0026#34;an err happened\u0026#34;); return } console.log(response.ok); } main().then() You may wonder why we need call then() after main(), the reason is that async functions always return a promise implicitly.\nIf the return value of an async function is not explicitly a promise, it will be implicitly wrapped in a promise. An async function is really just a fancy Promise wrapper.\nIf you call main like this:\n1 2 3 ... await main(); There will be an error:\n1 2 3 4 await main(); ^^^^^ SyntaxError: await is only valid in async functions and the top level bodies of modules This is another syntax to call async function:\n1 2 3 (async () =\u0026gt; { await main() })() # Use then() with Promise object 1 2 3 fetch(\u0026#39;https://www.google.com\u0026#39;) .then(response =\u0026gt; console.log(response.status)) .catch(err =\u0026gt; console.error(\u0026#34;An err occurred.\u0026#34;)) Because fetch() retuens a Promise object, we can call then() directly, this looks more concise than ealier example.\nBut we usually use await/async instead of then() for readability in real world projects, because we need handle errors and consider different situations.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 async function fetchMessage(messages) { setLoading(true) controller.current = new AbortController() try { const response = await fetch(fetchPath, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39;: localStorage.getItem(\u0026#39;token\u0026#39;), }, body: JSON.stringify({messages}), signal: controller?.current?.signal, }); const data = await response.json(); setLoading(false) if (!response.ok) { // remove the last message setMessages((messages) =\u0026gt; messages.slice(0, -1)); message.error({ content: \u0026#34;获取信息失败, 请联系截图主人喵~: \u0026#34; + data.error, duration: 5, }); return; } return data; } catch (e) { setLoading(false) // remove the last message setMessages((messages) =\u0026gt; messages.slice(0, -1)); if (e.name === \u0026#39;AbortError\u0026#39;) { return } message.error(\u0026#34;获取信息失败, 请联系截图主人喵~: \u0026#34; + e); } } ","date":"2023-11-28T12:34:29Z","permalink":"https://blog.yorforger.cc/p/promise-object-in-javascript/","title":"Promise Object in Javascript"},{"content":" # 1. An example to feel what it is enqueueLinks: is used to add the links exist in current page to a queue for crawler to process. maxRequestsPerCrawl: control the maximum of requests the crawler can make, you should always set it. headless: set it false, you will see all the actions on browser, browser will click link itself (controlled by your js program). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import { PlaywrightCrawler, Dataset } from \u0026#39;crawlee\u0026#39;; // PlaywrightCrawler crawls the web using a headless // browser controlled by the Playwright library. const crawler = new PlaywrightCrawler({ // Use the requestHandler to process each of the crawled pages. async requestHandler({ request, page, enqueueLinks, log }) { const title = await page.title(); log.info(`Title of ${request.loadedUrl} is \u0026#39;${title}\u0026#39;`); // Extract links from the current page // and add them to the crawling queue. await enqueueLinks(); }, // Uncomment this option to see the browser window. // headless: false, // This value should always be set in order to prevent infinite loops in misconfigured crawlers. maxRequestsPerCrawl: 3, }); // Add first URL to the queue and start the crawl. await crawler.run([\u0026#39;https://github.com/apify/crawlee\u0026#39;]); Output:\n1 2 3 4 5 6 7 8 9 10 11 12 ❯ npm start \u0026gt; my-crawler@0.0.1 start \u0026gt; node src/main.js INFO PlaywrightCrawler: Starting the crawler. INFO PlaywrightCrawler: Title of https://github.com/apify/crawlee is \u0026#39;GitHub - apify/crawlee\u0026#39; INFO PlaywrightCrawler: Title of https://github.com/ is \u0026#39;GitHub: Let’s build from here · GitHub\u0026#39; INFO PlaywrightCrawler: Title of https://github.com/signup?ref_cta=Sign+up\u0026amp;ref_loc=header+logged+out\u0026amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E\u0026amp;source=header-repo is \u0026#39;Join GitHub · GitHub\u0026#39; INFO PlaywrightCrawler: Crawler reached the maxRequestsPerCrawl limit of 3 requests and will shut down soon. ... INFO PlaywrightCrawler: Finished! Total 4 requests: 4 succeeded, 0 failed. {\u0026#34;terminal\u0026#34;:true} As you can see, the output above: Crawler reached the maxRequestsPerCrawl limit of 3 requests\u0026hellip;\nYou probably notice that we just specified only one url, why the crawler makes 3 requests?\nIt makes three requests because we set maxRequestsPerCrawl: 3 above. Besides, because of we added parameter enqueueLinks, which pushes the all links (exist in the current page) into the request queue of the crawler, and this is a loop, so this is why we need set maxRequestsPerCrawl: 3.\n# 2. CheerioCrawler, PuppeteerCrawler and PlaywrightCrawler # 2.1. CheerioCrawler Unless you have a good reason to start with a different one, you should try building a CheerioCrawler first. It is an HTTP crawler with HTTP2 support, anti-blocking features and integrated HTML parser - Cheerio. It\u0026rsquo;s fast, simple, cheap to run and does not require complicated dependencies. The only downside is that it won\u0026rsquo;t work out of the box for websites which require JavaScript rendering. But you might not need JavaScript rendering at all, because many modern websites use server-side rendering.\nUses headless Chrome browser, doesn\u0026rsquo;t support headful browser.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Add import of CheerioCrawler import { CheerioCrawler } from \u0026#39;crawlee\u0026#39;; const crawler = new CheerioCrawler({ // The `$` argument is the Cheerio object // which contains parsed HTML of the website. async requestHandler({ $, request }) { const title = $(\u0026#39;title\u0026#39;).text(); console.log(`The title of \u0026#34;${request.url}\u0026#34; is: ${title}.`); }, maxRequestsPerCrawl: 3, }) // Start the crawler with the provided URLs await crawler.run([\u0026#39;https://crawlee.dev\u0026#39;]); Learn more:\nQuick Start | Crawlee First crawler | Crawlee # 3. Real world example You can learn a lot from this:\nGetting some real-world data | Crawlee\nScraping the Store | Crawlee\n","date":"2023-11-26T21:29:35Z","permalink":"https://blog.yorforger.cc/p/clawlee-library-get-started/","title":"Clawlee Library - Get Started"},{"content":" # 1. Web crawler A Web crawler, sometimes called a spider or spiderbot and often shortened to crawler, is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing (web spidering). Web crawler\n# 2. Headless browser A headless browser is a web browser that operates without a graphical user interface (GUI). It runs in the background and interacts with web pages programmatically, just like a regular browser, but without displaying the web page to the user. Headless browsers are commonly used in web crawling and scraping applications.\nLearn more: Web Scraping Without Getting Blocked | ScrapeOps\n# 3. Playwright library Playwright is an open-source automation library for browser testing and web scraping developed by Microsoft and launched on 31 January 2020, which has since become popular among programmers and web developers. Playwright provides the ability to automate browser tasks in Chromium, Firefox and WebKit with a single API.\nPlaywright is a framework for Web Testing and Automation. It allows testing Chromium, Firefox and WebKit with a single API. microsoft/playwright\n# 4. JavaScript rendering JavaScript rendering is the process of executing JavaScript on a page to make changes in the page\u0026rsquo;s structure or content. It\u0026rsquo;s also called client-side rendering, the opposite of server-side rendering. Some modern websites render on the client, some on the server and many cutting edge websites render some things on the server and other things on the client.\nJavascript uses the document object model (DOM) to manipulate the DOM elements. Rendering refers to showing the output in the browser.\nLearn more:\nWhat is JavaScript rendering?\nJavaScript rendering | Crawlee\n# 5. Anti-Scraping It refers to all techniques, tools, and approaches to protect online data against scraping. In other words, anti-scraping makes it more difficult to automatically extract data from a web page. Specifically, it\u0026rsquo;s about identifying and blocking requests from bots or malicious users.\nLearn more: 7 Anti-Scraping Techniques You Need to Know - ZenRows\n# 6. Is there an easy way to tell if a website will allow scrapers or not? To find out if the website allows scraping then check out:\nRobots.txt file: This will say which pages can you access with a scraper.\nTerms \u0026amp; Conditions: Their T\u0026amp;Cs will normally say if they permit scraping, programmatic access, etc.\nCheck the link below, at the end there is: You will not access the Site through automated or non-human means, such as with bots, scripts or otherwise;\u0026hellip;. When you login you usually see: I have read and understood the Privacy Policy and the Terms of Use. Presence of Anti-Bot Protection: If you are getting ban pages from Cloudflare, Distill Network, Imperva, DataDome, etc. then it is highly likely that the website doesn\u0026rsquo;t want you to scrape them.\nIf you are getting blocked but still want to scrape the website then you can do the following:\nUser-Agents \u0026amp; Headers: You need to use fake user-agents when scraping as most HTTP clients clearly identify themselves by default, giving you away as a scraper. Better yet you should use full fake browser headers that match what a real browser should send to a website. More about how to optimize and generate browser headers here. Rotating Proxies: You should send your requests via a rotating proxy pool that will make it harder for the website to detect you as a scraper. Residential \u0026amp; Mobile proxies are better than datacenter proxies but are a lot more expensive. You can compare proxy provider plans here. Fortified Headless Browser: Depending on the anti-bot protection the website is using you make need to use a fortified headless browser that can solve its JS challenges without giving its identity away. Your options include the Puppeteer stealth plugin and Selenium undetected-chromedriver. Learn more about it here Guide to Web Scraping Without Getting Blocked.\nLearn more: https://www.reddit.com/r/webscraping/comments/xvqd6r/comment/ir4m0xx/?utm_source=share\u0026utm_medium=web2x\u0026context=3\n","date":"2023-11-26T19:29:35Z","permalink":"https://blog.yorforger.cc/p/basic-concepts/","title":"Basic Concepts"},{"content":" # 1. Basics concepts Try to understand the exception information (Traceback (most recent call last)) :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def foo(): print(10 * (1 / 0)) def main(): foo() if __name__ == \u0026#34;__main__\u0026#34;: main() Traceback (most recent call last): File \u0026#34;main.py\u0026#34;, line 10, in \u0026lt;module\u0026gt; main() File \u0026#34;main.py\u0026#34;, line 6, in main foo() File \u0026#34;main.py\u0026#34;, line 2, in foo print(10 * (1 / 0)) ZeroDivisionError: division by zero As you can see The last line of the error message indicates what happened: print(10 * (1 / 0)).\nErrors detected during execution are called exceptions. Errors and Exceptions — Python 3.12.0\n# 2. Types of Exceptions BaseException is the common base class of all exceptions. One of its subclasses, Exception, is the base class of all the non-fatal exceptions. Exceptions which are not subclasses of Exception are not typically handled, because they are used to indicate that the program should terminate. They include SystemExitwhich is raised by sys.exit() and KeyboardInterruptwhich is raised when a user wishes to interrupt the program.\nExceptioncan be used as a wildcard that catches (almost) everything. However, it is good practice to be as specific as possible with the types of exceptions that we intend to handle, and to allow any unexpected exceptions to propagate on.\n# 3. Handle Exceptions The most common pattern for handling Exception is to print or log the exception and then re-raise it (allowing a caller to handle the exception as well):\n1 2 3 4 5 6 7 8 9 10 11 12 import sys try: f = open(\u0026#39;myfile.txt\u0026#39;) s = f.readline() i = int(s.strip()) except OSError as err: print(\u0026#34;OS error:\u0026#34;, err) raise except Exception as err: print(f\u0026#34;Unexpected {err=}, {type(err)=}\u0026#34;) raise # Predefined clean-up actions Some objects define standard clean-up actions to be undertaken when the object is no longer needed, regardless of whether or not the operation using the object succeeded or failed. Look at the following example, which tries to open a file and print its contents to the screen.\n1 2 for line in open(\u0026#34;myfile.txt\u0026#34;): print(line, end=\u0026#34;\u0026#34;) The problem with this code is that it leaves the file open for an indeterminate amount of time after this part of the code has finished executing. This is not an issue in simple scripts, but can be a problem for larger applications. The with statement allows objects like files to be used in a way that ensures they are always cleaned up promptly and correctly.\n1 2 3 with open(\u0026#34;myfile.txt\u0026#34;) as f: for line in f: print(line, end=\u0026#34;\u0026#34;) After the statement is executed, the file f is always closed, even if a problem was encountered while processing the lines. Objects which, like files, provide predefined clean-up actions will indicate this in their documentation.\n# 4. Performance performance - How is exception handling implemented in Python? - Stack Overflow:\nI\u0026rsquo;ve learned from other SO answers that a try/catch block is cheaper than an if/else statement if the exception is expected to be raised rarely, and that it\u0026rsquo;s the call depth that\u0026rsquo;s important because filling the stacktrace is expensive. This is probably principally true for all programming languages.\nIf you throw an exception, all functions will be exited back to the point where it finds a try...catch block with a matching catch type. If your function isn\u0026rsquo;t called from within a try block, the program will exit with an unhandled exception.\nFind some discussions, from: c# - Do try/catch blocks hurt performance when exceptions are not thrown? - Stack Overflow\nDuring a code review with a Microsoft employee we came across a large section of code inside a try{} block. She and an IT representative suggested this can have effects on performance of the code. In fact, they suggested most of the code should be outside of try/catch blocks, and that only important sections should be checked. The Microsoft employee added and said an upcoming white paper warns against incorrect try/catch blocks.\ntry catch blocks have a negligible impact on performance but exception Throwing can be pretty sizable, this is probably where your coworker was confused.\nAnother disscussion: java - Is it expensive to use try-catch blocks even if an exception is never thrown? - Stack Overflow\ntry has almost no expense at all. Instead of doing the work of setting up the try at runtime, the code\u0026rsquo;s metadata is structured at compile time such that when an exception is thrown, it now does a relatively expensive operation of walking up the stack and seeing if any try blocks exist that would catch this exception. From a layman\u0026rsquo;s perspective, try may as well be free. It\u0026rsquo;s actually throwing the exception that costs you - but unless you\u0026rsquo;re throwing hundreds or thousands of exceptions, you still won\u0026rsquo;t notice the cost.\nAnother post: What are the effects of exceptions on performance in Java? - Stack Overflow\nIn Java much of the expense of throwing an exception is the time spent gathering the stack trace, which occurs when the exception object is created. The actual cost of throwing the exception, while large, is considerably less than the cost of creating the exception.\nhttps://stackoverflow.com/a/8024032/16317008\n# 5. When to catch # 5.1. At the lowest possible level This is the level at which you are integrating with third party code, such as an ORM tool or any library performing IO operations (accessing resources over HTTP, reading a file, saving to the database, you name it). That is, the level at which you leave your application’s native code to interact with other components.\nThe guidelines in this scenario are:\nHandle only specific exceptions, such as SqlTimeoutException or IOException. Never handle a generic exception (of type Exception) Handle it only if you have something meaningful to do about it, such as retries, triggering compensatory actions, or adding more data to the exception (e.g. context variables), and then re-throw it Do not perform logging here Let all other exceptions bubble up as they will be handled by the second case # 5.2. At the highest possible level This would be the last place where you can handle the exception before it is thrown directly to the user.\nYour goal here is to log the error and forward the details to the programmers so they can identify and correct the error. Add as much information as possible, record it, and then show an apology message to the user, as there’s probably nothing they can do about it, especially if it is a bug in the software.\nThe guidelines in this second scenario are:\nHandle the generic Exception class Add more information from current execution context Log the error and notify the programmers Show an apology to the user Work it out as soon as you can Learn more, very good explanation: https://stackoverflow.com/a/59511485\n# 6. When to throw exceptions Easier to explain in the context of developing a library. You should throw when you reached an error and there\u0026rsquo;s nothing more you can do besides letting the consumer of your APIs know about it, and letting them decide.\nImagine you\u0026rsquo;re the developer of some data access library. When you reach a network error, there\u0026rsquo;s nothing you can do besides throwing an exception. That\u0026rsquo;s an irreversible error from a data access library standpoint.\nLearn more, very good explanation: https://stackoverflow.com/a/59511485\n","date":"2023-11-24T09:37:26Z","permalink":"https://blog.yorforger.cc/p/exception-handling-expensive-or-not-python/","title":"Exception Handling \u0026 Expensive or Not Python"},{"content":" # 1. Definition yield is a keyword that is used like return, but it will make the function return a generator.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def create_generator(): print(\u0026#34;create_generator\u0026#34;) for i in range(10): yield i ite = create_generator() print(ite) # ite is a generator object for i in ite: print(i) # \u0026lt;generator object create_generator at 0x102d2bdd0\u0026gt; # create_generator # 0 # 1 # 2 When you call a function that contains a yield statement, you get a generator object, but no code runs. Then each time you extract from the generator, Python resumes the function from where it left off (from the yield), runs until the next yield, and then pauses again.\n# 2. Why yield Handling Large Data: If you\u0026rsquo;re working with a large amount of data that doesn\u0026rsquo;t fit into memory, a generator can be a great solution. It allows you to process one item at a time rather than loading everything into memory at once.\nYield is more efficient, memory-wise, and also sometimes execution-wise. If you iterate over a list of 1,000,000 elements, Python has to generate the entire list and store the contents in memory before beginning the first iteration. With a generator (using yield), the elements are created at the time of iteration, so 1,000,000 elements don’t need to be pre-calculated first and stored in memory.\nThere are other benefits such as using generators with async or performing logic after certain iterations (that generators can potentially give better control over than iterating over a list), but in my opinion the main reason for using them is the memory and processing considerations.\nhttps://www.reddit.com/r/learnpython/comments/rzukrb/comment/hrxetbx/?utm_source=share\u0026utm_medium=web2x\u0026context=3\nYep its 99% about memory. First time I used it was when I was working with huge amounts of data from a SQL server. Rather than do analysis on all 100,000,000 lines at once, not joking about how many, I pulled back 10k at a time used yield to send the 10k to the analysis function. Then sent the updated data on to the system that needed it. Used like 20MB of RAM as apposed to like 200GB.\nhttps://www.reddit.com/r/learnpython/comments/rzukrb/comment/hrxhm46/?utm_source=share\u0026utm_medium=web2x\u0026context=3\n# 3. Example - read large file The files range between 1Gb and ~20Gb in length which is too big to read into RAM. So I would like to read the lines in chunks/bins of say 10000 lines at a time so that I can perform calculations on the final column in these bin sizes.\nInstead of playing with offsets in the file, try to build and yield lists of 10000 elements from a loop:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def read_large_file(file_handler, block_size=10000): block = [] for line in file_handler: block.append(line) if len(block) == block_size: yield block block = [] # don\u0026#39;t forget to yield the last block if block: yield block with open(path) as file_handler: for block in read_large_file(file_handler): print(block) Learn more: using a python generator to process large text files - Stack Overflow\n# 4. Example - async 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from typing import Iterator, Any, cast from openai import OpenAI client = OpenAI() def chat(messages: list, stream: bool) -\u0026gt; Iterator[str]: response_iter = cast( # The Any type is a special type that represents an unconstrained value # and is often used when the type of the value is not known or needs to be dynamically determined. Any, # client.chat.completions.create() won\u0026#39;t return immediately, it will make requests to the API. # time.sleep(1), you can umcomment this to stimulate the time of the request. client.chat.completions.create( messages=messages, stream=stream, model=\u0026#34;gpt-3.5-turbo-1106\u0026#34;, max_tokens=200, ), ) if stream: for response in response_iter: if response.choices[0].delta.content is not None: chunk = response.choices[0].delta.content yield chunk else: yield response_iter.choices[0].message.content msg = [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello, help me write a essay about the history of the United States of America.\u0026#34;}] steam = True ite = chat(msg, steam) # async: you will get a generator object, but no code in function chat() runs for i in ite: print(i, end=\u0026#34;\u0026#34;, flush=True) ","date":"2023-11-23T16:04:26Z","permalink":"https://blog.yorforger.cc/p/yeild-generator-python/","title":"Yeild \u0026 Generator Python"},{"content":"首先可以联网, 谷歌和微软邮箱等其他的一些网站都可访问, 但是访问油管的时候总是会超时,\n首先想到了是 DNS 服务器的问题, 尝试把电脑的 DNS server 改为 8.8.8.8, 可惜需要管理员权限, 失败\n后来无聊想着看看解析出来youtube.com的实际ip是什么, 使用 nklookup 查了一下, (大概这样, 没在实验室)\n1 2 3 4 5 6 7 ❯ nslookup youtube.com Server:\t192.168.2.1 Address:\t192.168.100.1#53 Non-authoritative answer: Name:\tyoutube.com Address: 142.251.40.206 于是访问 142.251.40.206\n1 2 3 4 5 6 7 ❯ wget -O- 142.251.40.206 --2023-11-22 22:23:53-- http://142.251.40.206/ Connecting to 142.251.40.206:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: http://www.google.com/ [following] --2023-11-22 22:23:53-- http://www.google.com/ Resolving www.google.com (www.google.com)... 142.250.80.36 可以看到被重定向到了谷歌页面, 当时在实验室也是这种情况, 查了一下, 这确实是正确的地址, 只是谷歌给重定向了, 所以不是DNS的问题, 那应该就是防火墙的问题, 下次去实验室再看看,\n另外上面 nskookup 返回的是 Non-authoritative answer, 意思是 ip 地址不是由该域名的 NS 服务器返回的, 而是从 DNS cache获取的, 所以 Non-authoritative answer 并不是代表不安全或者不可靠, 但可能不是最新的, 比如你的域名刚更新了 NS server, 加了层 Cloudflare 的 https 加密代理, 即 your_domain-\u0026gt;proxy-ip-\u0026gt;your_ip, 而学校的 DNS cache 里缓存的却是你之前的 ip, 即 your_domain -\u0026gt; your_ip, 那代理就不会生效,\n去实验室查了一下我域名的NS, 竟然 time-out:\n1 2 3 4 5 6 7 c: \\Users\\student\u0026gt;nslookup shaowenzhu.top Server: Unknown Address:192.168.102.1 DNS request timed out. timeout was 2 seconds. ... 查了一下, 看到了一个与我相似的问题:\nDNS request timed out means NSLookup submitted the query to the DNS server, but did not get a response.\nIt\u0026rsquo;s possible the DNS server you queried was having a problem and couldn\u0026rsquo;t reply. Network errors could be to blame as well.\nHowever, given that you did this from a \u0026ldquo;public computer,\u0026rdquo; the most likely explanation is that your DNS lookup was blocked by the network\u0026rsquo;s firewall. It\u0026rsquo;s common for network administrators to require that DNS lookups originating from nodes on the internal network be done using approved DNS servers. These can either be DNS servers under the administrator\u0026rsquo;s control, or specific public DNS servers selected by the admin.\n突然想到 DNS 缓存在 Chrome 上也有备份, 于是去查了一下是不是可以修改 DNS server 在 Chrome 上, (在电脑上没有管理员权限),\n真的有, 具体参考(privacy-security -\u0026gt; Advanced -\u0026gt; Use Secure DNS): How to Change DNS Server in Google Chrome on Computer and Mobile? - MiniTool\n然后可以在Chrome上通过域名访问我的网站了, 终于不用输入 IP 了每次, nnd\n","date":"2023-11-22T21:43:22Z","permalink":"https://blog.yorforger.cc/p/%E6%8E%92%E6%9F%A5%E5%AE%9E%E9%AA%8C%E5%AE%A4%E7%94%B5%E8%84%91%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE%E4%B8%80%E4%BA%9B%E7%BD%91%E7%AB%99/","title":"排查实验室电脑无法访问一些网站"},{"content":"URLs can only be sent over the Internet using the ASCII character-set. Since URLs often contain characters outside the ASCII set, the URL has to be converted into a valid ASCII format.\nPercent encoding replaces special characters with a \u0026ldquo;%\u0026rdquo; followed by two hexadecimal digits that represent the character\u0026rsquo;s ASCII code.\nURLs cannot contain white spaces. URL encoding normally replaces a white space with a plus (+) sign or with %20. The whitespace character in ASCII has hexadecimal value 0x20 In URLs, the forward slash (/) is used as a path separator to separate different elements of the URL. In the case of the forward slash, its ASCII code is 2f, so it is represented as %2f in a URL. If the special character (Chinese) doesn\u0026rsquo;t exist in ASCII table, it will use the UTF-8 hexadecimal digits of that special character.\nFor example, if you want to include the Chinese characters \u0026ldquo;中国\u0026rdquo; (China) in a URL, its UTF-8 encoding would be \u0026ldquo;%E4%B8%AD%E5%9B%BD\u0026rdquo;. Each Chinese character is encoded as its three-byte UTF-8 representation, prefixed with a percent sign.\n","date":"2023-11-20T22:29:19Z","permalink":"https://blog.yorforger.cc/p/url-encoding-percent-encoding/","title":"URL Encoding (Percent Encoding)"},{"content":" # 1. OSPF, BGP and MPLS OSPF: OSPF is an interior gateway protocol (IGP) used for routing within an autonomous system (AS). It determines the best paths for routing packets based on metrics such as bandwidth, delay, and cost. ﻿﻿BGP: BGP is an exterior gateway protocol (EGP) used for routing between autonomous systems. It enables the exchange of routing information between different networks and determines the best paths between ASes. ﻿﻿MPLS: MPLS is not a routing protocol but a forwarding mechanism that operates at the network layer. It uses labels to encapsulate packets, creating virtual paths called Label Switched Paths (LSPs). MPLS allows for efficient forwarding and traffic engineering by quickly switching packets based on labels instead of performing complex IP routing. Routers refer to internal routing tables to make decisions about how to route packets along network paths. A routing table records the paths that packets should take to reach every destination that the router is responsible for.\n# 2. MPLS # 2.1. Basic concepts In a network that uses MPLS, each packet is assigned to a class called a forwarding equivalence class (FEC). The network paths that packets can take are called label-switched paths (LSP). A packet\u0026rsquo;s class (FEC) determines which path (LSP) the packet will be assigned to. Packets with the same FEC follow the same LSP.\nEach packet has one or more labels attached, and all labels are contained in an MPLS header, which is added on top of all the other headers attached to a packet. FECs are listed within each packet\u0026rsquo;s labels. Routers do not examine the packet\u0026rsquo;s other headers; they can essentially ignore the IP header. Instead, they examine the packet\u0026rsquo;s label and direct the packet to the right LSP.\nBecause MPLS-supporting routers only need to see the MPLS labels attached to a given packet, MPLS can work with almost any protocol (hence the name \u0026ldquo;multiprotocol\u0026rdquo;). It does not matter how the rest of the packet is formatted, as long as the router can read the MPLS labels at the front of the packet.\nLabel value: the name says it all, this is where you will find the value of the label. EXP: these are the three experimental bits. These are used for QoS, normally the IP precedence value of the IP packet will be copied here. S: this is the “bottom of stack” bit. With MPLS it’s possible to add more than one label, you’ll see why in some of the MPLS VPN lessons. When this bit is set to one, it’s the last MPLS header. When it’s set to zero then there is one or more MPLS headers left. TTL: just like in the IP header, this is the time to live field. You can use this for traces in the MPLS network. Each hop decrements the TTL by one. # 2.2. FECs FEC is a term used in MPLS networks to describe a set of incoming packets with similar characteristics, allowing those packets to be allocated the same label and forwarded down the same LSP (Label Switched Path). FEC - Forwarding Equivalence Class\nIn previous part we said: FECs are listed within each packet\u0026rsquo;s labels. But, wait, how?\nAn FEC is a set of packets that a single router:\n(1) Forwards to the same next hop;\n(2) Out the same interface; and\n(3) With the same treatment (such as queuing).\n# 2.3. Step-by-Step a packet in MPLS network Packet Enters the MPLS Network: Imagine a data packet entering an MPLS network at an \u0026lsquo;ingress router\u0026rsquo;. The ingress router examines the header of the packet (like destination IP address). Label Assignment: Based on this examination, the router assigns a \u0026rsquo;label\u0026rsquo; to the packet. This label is a short, fixed-length identifier. The process of assigning a label is known as \u0026rsquo;label imposition'. Each label corresponds to a pre-determined path through the network, known as a Label Switched Path (LSP). Packet Travels through the MPLS Network: Once the packet has been labeled, it is sent into the MPLS network. As it reaches each router (or label switch router, LSR) in the MPLS network, the router does not need to examine the IP header. Instead, it looks at the label. Based on the label, the LSR quickly determines where to send the packet next. This is a process called \u0026rsquo;label switching'. The LSR may also swap the packet\u0026rsquo;s existing label with a new label before forwarding it (label swapping). This is done to maintain the correct routing path as defined by the LSP. Approaching the Exit of the MPLS Network: As the packet approaches the end of its path through the MPLS network, it reaches an \u0026rsquo;egress router'. The egress router removes the MPLS label from the packet, a process known as \u0026rsquo;label popping'. Packet Leaves the MPLS Network: After the label is removed, the packet is forwarded based on its original IP header. It is now back in a standard IP-based network and can be routed to its final destination using traditional IP routing methods. # 3. OSPF, BGP and MPLS So each packet has a LSP theoretically? which means each packet belongs to a FEC, and each FEC has a LSP, the LSP built by using the routing table formed by OSPF and BGP\nYes, your understanding is on the right track. In MPLS (Multiprotocol Label Switching) networks, the concept of Forwarding Equivalence Classes (FECs) and Label Switched Paths (LSPs) is fundamental. Let\u0026rsquo;s break down these concepts:\n# 3.1. Forwarding Equivalence Class (FEC) A FEC is essentially a group of IP packets that are forwarded in the same manner (same path, same treatment). Each packet that enters an MPLS network is assigned to a FEC. The assignment is based on criteria like destination IP address, IP protocol type, source IP address, and even Layer 4 ports. The idea is that all packets within a FEC will follow the same LSP and receive similar forwarding treatment. Label Switched Path (LSP)\nAn LSP is a pre-established path that packets in a particular FEC will follow through the MPLS network. LSPs are set up by the MPLS control plane, which can use signaling protocols like RSVP-TE or LDP. Once an LSP is established, labels are assigned and used to forward the packets along this path. # 3.2. Interaction with OSPF and BGP OSPF and BGP are used to form the routing table, which provides the necessary information about network topology and available paths. OSPF is typically used within an autonomous system (internal routing), while BGP is used between autonomous systems (external routing). The information from OSPF and BGP helps in determining the best paths for the LSPs. Once the best paths are identified, MPLS signaling protocols set up LSPs along these paths, and labels are distributed to the routers (LSRs) on these paths. # 3.3. Practical Example Imagine a packet entering an MPLS network destined for a specific IP address. This packet is assigned to a FEC based on its destination IP. An LSP for this FEC has already been established using OSPF or BGP routing information. The ingress router assigns an MPLS label to the packet based on its FEC. The packet is then forwarded through the MPLS network, with each router making forwarding decisions based solely on the MPLS label. When the packet reaches the egress router of the MPLS network, the MPLS label is removed, and the packet is forwarded based on its original IP header. In this way, each packet is associated with a FEC, and each FEC has its LSP. The LSPs are constructed using the routing tables formed by OSPF and BGP, ensuring that packets are forwarded efficiently and along optimal paths within the MPLS network.\nReferences:\nWhat is MPLS (multiprotocol label switching)? | Cloudflare MPLS Labels and Devices Understanding the Role of FECs in MPLS | Network World ","date":"2023-11-17T08:27:59Z","permalink":"https://blog.yorforger.cc/p/mpls/","title":"MPLS"},{"content":" # 1. Hosting your domain on Cloudflare (Enable CDN) 1 client \u0026lt;----tls-1----\u0026gt; cloudflare \u0026lt;----tls-2----\u0026gt; your server Note: You cannot login your server with ssh user@your_domain any more after enabling Cloudflare. You need to login with ssh user@your_ip. Cause the Cloudflare acts as a reverse proxy, it only forwards the http/https traffic to your server, and only on some specific ports.\nThere are two steps to add your domain to Cloudflare:\nAdd your domain to Cloudflare Create and install TLS certificate to your server Cloudflare does this by serving as a reverse proxy for your web traffic. Actually, Clooudflare achieves this by using its CDN service, which caches your website\u0026rsquo;s static content and serves it to your visitors from the nearest Cloudflare data center. Refer to our Load Balancing reference architecture to learn more about advanced ways to forward traffic to your origins, as well as our CDN reference architecture to learn more about how Cloudflare processes and optimizes your web traffic.\n# 1.1. Add your domain to Cloudflare Go to this website: https://dash.cloudflare.com/\nAnd you will get instructions for updating your nameserver of your domain. After change your nameserver, waite about one hour, your site will be active on Cloudflare. Then you can choose the TLS encryption mode:\nNote: choose full mode, don\u0026rsquo;t use flexbile mode, otherwise you probably would get ERR_TOO_MANY_REDIRECTS when access your website.\n# 1.2. Install TLS certificate The generated two files cert.pem and cert.key is used for encryption between your server and Cloudflare.\nYou can use these two file like this:\n1 2 3 4 5 6 7 8 ... func main() { http.HandleFunc(\u0026#34;/hello\u0026#34;, HelloServer) err := http.ListenAndServeTLS(\u0026#34;:443\u0026#34;, \u0026#34;./conf/cert.pem\u0026#34;, \u0026#34;./conf/cert.key\u0026#34;, nil) if err != nil { log.Fatal(\u0026#34;ListenAndServe: \u0026#34;, err) } } Note: don\u0026rsquo;t add the keep-alive header when handle request on your server, otherwise you may get a 520 error when you access your website on browser: Error 520: Web server is returning an unknown error\n1 2 w.Header().Set(\u0026#34;Connection\u0026#34;, \u0026#34;Keep-Alive\u0026#34;) w.Header().Set(\u0026#34;Keep-Alive\u0026#34;, \u0026#34;timeout=2, max=1000\u0026#34;) Learn more: https://luyuhuang.tech/2020/06/03/cloudflare-free-https.html\nAfter config this, the DNS needs time to take effect (you change the nameservers of your domain to Cloudfalre from the defatult nameservers, this needs time to take effect)\nIf there still no HTTPS connection but the cloudflare displays your website is active on their service, you may try to check if your server is listening on 443 port and try to flush the DNS cache of your client computer (chrome + system DNS cache). Learn more: DNS Concepts (NameServer(NS), DNS Records and Caching) - David\u0026rsquo;s Blog\nBTW, you can check if your domain is proxied by the Cloudflare with nslookup command, which will get the A Record by default (IPv4) address of your domain, but in this case, with Cloudfalre proxy, you should get the ip of Cloudfalre Name Server.\n1 2 3 4 5 6 ❯ nslookup shaowenzhu.top Non-authoritative answer: Name:\tshaowenzhu.top Address: 172.67.171.207 # not my domain\u0026#39;s real ip, it\u0026#39;s Cloudfalre Name:\tshaowenzhu.top Address: 104.21.47.185 # not my domain\u0026#39;s real ip, it\u0026#39;s Cloudfalre Learn more: Add a site · Cloudflare Fundamentals docs\n# 2. Change A record If you changed a vps, all you need to do is to change the A record of your domain on Cloudflare, you don\u0026rsquo;t need to change the A record on your domain register website.\nCloudflare serves as a reverse proxy, directing all traffic for the specified proxied domain to the target IP address.\n# 3. Allow custom port The default port of https is 443. If you want to use other ports, you need to allow them with firewall first on your server.\nCloudflare only allows the following HTTPS ports:\n443 2053 2083 2087 2096 8443\nVery easy, you don\u0026rsquo;t need to do anything on Cloudflare, just allow the port on your server. Then you can access your website with https://your_domain:port. And your traffic will be encrypted by Cloudflare.\nLearn more:\nHow to allow custom port - Website, Application, Performance / Security - Cloudflare Community\nNetwork ports · Cloudflare Fundamentals docs\nLearn more about CDN:\nWhat is a content delivery network (CDN)? | How do CDNs work? | Cloudflare\nReference Architecture: Cloudflare Content Delivery Network (CDN) · Cloudflare Reference Architecture docs\n","date":"2023-11-15T20:09:22Z","permalink":"https://blog.yorforger.cc/p/get-free-tls-certificate-with-cloudflare/","title":"Get Free TLS certificate with Cloudflare"},{"content":" # 1. Official Docs Make request for the RESTful API of OpenAI: API Reference - OpenAI API Error status code for HTTP response: Error codes - OpenAI API Stream cookbook.openai.com/examples/how_to_stream_completions How to Print ChatGPT API Response as a Stream – PuppyCoding Cookbook: https://cookbook.openai.com/ # 2. Stream message If you write code like this:\n1 2 3 4 respense = openai.create(...) for chunk in response: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\u0026#34;\u0026#34;, flush=True) You probably won\u0026rsquo;t get the result you want, your program will print line by line, not word by word.\nThe actual reason is that Python buffers the chunks before printing them, whereas we want them printed as soon as they’re ready. To do this, we add a flush = True parameter to tell Python to flush the buffer with each chunk.\n1 2 3 4 5 def handle_stream(response): for chunk in response: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\u0026#34;\u0026#34;, flush=True) print() Normally output to a file or the console is buffered, with text output at least until you print a newline. The flush makes sure that any output that is buffered goes to the destination. https://stackoverflow.com/a/15608262/16317008\n# 3. Task types Completions: Given a prompt, the model will return one or more predicted completions. 1 POST https://api.openai.com/v1/completions Chat: Given a list of messages describing a conversation, the model will return a response. 1 POST https://api.openai.com/v1/chat/completions Images: Given a prompt and/or an input image, the model will generate a new image. 1 POST https://api.openai.com/v1/images/generations # 4. Openai package error: urllib3 only supports OpenSSL After import openai package, an error occurred:\n1 ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the \u0026#39;ssl\u0026#39; module is compiled with LibreSSL 2.8.3. See: https://github.com/urllib3/urllib3/issues/2168 urllib3 is a Python package for making HTTP requests, which is an HTTP client (like a browser):\n1 2 3 4 5 6 \u0026gt;\u0026gt;\u0026gt; import urllib3 \u0026gt;\u0026gt;\u0026gt; resp = urllib3.request(\u0026#34;GET\u0026#34;, \u0026#34;https://httpbin.org/robots.txt\u0026#34;) \u0026gt;\u0026gt;\u0026gt; resp.status 200 \u0026gt;\u0026gt;\u0026gt; resp.data b\u0026#34;User-agent: *\\nDisallow: /deny\\n\u0026#34; Solution: https://davidzhu.xyz/post/python/practice/003-openssl-issue/\nReferences:\nA Complete Guide to the ChatGPT API Fixing ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+ | Level Up Coding python - ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the \u0026lsquo;ssl\u0026rsquo; module is compiled with LibreSSL 2.8.3 - Stack Overflow OpenAI ChatGPT (GPT-3.5) API error: \u0026ldquo;\u0026lsquo;messages\u0026rsquo; is a required property\u0026rdquo; when testing the API with Postman - Stack Overflow GPT - OpenAI API GPT-3.5-turbo how to remember previous messages like Chat-GPT website - API - OpenAI Developer Forum ","date":"2023-11-14T00:07:25Z","permalink":"https://blog.yorforger.cc/p/chatgpt-bot/","title":"ChatGPT Bot"},{"content":" # 1. Check Python environment 1 2 3 ❯ ls /usr/bin/python* \u0026amp; ls /usr/local/bin/python* /usr/local/bin/python3 /usr/local/bin/python3.10 /usr/bin/python3 /usr/bin belongs to the system. Mess with it at your peril. /usr/local/bin is yours to fool with; if you mess something up in there, you can trash /usr/local and the system will chug along just fine. If you trash /usr/bin, you\u0026rsquo;ll probably end up reinstalling the OS.\n# 1.1. Multiple versions of Python interpreter As you can see, there are two python3 located at /usr/bin/ and /usr/local/bin/ respectively, and both of these dirs added into the $PATH, so when I input python3 on my terminal, which \u0026quot;python3\u0026quot; will be chosen?\nIt depends on the PATH environment variable, the PATH variable determines the order in which directories are searched to find executable files. If /usr/local/bin appears before /usr/bin in the PATH, then /usr/local/bin/python3 will be chosen when you enter \u0026ldquo;python3\u0026rdquo; in the terminal.\n# 2. Environment variable $PATH The PATH environment variable is an essential component of any Linux system. If you ever use the command line at all, the system is relying on the PATH variable to find the location of the commands you are entering.\n1 2 echo $path /opt/homebrew/bin /opt/homebrew/sbin /usr/local/bin /usr/bin /bin /usr/sbin /sbin Learn more: https://linuxconfig.org/linux-path-environment-variable\n# 3. Virtual environment venv (for Python 3) and virtualenv (for Python 2) allow you to manage separate package installations for different projects. They essentially allow you to create a “virtual” isolated Python installation and install packages into that virtual installation. When you switch projects, you can simply create a new virtual environment and not have to worry about breaking the packages installed in the other environments. It is always recommended to use a virtual environment while developing Python applications.\nIf you are using Python 2, replace venv with virtualenv in the below commands:\n1 2 python3 -m venv env_310 source env/bin/activate This command will create a virtual environment in your current directory. The version of Python in the virtual environment using this command will be the same is the version of the python interpreter you used.\nHere the python3 is 3.10, so the created env will use python 3.10 as interpreter, you can find the interpreter at ./env_310/bin/ folder.\nYou can also create different virtual env in a same project so that you can switch different version of Python interpreter.\n1 2 3 4 5 # python3 is python 3.10 on my machine $ python3 -m venv env_310 # /usr/bin/python3 is python 3.9 $ /usr/bin/python3 -m venv env_309 Then there will be two virtual environments, you can switch to any version of the Python virtual environment by deactivating one and activating another.\n1 2 3 4 5 6 7 8 9 $ source env_310/bin/activate $ which python /Users/David/Codes/PyCharm/tgpt/env_310/bin/python $ env_310 ❯ python --version Python 3.10.0 # switch to python 3.9 $ deactivate $ source env_309/bin/activate ... # 4. Command python -m \u0026lt;module-name\u0026gt; 1 2 3 4 5 6 7 8 9 python3 -m notebook python3 -m pip ... # if no -m, there will be an error python3 notebook // /Library/Developer/CommandLineTools/usr/bin/python3: can\u0026#39;t open file \u0026#39;/Users/shaowen/notebook\u0026#39;: [Errno 2] No such file or directory python pip // python: can\u0026#39;t open file \u0026#39;/Users/PyCharm/pythonProject/pip\u0026#39;: [Errno 2] No such file or directory What does -m in python -m pip install \u0026lt;package\u0026gt; mean? or while upgrading pip using python -m pip install --upgrade pip. What is difference when just running pip install.\n-m: run library module as a script (terminates option list)\n1 2 3 4 python3 main.py python3 -m main python3 main // error python3 -m main.py // warning remove \u0026#39;.py\u0026#39; # 5. if __name__ == \u0026quot;__main__\u0026quot; This is useful because you do not want this code block to run when importing into other files, but you do want it to run when invoked from the command line.\n1 2 3 4 5 6 7 //cat.py: if __name__ == \u0026#34;__main__\u0026#34;: print(\u0026#39;mow~\u0026#39;) //main.py: import cat print(\u0026#39;hello\u0026#39;) Execute on shell:\n1 2 3 4 5 6 python main.py hello # After the first line in cat.py is removed python main.py mow~ hello # 6. pip uninstall xxx use pip-autoremove to remove a package plus unused dependencies.\n1 2 3 4 5 pip list # install pip-autoremove pip install pip-autoremove # remove \u0026#34;somepackage\u0026#34; plus its dependencies: pip-autoremove xxx -y https://stackoverflow.com/a/10284948/16317008\n","date":"2023-11-09T19:12:40Z","permalink":"https://blog.yorforger.cc/p/virtual-environment-python/","title":"Virtual Environment Python"},{"content":" # 1. iTerm2 # 1.1. iTerm2 theme settings iTerm2 Preferences: Appearance \u0026gt; General \u0026gt; Theme: Minimal\niTerm2 Preferences: Advanced\niTerm2 — Snazzy theme\n1 $ curl -Ls https://raw.githubusercontent.com/sindresorhus/iterm2-snazzy/main/Snazzy.itermcolors \u0026gt; /tmp/Snazzy.itermcolors \u0026amp;\u0026amp; open /tmp/Snazzy.itermcolors Configure iTerm2 color theme: iTerm2 Preferences: Profiles \u0026gt; Colors \u0026gt; Color Presets: Snazzy\n# 1.2. Install Oh My Zsh and zplug 1 2 3 4 # Oh My Zsh sh -c \u0026#34;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # zplug brew install zplug .zshrc file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # ~.zshrc export ZSH=~/.oh-my-zsh # disable oh-my-zsh themes for pure prompt ZSH_THEME=\u0026#34;\u0026#34; source $ZSH/oh-my-zsh.sh export ZPLUG_HOME=/opt/homebrew/opt/zplug source $ZPLUG_HOME/init.zsh zplug \u0026#34;mafredri/zsh-async\u0026#34;, from:github zplug \u0026#34;sindresorhus/pure\u0026#34;, use:pure.zsh, from:github, as:theme zplug load # Install plugins if there are plugins that have not been installed if ! zplug check --verbose; then printf \u0026#34;Install? [y/N]: \u0026#34; if read -q; then echo; zplug install fi fi # 1.3. syntax highlighting \u0026amp; auto suggestions 1 2 3 zplug \u0026#34;zdharma/fast-syntax-highlighting\u0026#34;, as:plugin, defer:2 zplug \u0026#34;zsh-users/zsh-autosuggestions\u0026#34;, as:plugin, defer:2 Then reload your iTerm2, you will see the change after download.\nReference: Beautify your iTerm2 and prompt 💋 | by Steven Chim | airfrance-klm | Medium\n# 2. Clean PrettyClean\nmac-cleanup-sh\n# 3. Nvim # install 1 2 3 4 5 brew install neovim echo \u0026#34;alias vim=\u0026#39;nvim\u0026#39;\u0026#34; \u0026gt;\u0026gt; ~/.zshrc git clone -b v2.0 https://github.com/NvChad/NvChad ~/.config/nvim --depth 1 \u0026amp;\u0026amp; nvim # for live grep search brew install ripgrep # font Change font of iTerm2 otherwise you will see some weird characters in nvim.\n1 2 ❯ brew tap homebrew/cask-fonts ❯ brew install --cask font-jetbrains-mono-nerd-font Then set the font in iTerm2 Preferences: Profiles \u0026gt; Text \u0026gt; Font: jetbrains-mono-nerd\nLearn more: How to Install Nerd Fonts on mac\n# theme Chnage the theme of nvim, enter nvim and type space + t + h, choose onenord theme.\n# Highlight 1 2 3 :TSInstall markdown :TSInstall markdown_inline :setlocal spell spelllang=en_us, cjk spellcapcheck=\u0026#34;\u0026#34; # Plugins Edit ~/.config/nvim/lua/custom/chadrc.lua:\n1 2 3 4 5 6 7 local M = {} M.ui = { theme = \u0026#39;onenord\u0026#39; } M.plugins = \u0026#39;custom.plugins\u0026#39; return M Add new file ~/.config/nvim/lua/custom/plugins.lua:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 local plugins = { { \u0026#34;Pocco81/auto-save.nvim\u0026#34;, lazy = false, config = function() require(\u0026#34;auto-save\u0026#34;).setup { -- your config goes here -- or just leave it empty :) } end, } } return plugins # Shortcuts https://docs.rockylinux.org/books/nvchad/nvchad_ui/nvchad_ui/\n# Final custom/chadrc.lua:\n1 2 3 4 5 6 7 8 ---@type ChadrcConfig local M = {} M.ui = { theme = \u0026#39;onenord\u0026#39; } M.plugins = \u0026#39;custom.plugins\u0026#39; return M custom/plugins.lua:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 local plugins = { { \u0026#34;Pocco81/auto-save.nvim\u0026#34;, lazy = false, config = function() require(\u0026#34;auto-save\u0026#34;).setup { -- your config goes here -- or just leave it empty :) } end, }, { \u0026#34;github/copilot.vim\u0026#34;, lazy = false, }, { \u0026#39;MeanderingProgrammer/markdown.nvim\u0026#39;, dependencies = { \u0026#39;nvim-treesitter/nvim-treesitter\u0026#39; }, ft = \u0026#39;markdown\u0026#39;, config = function() require(\u0026#39;render-markdown\u0026#39;).setup({}) end, }, } return plugins ","date":"2023-11-08T22:50:37Z","permalink":"https://blog.yorforger.cc/p/my-mac-configuration/","title":"My Mac Configuration"},{"content":" # 1. Background When I first started writing backend code, if the server return a file (any type), and I would add a Content-Type header and set its value to application/octet-stream:\n1 2 3 4 w.Header().Set(\u0026#34;Content-Disposition\u0026#34;, \u0026#34;attachment; filename=\u0026#34;+strconv.Quote(info.Name())) w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/octet-stream\u0026#34;) http.ServeFile(w, r, filePath) return I don\u0026rsquo;t know what does this mean, but it works. Haha, it\u0026rsquo;s not a good habit, so I decide to take some time to learn it.\n# 2. MIME types A media type (also known as a Multipurpose Internet Mail Extensions or MIME type) indicates the nature and format of a document, file, or assortment of bytes.\n# 2.1. Structure of a MIME type 1 type/subtype The type represents the general category into which the data type falls, such as video or text.\nThe subtype identifies the exact kind of data of the specified type the MIME type represents. For example, for the MIME type text, the subtype might be plain (plain text), html (HTML source code), or calendar (for iCalendar/.ics) files.\nAn optional parameter can be added to provide additional details:\n1 type/subtype;parameter=value For any MIME type whose main type is text, you can add the optional charset parameter to specify the character set used for the characters in the data. If no charset is specified, the default is ASCII (US-ASCII) unless overridden by the user agent\u0026rsquo;s settings. To specify a UTF-8 text file, the MIME type text/plain;charset=UTF-8 is used.\nThere are two classes of type: discrete and multipart: https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_Types#types\n# 3. Common MIME types # 3.1. application/octet-stream This is the default for binary files. As it means unknown binary file, browsers usually don\u0026rsquo;t execute it, or even ask if it should be executed. They treat it as if the Content-Disposition header was set to attachment, and propose a \u0026ldquo;Save As\u0026rdquo; dialog.\n# 3.2. text/plain This is the default for textual files. Even if it really means \u0026ldquo;unknown textual file,\u0026rdquo; browsers assume they can display it.\n# 3.3. text/css CSS files used to style a Web page must be sent with text/css. If a server doesn\u0026rsquo;t recognize the .css suffix for CSS files, it may send them with text/plain or application/octet-stream MIME types. If so, they won\u0026rsquo;t be recognized as CSS by most browsers and will be ignored.\nLearn more: Common MIME types - HTTP | MDN\nReferences: MIME types (IANA media types) - HTTP | MDN\n","date":"2023-11-06T22:06:07Z","permalink":"https://blog.yorforger.cc/p/mime-types/","title":"MIME Types"},{"content":" # 1. Differences between gRPC and REST APIs gRPC utilizes HTTP/2 whereas REST utilizes HTTP 1.1 gRPC utilizes the protocol buffer data format as opposed to the standard JSON data format that is typically used within REST APIs With gRPC you can utilize HTTP/2 capabilities such as server-side streaming, client-side streaming or even bidirectional-streaming should you wish. Source: Go gRPC Beginners Tutorial | TutorialEdge.net\n# 2. Helloworld # 2.1. Intsall protoc and plugins Ptotocol Buffers - David\u0026rsquo;s Blog\n# 2.2. Install gRPC in your project 1 ❯ go get -u google.golang.org/grpc # 2.3. HelloWorld example Go gRPC Beginners Tutorial | TutorialEdge.net\n","date":"2023-11-05T16:01:06Z","permalink":"https://blog.yorforger.cc/p/intro-to-grpc/","title":"Intro to gRPC"},{"content":"Source: Introduction to HTTP/2 | Google Developers\n# 1. Binary Framing # 1.1. Streams, messages, and frames At the core of all performance enhancements of HTTP/2 is the new binary framing layer, which dictates how the HTTP messages are encapsulated and transferred between the client and server.\nThe introduction of the new binary framing mechanism changes how the data is exchanged between the client and server. To describe this process, let’s familiarize ourselves with the HTTP/2 terminology:\nStream: A bidirectional flow of bytes within an established connection, which may carry one or more messages. Message: A complete sequence of frames that map to a logical request or response message. Frame: The smallest unit of communication in HTTP/2, each containing a frame header, which at a minimum identifies the stream to which the frame belongs. The relation of these terms can be summarized as follows:\nAll communication is performed over a single TCP connection that can carry any number of bidirectional streams. Each stream has a unique identifier and optional priority information that is used to carry bidirectional messages. Each message is a logical HTTP message, such as a request, or response, which consists of one or more frames. The frame is the smallest unit of communication that carries a specific type of data—e.g., HTTP headers, message payload, and so on. Frames from different streams may be interleaved and then reassembled via the embedded stream identifier in the header of each frame. In short, HTTP/2 breaks down the HTTP protocol communication into an exchange of binary-encoded frames, which are then mapped to messages that belong to a particular stream, all of which are multiplexed within a single TCP connection. This is the foundation that enables all other features and performance optimizations provided by the HTTP/2 protocol.\n# 1.2. Request and response multiplexing With HTTP/1.x, if the client wants to make multiple parallel requests to improve performance, then multiple TCP connections must be used (see Using Multiple TCP Connections ). This behavior is a direct consequence of the HTTP/1.x delivery model, which ensures that only one response can be delivered at a time (response queuing) per connection. Worse, this also results in head-of-line blocking and inefficient use of the underlying TCP connection. I have talked this in previous post.\nThe new binary framing layer in HTTP/2 removes these limitations, and enables full request and response multiplexing, by allowing the client and server to break down an HTTP message into independent frames, interleave them, and then reassemble them on the other end.\n\u0026hellip;.\nLearn more: Introduction to HTTP/2 | Web Fundamentals | Google Developers\n# 2. Server push Another powerful new feature of HTTP/2 is the ability of the server to send multiple responses for a single client request. That is, in addition to the response to the original request, the server can push additional resources to the client, without the client having to request each one explicitly.\nNote: HTTP/2 breaks away from the strict request-response semantics and enables one-to-many and server-initiated push workflows that open up a world of new interaction possibilities both within and outside the browser. This is an enabling feature that will have important long-term consequences both for how we think about the protocol, and where and how it is used.\n# 3. Header compression \u0026hellip;\n","date":"2023-11-04T00:45:30Z","permalink":"https://blog.yorforger.cc/p/http-2.0-binary-frame-server-push-header-compression/","title":"HTTP 2.0 (Binary Frame, Server Push, Header Compression)"},{"content":" # 1. Form data vs query string # 1.1. Client side Query string resides in the url:\n1 http://www.blabla.com?a=1\u0026amp;b=2 Form data is sent with the request body. Therefore, when there is sensitive data, we usually put data in the form not the query string. Because the requested URL might show up in Web server logs and browser history/bookmarks which is not a good thing. source\n# 1.2. Server side # 1.2.1. Go web For all requests, ParseForm() parses the raw query from the URL and updates r.Form. For POST, PUT, and PATCH requests, it also reads the request body, parses it as a form and puts the results into both r.PostForm and r.Form. When the Content-Type is not application/x-www-form-urlencoded, the request Body is not read.\nLearn more: Some HTTP Issues with Go - David\u0026rsquo;s Blog\n# 1.2.2. Spring web If your server is wiritten in Java Spring, and you need to POST form data to the server, you need to set your Content-Type header of your request to application/json.\nWhen we write test we usually use curl command, looks like this:\n1 $ curl localhost:8080/hello -d \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;davidzhu\u0026#34;, \u0026#34;password\u0026#34;:\u0026#34;778899a\u0026#34;}\u0026#39; However, this command with -d option will set Content-Type header to application/x-www-form-urlencoded by default, which is not accepted on Spring\u0026rsquo;s side.\nTherefore you have to set Content-Type explicitly:\n1 2 3 $ curl localhost:8080/postbody -d \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;davidzhu\u0026#34;, \u0026#34;password\u0026#34;:\u0026#34;778899a\u0026#34;}\u0026#39; -H \u0026#34;Content-Type: application/json\u0026#34; Note the format of the -d is josn format not url query string format: \u0026quot;username=davidzhu\u0026amp;password=778899a\u0026quot;, this format is application/x-www-form-urlencoded, not application/json.\nSource: https://stackoverflow.com/a/7173011/16317008\nLearn more about curl post: https://reqbin.com/req/c-dwjszac0/curl-post-json-example\n# 2. POST data to server The HTTP POST method sends data to the server. The type of the body of the request is indicated by the Content-Type header.\nA POST request is typically sent via an HTML form and results in a change on the server. In this case, the content type is selected by putting the adequate string in the enctype attribute of the form element or the formenctype attribute of the input or button elements:\nReferences: https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/POST\n# 3. POST data to server - example Let\u0026rsquo;s say there is a html file looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;form action=\u0026#34;http://localhost:8080/postform\u0026#34; method=\u0026#34;post\u0026#34; class=\u0026#34;form-example\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;form-example\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;username\u0026#34;\u0026gt;username\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;usernam\u0026#34; id=\u0026#34;username\u0026#34; required /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-example\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;password\u0026#34;\u0026gt;password\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; name=\u0026#34;passwor\u0026#34; id=\u0026#34;password\u0026#34; required /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-example\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;register\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; When I push the regiser buttion to submit the form, the http request looks like this:\nAs you can see the form data resides in the request body, not in the URL.\nSometimes you send data to server with query string or with form. Although, data in this two cases reside in different places, former located in the URL, latter at the payload (request body), these two cases may result same thing (the server will get same data), which may give you a wrong impression that form data wil be put into URL too.\n1 2 $ curl -X POST localhost:8080/postform -d \u0026#34;username=davidzhu\u0026amp;password=778899a\u0026#34; $ curl -X POST \u0026#34;localhost:8080/postform?username=david\u0026amp;password=778899a\u0026#34; The server may get same data for these two command, this is because the server may try to parse the query string and form data at the same time, I have talked this in previous post.\n# 4. Form data restriction As we talk above, form data can only be these three type by by default:application/x-www-form-urlencoded, multipart/form-data, text/plain, and you have to set the Content-Type to the corresponding type with correct format.\nIf you want application/json type, you need encode the form data into josn at client and decode the request body at server side. Besides, don\u0026rsquo;t forget to set the Content-Type header to application/json which will tell the server the data format resides in the request body.\nAn answer from stackoverflow, hope it will help:\nHTML provides no way to generate JSON from form data. If you really want to handle it from the client, then you would have to resort to using JavaScript to:\ngather your data from the form via DOM organise it in an object or array generate JSON with JSON.stringify POST it with XMLHttpRequest You\u0026rsquo;d probably be better off sticking to application/x-www-form-urlencoded data and processing that on the server instead of JSON. Your form doesn\u0026rsquo;t have any complicated hierarchy that would benefit from a JSON data structure.\nThis is same in Go, if you want send json data in the request body, you should encode it into bytes and decode request body at server:\nclient:\n1 2 3 4 5 6 7 // string in back quote is raw string which means // the double quote here will lose its special meaning // decode the json string into reader_json := strings.NewReader(`{\u0026#34;username\u0026#34;: \u0026#34;david\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;my_password\u0026#34;}`) r, _ := http.NewRequest(http.MethodPost, \u0026#34;/postbody\u0026#34;, reader_json) r.Header.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) ... server:\n1 2 3 4 5 6 7 8 9 10 11 12 13 // Parse username and password in form. type info struct { Username string `json:\u0026#34;username\u0026#34;` Password string `json:\u0026#34;password\u0026#34;` } user := info{} // note that r.Body is an io.Reader // which means decoder.Decode() method will consume data stroed in r.Body // you cannot read same data twice decoder := json.NewDecoder(r.Body) err := decoder.Decode(\u0026amp;user) print(user.password) print(user.username) ","date":"2023-11-04T00:02:19Z","permalink":"https://blog.yorforger.cc/p/form-data-query-string/","title":"Form Data \u0026 Query String"},{"content":" # 1. HTTP/1.1 The first standardized version of HTTP, HTTP/1.1, was published in early 1997, only a few months after HTTP/1.0.\nHTTP/1.1 clarified ambiguities and introduced numerous improvements:\nA underlying TCP connection could be reused. It no longer needed to be opened multiple times to request the resources embedded in the single original document.\nHTTP keep-alive, a.k.a., HTTP persistent connection, is an instruction that allows a single TCP connection to remain open for multiple HTTP requests/responses. Learn more: HTTP Headers (2) - David\u0026rsquo;s Blog Pipelining was added. This allowed a second request to be sent before the answer to the first one was fully transmitted. This lowered the latency of the communication.\nPipelining will cause another issue: Head-of-line (HOL) blocking, we will talk this later. \u0026hellip;\nThe persistent-connection model keeps connections opened between successive requests, reducing the time needed to open new connections. The HTTP pipelining model goes one step further, by sending several successive requests without even waiting for an answer, reducing much of the latency in the network. Connection management in HTTP/1.x | MDN\n# 2. HTTP/2.0 The primary goals for HTTP/2 are to reduce latency by enabling full request and response multiplexing, minimize protocol overhead via efficient compression of HTTP header fields, and add support for request prioritization and server push.\nFrom a technical point of view, one of the most significant features that distinguishes HTTP/1.1 and HTTP/2 is the binary framing layer, which can be thought of as a part of the application layer in the internet protocol stack. As opposed to HTTP/1.1, which keeps all requests and responses in plain text format, HTTP/2 uses the binary framing layer to encapsulate all messages in binary format, while still maintaining HTTP semantics, such as verbs, methods, and headers. An application level API would still create messages in the conventional HTTP formats, but the underlying layer would then convert these messages into binary. This ensures that web applications created before HTTP/2 can continue functioning as normal when interacting with the new protocol.\nThe conversion of messages into binary allows HTTP/2 to try new approaches to data delivery not available in HTTP/1.1, a contrast that is at the root of the practical differences between the two protocols.\n# 3. HTTP/1.1 — Pipelining and Head-of-Line Blocking The first response that a client receives on an HTTP GET request is often not the fully rendered page. Instead, it contains links to additional resources needed by the requested page. Because of this, the client will have to make additional requests to retrieve these resources. In HTTP/1.0, the client had to break and remake the TCP connection with every new request, a costly affair in terms of both time and resources.\nHTTP/1.1 takes care of this problem by introducing persistent connections and pipelining. With persistent connections, HTTP/1.1 assumes that a TCP connection should be kept open unless directly told to close. This allows the client to send multiple requests along the same connection without waiting for a response to each, greatly improving the performance of HTTP/1.1 over HTTP/1.0.\nUnfortunately, there is a natural bottleneck to this optimization strategy. We said client to send multiple requests along the same connection (means a same thread on server), assume there are 3 request needed to be sent to server, one is for html, one is for an image and another one for a video. Now with persistent connections and pipelining, the client can send these three requests at a time within one TCP connection, and these requests will be procceeded by the server one by one (they are in a same tcp connection), if the second request is very time consuming at server, then the third request has to wait for the second finished. Adding separate, parallel TCP connections at client could alleviate this issue, but there are limits to the number of concurrent TCP connections possible between a client and server, and each new connection requires significant resources.\nHead of Line blocking in HTTP terms is often referring to the fact that each browser/client has a limited number of connections to a server and doing a new request over one of those connections has to wait for the ones to complete before it can fire it off. The head of line requests block the subsequent ones. HTTP/2 solves this by introducing multiplexing so that you can issue new requests over the same connection without having to wait for the previous ones to complete.\nHTTP/2 does however still suffer from another kind of HOL, namely on the TCP level. One lost packet in the TCP stream makes all streams wait until that packet is re-transmitted and received. This HOL is being addressed with the QUIC protocol\u0026hellip;\nLearn more: https://stackoverflow.com/a/45583977/16317008\nThe multiplex here is not the multiplex on linux (epoll, select), learn more: HTTP 2.0 Binary Framing - David\u0026rsquo;s Blog\n# 4. What are the other differences between HTTP/2 and HTTP/1.1 that impact performance? Multiplexing: HTTP/1.1 loads resources one after the other, so if one resource cannot be loaded, it blocks all the other resources behind it. In contrast, HTTP/2 is able to use a single TCP connection to send multiple streams of data at once so that no one resource blocks any other resource. HTTP/2 does this by splitting data into binary-code messages and numbering these messages so that the client knows which stream each binary message belongs to.\nServer push: Typically, a server only serves content to a client device if the client asks for it. However, this approach is not always practical for modern webpages, which often involve several dozen separate resources that the client must request. HTTP/2 solves this problem by allowing a server to \u0026ldquo;push\u0026rdquo; content to a client before the client asks for it. The server also sends a message letting the client know what pushed content to expect – like if Bob had sent Alice a Table of Contents of his novel before sending the whole thing.\nHeader compression: Small files load more quickly than large ones. To speed up web performance, both HTTP/1.1 and HTTP/2 compress HTTP messages to make them smaller. However, HTTP/2 uses a more advanced compression method called HPACK that eliminates redundant information in HTTP header packets. This eliminates a few bytes from every HTTP packet. Given the volume of HTTP packets involved in loading even a single webpage, those bytes add up quickly, resulting in faster loading.\nReferences:\nHTTP/1.1 vs HTTP/2: What\u0026rsquo;s the Difference? | DigitalOcean Introduction to HTTP/2 | Web Fundamentals | Google Developers HTTP/2 vs. HTTP/1.1 | Cloudflare Connection management in HTTP/1.x - HTTP | MDN https://stackoverflow.com/a/45583977/16317008 Evolution of HTTP - HTTP | MDN ","date":"2023-11-03T22:45:30Z","permalink":"https://blog.yorforger.cc/p/http-1.1-vs-http-2.0/","title":"HTTP 1.1 vs HTTP 2.0"},{"content":" # 1. Compilation process in C C/C++ programs are built in two main phases (ignore the preprocess, assemble):\nCompilation - produces object code (.obj, .o)\n1 2 # Compile source code into an object file without linking: gcc -c path/to/source.c The extension of the object file in DOS is .obj, and in UNIX, the extension is .o. Linking - produces executable code (.exe or .dll)\nexecutable file with an extension of exe in DOS and .out in UNIX OSL # 2. Static linking and dynamic linking # 2.1. Static linking - portable, fast Mainly, all the programs written in C use library functions. These library functions are pre-compiled, and the object code of these library files is stored with .lib (or .a) extension. The main working of the linker is to combine the object code of library files with the object code of our program. The output of the linker is the executable file. So the static Linking creates larger binary files. Note that this is the process of static linking, and .lib and .a is static library in windows and linux respectively.\nIn static linking, everything is bundled into your application, you don’t have to worry that the client will have the right library (and version) available on their system. Since all library code have connected at compile time, the final executable has no dependencies on the library at run time. You have everything under your control and there is no dependency.\nOne major advantage of static libraries being preferred even now “is speed”. There will be no dynamic querying of symbols in static libraries.\nOne drawback of static libraries is, for any change(up-gradation) in the static libraries, you have to recompile the main program every time.\n# 2.2. Dynamic linking - smaller binary Dynamic Linking doesn’t require the code to be copied, it is done by just placing name of the library in the binary file. The actual linking happens when the program is run, when both the binary file and the library are in memory.\nSource: Static and Dynamic Libraries | Set 1 - GeeksforGeeks\n# 3. Static library vs dynamic library Static library: windows .lib, linux .a\nDynamic library (shared library): windows .dll, linux .so\nStatic library .lib is just a bundle of .obj files and therefore isn\u0026rsquo;t a complete program. It hasn\u0026rsquo;t undergone the second (linking) phase of building a program. Dlls, on the other hand, are like exe\u0026rsquo;s and therefore are complete programs.\nIf you build a static library, it isn\u0026rsquo;t linked yet and therefore consumers of your static library will have to use the same compiler that you used (if you used g++, they will have to use g++). If the static library uses C++ library, such as #include \u0026lt;iostream\u0026gt;.\nIf instead you built a dll (and built it correctly), you have built a complete program that all consumers can use, no matter which compiler they are using. There are several restrictions though, on exporting from a dll, if cross compiler compatibility is desired.\nSource: https://stackoverflow.com/a/25209275/16317008\n# 4. Example in practice Dynamic linking means the use of shared libraries. Shared libraries usually end with .so (short for \u0026ldquo;shared object\u0026rdquo;) or .dylib on MacOS.\nAnother thing to note is that when a bug is fixed in a shared library, every application that references this library will profit from it. This also means that if the bug remains undetected, each referencing application will suffer from it (if the application uses the affected parts).\nIt can be very hard for beginners when an application requires a specific version of the library, but the linker only knows the location of an incompatible version. In this case, you must help the linker find the path to the correct version.\nAlthough this is not an everyday issue, understanding dynamic linking will surely help you in fixing such problems.\nFortunately, the mechanics for this are quite straightforward. To detect which libraries are required for an application to start, you can use ldd, which will print out the shared libraries used by a given file.\n1 2 3 4 5 $ ldd my_app linux-vdso.so.1 (0x00007ffd1299c000) libmy_shared.so =\u0026gt; not found libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00007f56b869b000) /lib64/ld-linux-x86-64.so.2 (0x00007f56b8881000) Note that the library libmy_shared.so is part of the repository but is not found. This is because the dynamic linker, which is responsible for loading all dependencies into memory before executing the application, cannot find this library in the standard locations it searches.\nErrors associated with linkers finding incompatible versions of common libraries (like bzip2, for example) can be quite confusing for a new user. One way around this is to add the repository folder to the environment variable LD_LIBRARY_PATH to tell the linker where to look for the correct version. In this case, the right version is in this folder, so you can export it:\n1 2 $ LD_LIBRARY_PATH=$(pwd):$LD_LIBRARY_PATH $ export LD_LIBRARY_PATH Now the dynamic linker knows where to find the library, and the application can be executed. You can rerun ldd to invoke the dynamic linker, which inspects the application\u0026rsquo;s dependencies and loads them into memory. The memory address is shown after the object path:\n1 2 3 4 5 $ ldd my_app linux-vdso.so.1 (0x00007ffd385f7000) libmy_shared.so =\u0026gt; /home/stephan/library_sample/libmy_shared.so (0x00007f3fad401000) libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00007f3fad21d000) /lib64/ld-linux-x86-64.so.2 (0x00007f3fad408000) To find out which linker is invoked, you can use file:\n1 2 $ file my_app my_app: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=26c677b771122b4c99f0fd9ee001e6c743550fa6, for GNU/Linux 3.2.0, not stripped The linker /lib64/ld-linux-x86–64.so.2 is a symbolic link to ld-2.30.so, which is the default linker for my Linux distribution:\n1 2 $ file /lib64/ld-linux-x86-64.so.2 /lib64/ld-linux-x86-64.so.2: symbolic link to ld-2.31.so Looking back to the output of ldd, you can also see (next to libmy_shared.so) that each dependency ends with a number (e.g., /lib64/libc.so.6). The usual naming scheme of shared objects is:\n1 **lib** XYZ.so **.\u0026lt;MAJOR\u0026gt;** . **\u0026lt;MINOR\u0026gt;** On my system, libc.so.6 is also a symbolic link to the shared object libc-2.30.so in the same folder:\n1 2 $ file /lib64/libc.so.6 /lib64/libc.so.6: symbolic link to libc-2.31.so If you are facing the issue that an application will not start because the loaded library has the wrong version, it is very likely that you can fix this issue by inspecting and rearranging the symbolic links or specifying the correct search path (see \u0026ldquo;The dynamic loader: ld.so\u0026rdquo; below).\nFor more information, look on the ldd man page.\nSource: How to handle dynamic and static libraries in Linux | Opensource.com\nNote that the dynamic linker on MacOS is called dyld, try man dyld to check the details. Learn more: https://stackoverflow.com/a/34905091/16317008\n1 2 3 4 func main() { fmt.Println(\u0026#34;hello\u0026#34;) } // go build -o server main.go Then check the shared libraries this executable file server required:\n1 2 3 4 5 # otool -L: print shared libraries used $ otool -L server server: /usr/lib/libSystem.B.dylib (compatibility version 0.0.0, current version 0.0.0) /usr/lib/libresolv.9.dylib (compatibility version 0.0.0, current version 0.0.0) ldd is on linux, on MaxOS you should use otool\n","date":"2023-11-03T10:49:20Z","permalink":"https://blog.yorforger.cc/p/statically-linking-in-c/","title":"Statically Linking in C"},{"content":"声明: 本文只用于学习目的, 请勿用于非法用途.\n# 1. DNS spoofing DNS cache poisoning is the act of entering false information into a DNS cache, so that DNS queries return an incorrect response and users are directed to the wrong websites. DNS cache poisoning is also known as \u0026lsquo;DNS spoofing\u0026rsquo;.\n# 1.1. DNS Caching Learn more about DNS caching: DNS Concepts (NameServer(NS), DNS Records and Caching) - David\u0026rsquo;s Blog\n# 1.2. How do attackers poison DNS caches? Attackers can poison DNS caches by impersonating DNS nameservers, making a request to a DNS resolver, and then forging the reply when the DNS resolver queries a nameserver. This is possible because DNS servers use UDP instead of TCP, and because currently there is no verification for DNS information.\nDNS Cache Poisoning Process:\nPoisoned DNS Cache:\nIf a DNS resolver receives a forged response, it accepts and caches the data uncritically because there is no way to verify if the information is accurate and comes from a legitimate source.\nDespite these major points of vulnerability in the DNS caching process, DNS poisoning attacks are not easy. Because the DNS resolver does actually query the authoritative nameserver, attackers have only a few milliseconds to send the fake reply before the real reply from the authoritative nameserver arrives.\nThe way to prevent this is to set the /etc/host file directly, because checking host file happens before DNS resolution.\nLearn more: DNS Stub and Recursive Resolver - Config Files - David\u0026rsquo;s Blog\nLearn more: What is DNS cache poisoning? | DNS spoofing | Cloudflare\n# 2. GFW # 2.1. HTTP 劫持 原文: 浅谈HTTP劫持、DNS污染的影响及解决办法（仅个人理解） | 逗比根据地\nHTTP劫持很容易理解，因为HTTP传输协议是明文的，并且我的网站服务器是在海外，要访问我的网站就要通过中国的国际宽带出口，出去与我的网站建立连接。我的网站也是因为关键词的原因在经过出口的时候，被“检查站：墙”扫描到了违规关键词，于是掐断了TCP链接。所以当时用户访问网站会遇到：链接已重置、该网站已永久移动到其他地址等等。\nHTTP劫持很容易解决，那就是加上SSL证书，网站链接全部内容加密，这样“检查站：墙”就无法解密数据分析关键词了。但是这不是绝对能解决这个问题的，如果你的网站只是误杀或者违规擦边球，那还好，如果是大型网站，就会特殊对待了。\nHTTPS在建立加密连接的时候，需要一次握手，也就是达成链接协议建立加密连接，但是这次握手是明文的（建立加密链接首先就是链接双方信任，比如网站的SSL证书是自己签的，或者SSL证书到期或伪造的，在访问这个网站的时候浏览器就会进行提示，表示此网站不安全啥的。）握手是明文的就意味着，如果你的域名被重点关注，即使你加上了SSL证书，也会在首次握手的时候，被关键词匹配然后掐断链接。 learn more: HTTPS SSL TLS - David\u0026rsquo;s Blog\n# 2.2. DNS污染投毒 我们假设A为用户端也就是你的电脑设备，B为DNS服务器，C为A到B链路中一个节点的网络设备（路由器、交换机、网关等）。\n然后我们模拟一次被污染的DNS请求过程。\nA访问一个网站，比如 google.com ，然后，A向B通过UDP方式发送查询请求，比如查询内容 A google.com ，这个数据库在前往B的时候要经过数个节点网络设备比如C，然后继续前往DNS服务器B。\n然而在这个传输过程中，C针对这个数据包进行特征分析，（DNS端口为53，进行特定端口监视扫描，对UDP明文传输的DNS查询请求进行特征和关键词匹配分析，比如“google.com”是关键词，也或者是“A记录”），从而立刻返回一个错误的解析结果（比如返回了 A 233.233.233.233 ）。\n众所周知，作为链路上的一个节点，网络设备 C 必定比真正的 DNS 服务器 B 更快的返回结果到 用户电脑A，而目前的DNS解析机制策略有一个重要的原则，就是只认第一。因此 节点网络设备C所返回的查询结果就被 用户电脑A当作了最终结果，于是用户电脑A因为获得了错误的IP，导致无法正常访问 google.com 。\n验证污染 我的 doub.ssrshare.usm 主域名虽然在大部分地区解除了DNS污染，但是我的两个SS站域名并没有，所以我尝试对我的SS 子域名进行nslookup测试。\n1 2 3 4 5 6 7 8 C:\\Users\\Administrator\u0026gt;nslookup ss.dou-bi.com 8.8.8.8 服务器: google-public-dns-a.google.com Address: 8.8.8.8 非权威应答: 名称: ss.dou-bi.com Addresses: 200:2:9f6a:794b:: 8.7.198.45 我使用的是谷歌的 8.8.8.8 DNS，但是我得到的A记录 IP却是8.7.198.45，这个明显不是我的IP，看一下其他被DNS污染的域名就会发现都会有这个IP。很明显，我的 ss.dou-bi.com 域名受到了DNS污染。\n解决办法\n使用加密代理，比如Shadowsocks，在加密代理里进行远程DNS解析，或者使用VPN上网。\n修改hosts文件，操作系统中Hosts文件的权限优先级高于DNS服务器，操作系统在访问某个域名时，会先检测HOSTS文件，然后再查询DNS服务器。可以在hosts添加受到污染的DNS地址来解决DNS污染和DNS劫持。\n通过一些软件编程处理，可以直接忽略返回结果是虚假IP地址的数据包，直接解决DNS污染的问题。如果你是Firefox用户，并且只用Firefox，又懒得折腾，直接打开Firefox的远程DNS解析就行了。在地址栏中输入：about:config找到 network.proxy.socks_remote_dns 一项改成true。\n使用DNSCrypt软件，此软件与使用的OpenDNS直接建立相对安全的TCP连接并加密请求数据，从而不会被污染。 对于被DNS污染的网站站长来说，最有效的方法就是 换域名或者IP 了。\n# 2.3. ip黑名单 即使没有DNS污染 或者 你获得了正确的IP，你就能正常访问这些被屏蔽的网站了吗？\n不，墙目前已经有了IP黑名单，针对谷歌这种网站已经不再是普通的DNS污染了，因为总会有办法访问被DNS污染的网站（比如指定Hosts）。\n那么就直接把所有的谷歌IP拉黑不就好了？就算你获得了正确的谷歌IP，但是当你去访问这个IP的时候，墙会发现这个IP存在于黑名单中，于是直接阻断，于是浏览器就会提示：www.google.com 的响应时间过长等等\n# 3. 常见代理方式 VPN 常见协议: IPSec, OpenVPN, L2TP, WireGuard Shadowsocks (代理服务器) 常见协议: SOCKS5 V2Ray (代理服务器) 常见协议: VMess HTTP 代理 (代理服务器) 已被封锁, 原因是HTTP代理并不对数据进行加密, 即使流量能够绕过初步的审查，传输的内容仍然是透明的，容易被监控。 即使使用 HTTPS 也不会实现全局加密, 在建立 TLS 握手验证时还是需要一个HTTP明文连接 了解更多: 上网限制和翻墙基本原理 | superxlcr\u0026rsquo;s notebook\n","date":"2023-10-31T15:29:25Z","permalink":"https://blog.yorforger.cc/p/dns-spoofing-gfw/","title":"DNS Spoofing - GFW"},{"content":" # 1. nslookup # 1.1. Query A Record 1 2 3 4 5 6 7 8 # basic DNS lookup $ nslookup google.com Server:\t192.168.2.1 Address:\t192.168.2.1#53 Non-authoritative answer: Name:\tgoogle.com Address: 142.250.81.238 # 1.2. Reverse DNS look-up 1 2 3 4 5 6 7 8 9 $ nslookup 18.219.46.189 Server:\t192.168.2.1 Address:\t192.168.2.1#53 Non-authoritative answer: 189.46.219.18.in-addr.arpa\tname = ec2-18-219-46-189.us-east-2.compute.amazonaws.com. Authoritative answers can be found from: 46.219.18.in-addr.arpa\tnameserver = ns1-24-us-east-2.ec2-rdns.amazonaws.com. # 1.3. Query name server 1 2 3 4 5 6 7 8 $ nslookup -type=ns google.com Server:\t192.168.2.1 Address:\t192.168.2.1#53 Non-authoritative answer: google.com\tnameserver = ns1.google.com. google.com\tnameserver = ns3.google.com. .... When using the nslookup utility to query Domain Name System (DNS) servers, you could see the message “Non-authoritative answer.” This tells you that the DNS server you’re asking can’t ensure that it has the official, up-to-date information for the domain name or IP address you’re seeking up and is instead giving you a cached response that it got from another DNS server.\nPlease note that getting non-authoritative answers doesn\u0026rsquo;t mean incorrect or unreliable. However, if you need the most accurate and up-to-date information, it is recommended to use authoritative DNS servers for queries.\nReference:\nNslookup Command in Linux with Examples - GeeksforGeeks\nWhy is “Non-authoritative answer” given by nslookup? DNS Explained\n# 1.4. Get authoritative answer Step 1: Use the nslookup command to query the SOA (Start of Authority) record of the domain name. The SOA record contains information about the authoritative name servers for the domain.\n1 2 3 4 5 6 7 8 ❯ nslookup -type=soa davidzhu.xyz Non-authoritative answer: ... Authoritative answers can be found from: davidzhu.xyz\tnameserver = ns1.dnsowl.com. davidzhu.xyz\tnameserver = ns2.dnsowl.com. davidzhu.xyz\tnameserver = ns3.dnsowl.com. Step 2: Identify the primary name server from previous response:\n1 2 3 4 5 6 ❯ nslookup davidzhu.xyz ns1.dnsowl.com Server:\tns1.dnsowl.com Address:\t162.159.27.173#53 Name:\tdavidzhu.xyz Address: 185.199.108.153 # 2. dig dig command stands for Domain Information Groper. It is used for retrieving information about DNS name servers. Dig command replaces older tools such as nslookup and the host.\n# 2.1. Query A Record To query domain “A” record with +short:\n1 2 $ dig geeksforgeeks.org +short 34.218.62.116 Specify DNS server:\n1 $ dig geeksforgeeks.org +short @8.8.8.8 By default, dig command will query the name servers listed in “/etc/resolv.conf” to perform a DNS lookup. We can change it by using @ symbol followed by a hostname or IP address of the name server.\nLearn more about /etc/resolv.conf and /etc/hosts: DNS Stub and Recursive Resolver - Config Files - David\u0026rsquo;s Blog\n# 2.2. Reverse DNS lookup 1 2 ❯ dig -x 18.219.46.189 +short ec2-18-219-46-189.us-east-2.compute.amazonaws.com. # 2.3. Query name server 1 2 3 4 ❯ dig davidzhu.xyz NS +short ns1.dnsowl.com. ns2.dnsowl.com. ns3.dnsowl.com. # 3. dig vs nslookup dig Process: dig follows the standard DNS resolution process, starting with a query to the root name servers to obtain the list of TLD name servers. It then queries a TLD name server to obtain the authoritative name servers for the domain. After obtaining the authoritative name servers, dig sends a direct query to one of these name servers to retrieve the A record for the domain. The response obtained from dig is typically authoritative, as it comes directly from the authoritative name server responsible for the domain. nslookup Process: nslookup queries the DNS server configured on the local system by default. This DNS server may be provided by the ISP or manually configured. The response from nslookup may be non-authoritative, indicating that the DNS server providing the response is not the authoritative server for the queried domain. It may have obtained the response from its cache or forwarded the query to another DNS server. In summary, dig directly queries authoritative name servers to obtain DNS information, resulting in authoritative responses. On the other hand, nslookup queries the local DNS server, which may or may not provide authoritative responses, depending on its configuration and the nature of the query.\n","date":"2023-10-31T15:23:08Z","permalink":"https://blog.yorforger.cc/p/tools-commonly-used-in-networking/","title":"Tools Commonly Used in Networking"},{"content":" # 1. DNS DNS stands for Domain Name System, and it is a distributed hierarchical system that translates human-readable domain names into IP addresses. The process of DNS resolution involves converting a hostname (such as www.example.com) into a computer-friendly IP address (such as 192.168.1.1).\nThe main components of the DNS are DNS servers. All DNS servers fall into one of four categories: Recursive resolvers, root nameservers, TLD nameservers, and authoritative nameservers.\n# 1.1. DNS stub resolver Before introduce the DNS servers, there is a software runs on the client whcih called DNS stub resolver should be noted.\nDNS stub resolver runs on the client machine, which is used to initiate DNS queries, and it sends the DNS query to the recursive resolver, which then performs the actual DNS resolution process (usually provided by your internet service provider (ISP) or a public DNS resolver like Google\u0026rsquo;s 8.8.8.8 or Cloudflare\u0026rsquo;s 1.1.1.1).\nLearn more: DNS Stub and Recursive Resolver - Config Files - David\u0026rsquo;s Blog\n# 1.2. Recursive resolver As we mentioned above, recursive resolver performs the actual DNS resolution process, and it\u0026rsquo;s a DNS server actually, usually provided by your internet service provider (ISP) or a public DNS resolver like Google\u0026rsquo;s 8.8.8.8 or Cloudflare\u0026rsquo;s 1.1.1.1.\nA recursive resolver (also known as a DNS recursor) is the first stop in a DNS query. The recursive resolver acts as a middleman between a client and a DNS nameserver. When a recursive resolver receives a query from a stub resolver, a recursive resolver will either respond with cached data, or send a request to a root nameserver, followed by another request to a TLD nameserver, and then one last request to an authoritative nameserver. After receiving a response from the authoritative nameserver containing the requested IP address, the recursive resolver then sends a response to the client.\nDuring this process, the recursive resolver will cache information received from authoritative nameservers. When a client requests the IP address of a domain name that was recently requested by another client, the resolver can circumvent the process of communicating with the nameservers, and just deliver the client the requested record from its cache.\n# 1.3. Root nameserver \u0026amp; TLD nameserver The 13 DNS root nameservers are known to every recursive resolver, and they are the first stop in a recursive resolver’s quest for DNS records. A root server accepts a recursive resolver’s query which includes a domain name, and the root nameserver responds by directing the recursive resolver to a TLD nameserver, based on the extension of that domain (.com, .net, .org, etc.).\nA TLD nameserver maintains information for all the domain names that share a common domain extension, such as .com, .net, or whatever comes after the last dot in a URL. For example, a .com TLD nameserver contains information for every website that ends in ‘.com’. If a user was searching for google.com, after receiving a response from a root nameserver, the recursive resolver would then send a query to a .com TLD nameserver, which would respond by pointing to the authoritative nameserver (see below) for that domain.\n# 1.4. Authoritative nameserver When a recursive resolver receives a response from a TLD nameserver, that response will direct the resolver to an authoritative nameserver.\nThe authoritative nameserver contains information specific to the domain name it serves (e.g. google.com) and it can provide a recursive resolver with the IP address of that server found in the DNS A record, or if the domain has a CNAME record (alias) it will provide the recursive resolver with an alias domain, at which point the recursive resolver will have to perform a whole new DNS lookup to procure a record from an authoritative nameserver (often an A record containing an IP address).\nDNS A record is actually a map between the domain name and its ipv4 address.\nDNS CNAME record is an alias for that domain.\nAfter buy a domain on a website, the most common operations we use are that change its nameservers and update its DNS records:\nHere, Change the NameServers option if to change the authoritative nameserver of your domain, and update the DNS records is to set ip address (A record) or alias (CNAME record) for your domain.\n# 2. DNS queries Recursive query - In a recursive query, a DNS client requires that a DNS server (typically a DNS recursive resolver) will respond to the client with either the requested resource record or an error message if the resolver can\u0026rsquo;t find the record. Iterative query - in this situation the DNS client will allow a DNS server to return the best answer it can. Non-recursive query: learn more: What is DNS? | How DNS works | Cloudflare # 3. DNS caching # 3.1. Browser DNS caching Modern web browsers are designed by default to cache DNS records for a set amount of time. The purpose here is obvious; the closer the DNS caching occurs to the web browser, the fewer processing steps must be taken in order to check the cache and make the correct requests to an IP address. When a request is made for a DNS record, the browser cache is the first location checked for the requested record.\nIn Chrome, you can see the status of your DNS cache by going to chrome://net-internals/#dns.\n# 3.2. Operating system (OS) level DNS caching The operating system level DNS resolver is the second and last local stop before a DNS query leaves your machine. The process inside your operating system that is designed to handle this query is commonly called a “stub resolver” or DNS client. When a stub resolver gets a request from an application, it first checks its own cache to see if it has the record. If it does not, it then sends a DNS query (with a recursive flag set), outside the local network to a DNS recursive resolver inside the Internet service provider (ISP).\nWhen the recursive resolver inside the ISP receives a DNS query, like all previous steps, it will also check to see if the requested host-to-IP-address translation is already stored inside its local persistence layer.\nSource: What is DNS? | How DNS works | Cloudflare\nNote that checking /etc/hosts happens before DNS.\n# 3.3. Recursive resolver caching We have mentioned above, during DNS resolution process, the recursive resolver will cache information received from authoritative nameservers.\nA DNS resolver will save responses to IP address queries for a certain amount of time. In this way, the resolver can respond to future queries much more quickly, without needing to communicate with the many servers involved in the typical DNS resolution process.\n","date":"2023-10-31T09:50:20Z","permalink":"https://blog.yorforger.cc/p/dns-concepts-nameserverns-dns-records-and-caching/","title":"DNS Concepts (NameServer(NS), DNS Records and Caching)"},{"content":" # 1. Concepts Every device on a TCP/IP-based network must have a unique unicast IP address to access the network and its resources. Without DHCP, IP addresses for new computers or computers that are moved from one subnet to another must be configured manually; IP addresses for computers that are removed from the network must be manually reclaimed.\nWith DHCP, this entire process is automated and managed centrally. The DHCP server maintains a pool of IP addresses and leases an address to any DHCP-enabled client when it starts up on the network. Because the IP addresses are dynamic (leased) rather than static (permanently assigned), addresses no longer in use are automatically returned to the pool for reallocation.\nThe Dynamic Host Configuration Protocol (DHCP) is a network management protocol used on Internet Protocol (IP) networks for automatically assigning IP addresses and other communication parameters to devices connected to the network using a client–server architecture. Dynamic Host Configuration Protocol\n# 2. Process of DHCP DHCP Discover: When a computer (client) joins a network, it sends out a DHCP Discover message as a broadcast to discover DHCP servers available on the network. DHCP Offer: DHCP servers that receive the Discover message respond with a DHCP Offer message. This message contains configuration parameters, including IP address, subnet mask, default gateway, and DNS server addresses. DHCP Request: The computer selects one of the DHCP Offers and sends a DHCP Request message to the chosen DHCP server, indicating its acceptance of the offered configuration. DHCP Acknowledge: Upon receiving the DHCP Request, the DHCP server sends a DHCP Acknowledge message to the computer, confirming the lease of the offered configuration parameters. The computer now configures its network interface with the provided settings. # 3. DHCP discover message \u0026amp; DHCP offer message Learn more about these two messages\u0026rsquo; format: Dynamic Host Configuration Protocol\nReferences: Dynamic Host Configuration Protocol (DHCP) | Microsoft Learn\n","date":"2023-10-30T23:43:06Z","permalink":"https://blog.yorforger.cc/p/dhcp-basics/","title":"DHCP Basics"},{"content":" # 1. Man-in-the-middle attack Learn more:\nhttps://davidzhu.xyz/post/cs-basics/002-ssh/\nHTTPS SSL TLS - David\u0026rsquo;s Blog\n# 2. DDoS attack Learn more: DDoS Attack - David\u0026rsquo;s Blog\n# 3. CSRF attack Learn more: CSRF Attack and CORS - David\u0026rsquo;s Blog\n# 4. SSL stripping SSL stripping attacks, also known as SSL strip, SSL downgrade, or HTTP downgrade attacks, strip the encryption offered by HTTPS, reducing the connection to the less-secure HTTP.\nIn order to “strip” the SSL, an attacker intervenes in the redirection of the HTTP to the secure HTTPS protocol and intercepts a request from the user to the server. The attacker will then continue to establish an HTTPS connection between himself and the server, and an unsecured HTTP connection with the user, acting as a “bridge” between them.\nHow can the SSL strip trick both the browser and the website’s server? The SSL strip takes advantage of the way most users come to SSL websites. The majority of visitors connect to a website’s page that redirects through a 302 redirect, or they arrive on an SSL page via a link from a non-SSL site. If the victim wants, for instance, to buy a product and types the URL www.buyme.com in the address bar, the browser connects to the attacker\u0026rsquo;s machine and waits for a response from the server. In an SSL strip, the attacker, in turn, forwards the victim’s request to the online shop’s server and receives the secure HTTPS payment page. For example https://www.buyme.com. At this point, the attacker has complete control over the secure payment page. He downgrades it from HTTPS to HTTP and sends it back to the victim’s browser. The browser is now redirected to http://www.buyme.com. From now onward, all the victim’s data will be transferred in plain text format, and the attacker will be able to intercept it. Meanwhile, the website’s server will think that it has successfully established the secure connection, which indeed it has—but with the attacker’s machine, not the victim’s.\n# 4.1. Enable SSL sitewide at all websites To mitigate this threat, financial institutions and technology firms have already enabled HTTPS on a site-wide basis. Enabling HTTPS encrypts the connection between a browser and the website, thereby securing sensitive data transmissions. Therefore it makes perfect sense for banks and high-profile technology firms to enable HTTPS on their dynamic websites because of the transaction of important and sensitive information.\n# 4.2. Why enable HSTS? In addition to enabling HTTPS on a site-wide basis, corporations should weigh the benefits of enabling HSTS (HTTP Strict Transport Security), which is a web security policy mechanism that helps to protect websites against SSL stripping attacks and cookie hijacking. It allows web servers to declare that web browsers should interact with them using only secure HTTPS connections, and never via the insecure HTTP protocol.\nWhen a web application issues HSTS Policy to user browsers, conformant user browsers will automatically redirect any insecure HTTP requests to HTTPS for the target website. In addition, when a man-in-the-middle attacker attempts to intercept traffic from a victim using an invalid certificate, HSTS does not allow the user to override the invalid certificate warning message. By having a HSTS policy installed, it will be nearly impossible for the attackers to intercept any information at all!\nSource: What Are SSL Stripping Attacks? | Venafi\n# 5. DNS # 5.1. DNS hijacking To prevent DNS hijacking, first, you have to know the different kinds of attacks. DNS hijacking can take four different forms:\nLocal DNS hijacking: An attacker installs Trojan software on a user\u0026rsquo;s computer, then modifies the local DNS settings (cahnge its DNS server to a Rogue DNS server). DNS hijacking using a router: Many routers have weak firmware or use the default passwords they were shipped with. Attackers can take advantage of this to hack a router and change its DNS settings, which will affect everyone that uses that router. Man-in-the-middle (MITM) attacks: Attackers use man-in-the-middle attack techniques to intercept communications between users and a DNS server. They then direct the target to malicious websites. Learn more What Is DNS Hijacking? How to Detect \u0026amp; Prevent It | Fortinet\n# 5.2. DNS spoofing vs DNS (cache) poisoning DNS Poisoning:\nDefinition: The attacker inserts false address records into the DNS server\u0026rsquo;s cache, causing the server to return incorrect IP addresses for domain names. Target: The attack is on the DNS server. Example: If www.example.com is supposed to resolve to IP 1.2.3.4, in a DNS poisoning attack, the DNS server might be tricked into resolving it to 5.6.7.8. DNS Spoofing:\nDefinition: The attacker intercepts and responds to DNS requests with false information, usually before the legitimate response is received. Example: When a user tries to access www.example.com, an attacker might intercept this request and send a fake response directing the user to IP 5.6.7.8 instead of the real IP 1.2.3.4. Learn more: GFW and DNS Poisoning - David\u0026rsquo;s Blog\n# 5.3. Others DNS Poisoning vs DNS Spoofing:\nDNS Poisoning: Inject false DNS info to the real server\u0026rsquo;s cache. DNS Spoofing: Send false response back with a malicious DNS server. Purpose of MAC Address Spoofing:\nTo make a switch forward packets to an attacker\u0026rsquo;s device by mimicking a legitimate MAC address. Use of ICMP Redirects in Man-in-the-Middle Attacks:\nAn attacker sends forged \u0026ldquo;ICMP redirect messages\u0026rdquo; to mislead a host into changing its routing table, diverting traffic through the attacker\u0026rsquo;s machine. ","date":"2023-10-30T22:44:57Z","permalink":"https://blog.yorforger.cc/p/common-network-attacks/","title":"Common Network Attacks"},{"content":" # 1. Strict-Transport-Security Q: Disable HTTP access to the domain, don’t even redirect or link it to SSL. Just inform the users this website is not accessible over HTTP and they have to access it over SSL.\nA: I can\u0026rsquo;t see any technical reason why HTTP needs to be completely blocked either, and many sites do forward HTTP to HTTPS. When doing this it is highly advisable to implement HTTP Strict Transport Security (HSTS) which is a web security mechanism which declares that browsers are to only use HTTPS connections.\nHSTS is implemented by specifying a response header such as Strict-Transport-Security: max-age=31536000. Complying user agents will automatically turn insecure links into secure links, thereby reducing the risk of man-in-the-middle attacks.\nSource: security - Is redirecting http to https a bad idea? - Stack Overflow\nLearn more: Common Network Attacks - David\u0026rsquo;s Blog\n# 2. keep-alive HTTP keep-alive, a.k.a., HTTP persistent connection, is an instruction that allows a single TCP connection to remain open for multiple HTTP requests/responses.\nBy default, HTTP connections close after each request. When someone visits your site, their browser needs to create new connections to request each of the files that make up your web pages (e.g. images, Javascript, and CSS stylesheets), a process that can lead to high page load times.\nEnabling the keep-alive header allows you to serve all web page resources over a single TCP connection. Keep-alive also reduces both CPU and memory usage on your server.\nLearn more: What is HTTP Keep Alive | Benefits of Connection Keep Alive | Imperva\n# 3. user-agent The User-Agent request header is a characteristic string that lets servers and network peers identify the application, operating system, vendor, and/or version of the requesting user agent.\n","date":"2023-10-30T22:06:07Z","permalink":"https://blog.yorforger.cc/p/http-headers-2/","title":"HTTP Headers (2)"},{"content":" # 1. Parse raw query string 1 2 3 4 5 6 7 8 9 10 func (s *server) ServeHTTP(w http.ResponseWriter, r *http.Request) { // handle asset. const assetPrefix = \u0026#34;asset=\u0026#34; if strings.HasPrefix(r.URL.RawQuery, assetPrefix) { r.URL.RawQuery[len(assetPrefix):] s.asset(w, r, assetName) return } ... } If there are whitespaces in query string, the whitespace will be encoded to %20 or + automatically, js code:\n1 2 3 4 5 6 async function getUrl(filepath) { const filename = \u0026#34;back ground.png\u0026#34; const url = \u0026#34;?asset=\u0026#34; + filename let response = await fetch(url) ... } The request URL will be: http://localhost:8080?filepath=back%20ground.png. Therefore, if you use r.URL.RawQuery in Go:\n1 2 log.Println(r.URL.RawQuery) log.Println(r.URL.Query()) This will print:\n1 2 2023/10/29 12:19:36 filepath=back%20ground.png 2023/10/29 12:19:36 map[filepath:[back ground.png]] As you can see, r.URL.RawQuery isn\u0026rsquo;t friendly to string with whitespace, you should try to use r.URL.Query() to emliminate the %20 characters.\nSo the recommended way is as below:\n1 2 3 4 5 6 7 8 9 10 11 func (s *server) ServeHTTP(w http.ResponseWriter, r *http.Request) { // handle asset. const assetPrefix = \u0026#34;asset=\u0026#34; if strings.HasPrefix(r.URL.RawQuery, assetPrefix) { // r.URL.Query() returns a map[string][]string assetName := r.URL.Query()[strings.TrimSuffix(assetPrefix, \u0026#34;=\u0026#34;)][0] s.asset(w, r, assetName) return } ... } # 2. r.URL.Path vs r.URL.RawPath 1 2 3 4 5 6 7 8 9 func main() { u, err := url.Parse(\u0026#34;http://example.com/x/xx%20a\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(\u0026#34;Path:\u0026#34;, u.Path) fmt.Println(\u0026#34;RawPath:\u0026#34;, u.RawPath) fmt.Println(\u0026#34;EscapedPath:\u0026#34;, u.EscapedPath()) } 1 2 3 Path: /x/xx a RawPath: EscapedPath: /x/xx%20a If url changes to \u0026ldquo;http://example.com/x/xx%2Fa\", then will print:\n1 2 3 Path: /x/xx/a RawPath: /x/xx%2Fa EscapedPath: /x/xx%2Fa In general, code should call EscapedPath() instead of reading u.RawPath directly.\nLearn more:\nhttps://pkg.go.dev/net/url#URL.EscapedPath\nURL Encoding (Percent Encoding) - David\u0026rsquo;s Blog\n# 3. Relative path You can write relative path directly for the endpoint, because the browser know the Origin, when you make HTTP request, it knows where should go.\n1 2 3 \u0026lt;form method=\u0026#34;post\u0026#34; action=\u0026#34;/login\u0026#34;\u0026gt; ... \u0026lt;/form\u0026gt; And it\u0026rsquo;s ok to write relative path when redirect in Go code:\n1 2 // Redirect to login page. http.Redirect(w, r, \u0026#34;/login\u0026#34;, http.StatusFound) # 4. Redirection # 4.1. Redirect at front end For redirection, you can use js code to redirect based on the status code passed from server:\n1 2 3 4 5 6 7 8 9 10 11 const response = await fetch(\u0026#34;/login\u0026#34;, { method: \u0026#34;POST\u0026#34;, body: data, }) if (!response.ok) { ... return } // If login successfully, redirect to /home window.location = \u0026#34;/home\u0026#34; # 4.2. Redirect at server with Location header Learn more: HTTP Headers - David\u0026rsquo;s Blog\n# 4.3. Redirect at server with http.Redirect() method See above Relative path section.\n# 5. Parse form and query string 1 func (r *Request) ParseForm() error For all requests, ParseForm() parses the raw query from the URL and updates r.Form. For POST, PUT, and PATCH requests, it also reads the request body, parses it as a form and puts the results into both r.PostForm and r.Form. Request body parameters take precedence over URL query string values in r.Form. For other HTTP methods, or when the Content-Type is not application/x-www-form-urlencoded, the request Body is not read.\nTherefore, you can pass data in query string or in the request body. When you pass data with request body and the Content-Type header is application/x-www-form-urlencoded, it will be same to sending data with query stirng.\nThe Content-Type header can be application/json, application/x-www-form-urlencoded, multipart/form-data, learn more: Curl Basics - David\u0026rsquo;s Blog\nHow to send form data in application/x-www-form-urlencoded format: Tricks in Javascript - David\u0026rsquo;s Blog\nLearn more: http package - net/http - Go Packages\n# 6. Check the type of the request When You are starting a HTTP/s server You use either ListenAndServe or ListenAndServeTLS or both together on different ports. If You are using just one of them, then from Listen.. it\u0026rsquo;s obvious which scheme request is using and You don\u0026rsquo;t need a way to check and set it. But if You are serving on both HTTP and HTTP/s then You can use request.TLS state. if its nil it means it\u0026rsquo;s HTTP.\n1 2 3 4 5 6 7 8 // TLS allows HTTP servers and other software to record // information about the TLS connection on which the request // was received. This field is not filled in by ReadRequest. // The HTTP server in this package sets the field for // TLS-enabled connections before invoking a handler; // otherwise it leaves the field nil. // This field is ignored by the HTTP client. TLS *tls.ConnectionState an example:\n1 2 3 4 5 6 7 func index(w http.ResponseWriter, r *http.Request) { scheme := \u0026#34;http\u0026#34; if r.TLS != nil { scheme = \u0026#34;https\u0026#34; } w.Write([]byte(fmt.Sprintf(\u0026#34;%v://%v%v\u0026#34;, scheme, r.Host, r.RequestURI))) } Source: https://stackoverflow.com/a/76143800/16317008\n# 7. Receive file from http request # 7.1. Issue 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 func (s *server) handleUpload(w http.ResponseWriter, r *http.Request, currentDir string) (error, int) { maxFileSize := int64(s.maxFileSize * 1024 * 1024) // limit the size of incoming request bodies. r.Body = http.MaxBytesReader(w, r.Body, maxFileSize) // parse form from request body. // !!!This will load the file into RAM if err := r.ParseMultipartForm(maxFileSize); err != nil { return fmt.Errorf(\u0026#34;file is too large:%v\u0026#34;, err), http.StatusBadRequest } // obtain file from parsed form. parsedFile, parsedFileHeader, err := r.FormFile(\u0026#34;file\u0026#34;) if errors.Is(err, http.ErrMissingFile) { w.Header().Set(\u0026#34;Location\u0026#34;, r.URL.String()) } if err != nil { return err, http.StatusSeeOther } defer parsedFile.Close() dstPath := filepath.Join(currentDir, filepath.Base(parsedFileHeader.Filename)) var dst *os.File _, err = os.Stat(dstPath) // the name of parsed file already exists, create a dst file with a new name. if err == nil { filename := strings.Split(parsedFileHeader.Filename, \u0026#34;.\u0026#34;)[0] + \u0026#34;_\u0026#34; + strconv.FormatInt(time.Now().UnixNano(), 10) + filepath.Ext(parsedFileHeader.Filename) dst, err = os.Create(filepath.Join(currentDir, filename)) } else if os.IsNotExist(err) { // the name of parsed file already exists, create a dst file with original name. dst, err = os.Create(dstPath) } if err != nil { return fmt.Errorf(\u0026#34;failed to create file: %v\u0026#34;, err), http.StatusInternalServerError } defer dst.Close() _, err = io.Copy(dst, parsedFile) if err != nil { return fmt.Errorf(\u0026#34;failed to copy file: %v\u0026#34;, err), http.StatusInternalServerError } // considering the buffering mechanism, getting error when close a writable file is needed. if err = dst.Close(); err != nil { return fmt.Errorf(\u0026#34;failed to close dst fileHtml: %v\u0026#34;, err), http.StatusInternalServerError } // clean url and redirect r.URL.RawQuery = \u0026#34;\u0026#34; w.Header().Set(\u0026#34;Location\u0026#34;, r.URL.String()) w.WriteHeader(http.StatusSeeOther) return nil, http.StatusSeeOther } http.MaxBytesReader(): Doesn\u0026rsquo;t load any data, it just wraps the request body with a http.maxBytesReader, learn more: IO in Golang - David\u0026rsquo;s Blog\nUsage of r.ParseMultipartForm(maxFileSize): This is where the actual parsing of data (request body) occurs. The whole request body is parsed and up to a total of maxMemory bytes of its file parts are stored in memory, with the remainder stored on disk in temporary files. This will cause a spike in the usage of RAM, because it doesn\u0026rsquo;t stream the data, it loads maxMemory bytes of data into memory, and the rest of data into disk one time. Learn more: IO in Golang - David\u0026rsquo;s Blog\nReading and Writing the File: The file is read from the form data and written to the destination path. The io.Copy function used here is efficient as it streams the data to dst file.\n# 7.2. Solution 1 func (r *Request) MultipartReader() (*multipart.Reader, error) MultipartReader returns a MIME multipart reader if this is a multipart/form-data or a multipart/mixed POST request, else returns nil and an error. Use this function instead of ParseMultipartForm to process the request body as a stream.\nNote: When use MultipartReader(), you cannot use http.MaxBytesReader() to limit the size of incoming request bodies as what we did with ParseMultipartForm(). Because r.ParseMultipartForm() will return an error if the request body is larger than maxMemory, which MultipartReader() doesn\u0026rsquo;t. If you try use http.MaxBytesReader() with MultipartReader(), when the request body exceeds the specified max size, part, err := reader.NextPart() will return an error:\n1 err.Error(): multipart: NextPart: http: request body too large You can do something like this:\n1 2 3 4 5 6 7 8 9 10 11 reader, err := r.MultipartReader() ... for { part, err := reader.NextPart() if err != nil { if err.Error() == \u0026#34;multipart: NextPart: http: request body too large\u0026#34; { ... } ... } } ","date":"2023-10-29T11:30:50Z","permalink":"https://blog.yorforger.cc/p/some-http-issues-with-go/","title":"Some HTTP Issues with Go"},{"content":" # 1. Delay function 1 2 3 4 5 function delay(minutes) { return new Promise(resolve =\u0026gt; setTimeout(resolve, minutes * 1000 * 60)); } // await delay(2) # 2. Click copy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;p id=\u0026#34;url-text\u0026#34; style=\u0026#34;display: none\u0026#34;\u0026gt;{{.SharedUrl}}\u0026lt;/p\u0026gt; \u0026lt;button onclick=\u0026#34;copyText()\u0026#34;\u0026gt;share\u0026lt;/button\u0026gt; \u0026lt;script\u0026gt; function copyText() { const copyText = document.getElementById(\u0026#34;url-text\u0026#34;); navigator.clipboard.writeText(copyText.textContent) .then(() =\u0026gt; { alert(\u0026#34;successfully copied\u0026#34;); }) .catch(() =\u0026gt; { alert(\u0026#34;something went wrong\u0026#34;); }); } \u0026lt;/script\u0026gt; # 3. Relative path You can write relative path directly for the endpoint, because the browser know the Origin, when you make HTTP request, it knows where should go.\n1 2 3 \u0026lt;form method=\u0026#34;post\u0026#34; action=\u0026#34;/login\u0026#34;\u0026gt; ... \u0026lt;/form\u0026gt; And it\u0026rsquo;s ok to write relative path when redirect in Go code:\n1 2 // Redirect to login page. http.Redirect(w, r, \u0026#34;/login\u0026#34;, http.StatusFound) # 4. Send username and password form If you have a login page with user name and password, do not tell users that they entered a wrong password. Tell them that they entered wrong credentials. You don’t want to give hackers a way to know if a user name is right or wrong. source\nTraditional way to submit a form:\n1 2 3 4 5 \u0026lt;form action=\u0026#34;/login\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;name\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; name=\u0026#34;psaaword\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Submit\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; The form data will be sent to /login endpoint of the server automatically. After check the input, you may make ajax with fetch() to send form data:\n1 2 3 4 5 6 7 let data = new FormData(document.getElementById(\u0026#34;login-form\u0026#34;)) ... let response = await fetch(\u0026#34;/login\u0026#34;, { method: \u0026#34;POST\u0026#34;, headers:{\u0026#34;Content-Type\u0026#34;:\u0026#34;application/x-www-form-urlencoded\u0026#34;}, body: data, }) Then the actual request body sent by browser will be:\n1 2 3 4 5 6 7 8 \u0026#34;username\u0026#34; david ------WebKitFormBoundaryGx7UioRfdfhdT5U5 Content-Disposition: form-data; name=\u0026#34;password\u0026#34; 778899sS ------WebKitFormBoundaryGx7UioRfdfhdT5U5-- This is not correct and you will get error at the server, the suggestion to get a correct request body is to not set header explictly:\n1 2 3 4 5 let response = await fetch(\u0026#34;/login\u0026#34;, { method: \u0026#34;POST\u0026#34;, //headers:{\u0026#34;Content-Type\u0026#34;:\u0026#34;application/x-www-form-urlencoded\u0026#34;}, body: data, }) Then the correct form data will be sent:\n1 2 username: david password: 778899s Note: if you don\u0026rsquo;t set Content-Type header explicitly, the browser will set Content-Type header to multipart/form-data automatically.\n# 4.1. Special situation in Go If your server written in Go, you may parse the form like this:\n1 2 3 4 5 6 if e := r.ParseForm(); err != nil { err = fmt.Errorf(\u0026#34;failed to parse username and password: %v\u0026#34;, e) return } username = r.Form.Get(\u0026#34;username\u0026#34;) password = r.Form.Get(\u0026#34;password\u0026#34;) According to Golang docs for ParseForm() :\nFor other HTTP methods, or when the Content-Type is not application/x-www-form-urlencoded, the request Body is not read.\nTherefore, ParseForm() method will ignore the request body, which means you cannot get the username and password. The URLSearchParams() in JS can help:\n1 2 3 4 5 6 7 8 9 const data = new URLSearchParams() for (const pair of new FormData(document.getElementById(\u0026#34;login-form\u0026#34;))) { data.append(pair[0], String(pair[1])) } let response = await fetch(\u0026#34;/login\u0026#34;, { method: \u0026#34;POST\u0026#34;, //headers:{\u0026#34;Content-Type\u0026#34;:\u0026#34;application/x-www-form-urlencoded\u0026#34;}, body: data, }) I find this solution on an answer at Stackoverflow, the problem caused by new FormData():\nThe FormData interface provides a way to easily construct a set of key/value pairs representing form fields and their values, which can then be easily sent using the XMLHttpRequest.send() method. It uses the same format a form would use if the encoding type were set to \u0026quot;multipart/form-data\u0026quot;.\nSo when using FormData you are locking yourself into multipart/form-data. There is no way to send a FormData object as the body and not sending data in the multipart/form-data format.\nIf you want to send the data as application/x-www-form-urlencoded you will either have to specify the body as an URL-encoded string, or pass a URLSearchParams object.\n","date":"2023-10-28T22:54:02Z","permalink":"https://blog.yorforger.cc/p/tricks-in-javascript/","title":"Tricks in Javascript"},{"content":" # 2xx 200 OK: Request is okay, entity body contains requested resource. 201 Created: For requests that create server objects (e.g., PUT). The entity body of the response should contain the various URLs for referencing the created resource, with the Location header containing the most specific reference. The server must have created the object prior to sending this status code. # Redirect # 301: permanent redirect: 可能会修改请求方法 The HyperText Transfer Protocol (HTTP) 301 Moved Permanently redirect status response code indicates that the requested resource has been definitively moved to the URL given by the Location headers. A browser redirects to the new URL and search engines update their links to the resource. 根据描述可以看出, 301 是用来回复 GET 请求的, 与下面 303 刚好相反\nAlthough the specification requires the method and the body to remain unchanged when the redirection is performed, not all user-agents meet this requirement. Use the 301 code only as a response for GET or HEAD methods and use the 308 Permanent Redirect for POST methods instead, as the method change is explicitly prohibited with this status. 301 Moved Permanently - HTTP | MDN\n这段话指的是在处理 HTTP 重定向时，不同的状态码会对浏览器或用户代理（如 Web 浏览器）的行为有不同的指示，尤其是在处理 HTTP 方法（如 GET, POST, HEAD 等）和请求体的保留方面。\n场景: 假设你有一个表单提交到 URL http://example.com/form，使用 POST 方法。由于某些原因，这个表单处理的 URL 永久变更到了 http://example.com/new-form。\n常规做法: 使用 301 状态码来重定向到新的 URL。但是，根据 HTTP/1.0 标准，这可能会导致某些用户代理（特别是旧的浏览器或不完全遵循标准的客户端）在重定向时将 POST 请求改变为 GET 请求，这可能不是预期的行为。\n场景: 同样的情况，表单从 http://example.com/form 移动到 http://example.com/new-form。\n更安全的做法: 使用 308 Permanent Redirect。这个状态码明确禁止在重定向过程中改变请求方法。因此，如果原始请求是 POST，重定向后的请求也必须是 POST。这样可以确保重定向行为的一致性和预期性，不管用户代理是什么。\n# 303: temporary redirect: 修改请求方法为 GET 303: Seee Other, 看名字就可以知道是 303 是告诉浏览器去另一个页面, 注意是一个页面, 所以浏览器收到 303 请求后, 会用 GET 请求来访问重定向的 URL, 即使之前的请求是 POST 请求. 这个状态码经常是用来做表单提交后的重定向, 即用来回复 POST 和 PUT 请求. The HyperText Transfer Protocol (HTTP) 303 See Other redirect status response code indicates that the redirects don\u0026rsquo;t link to the requested resource itself, but to another page (such as a confirmation page, a representation of a real-world object). This response code is often sent back as a result of PUT or POST. The method used to display this redirected page is always GET. 303 See Other - HTTP | MDN # 307: temporary redirect: 不会修改请求方法和请求体 The method and the body of the original request are reused to perform the redirected request.\nIn the cases where you want the method used to be changed to GET, use 303 See Other instead. This is useful when you want to give an answer to a PUT method that is not the uploaded resources, but a confirmation message (like \u0026ldquo;You successfully uploaded XYZ\u0026rdquo;).\nThe only difference between 307 and 302 is that 307 guarantees that the method and the body will not be changed when the redirected request is made. With 302, some old clients were incorrectly changing the method to GET: the behavior with non-GET methods and 302 is then unpredictable on the Web, whereas the behavior with 307 is predictable. For GET requests, their behavior is identical.\nLearn more: 307 Temporary Redirect - HTTP | MDN\n# 308: permanent redirect, 不修改请求方法和请求体 The request method and the body will not be altered, whereas 301 may incorrectly sometimes be changed to a GET method.\nLearn more: 308 Permanent Redirect - HTTP | MDN\n# Redirect Code mistakes Different redirect status code can cause browser behave differently.\nAn unauthorized user make a request to /home which needs authorized state to access, he will be redirect to /login, if server respond 308, then the unauthorized user will be redirect to /login, this woks fine. But 308 tell the brower all following http requests for /home need be redirected to /login even the user login successfully, you can clear the browser cache to reset, make sure don\u0026rsquo;t repond a wrong status code.\n","date":"2023-10-26T15:30:05Z","permalink":"https://blog.yorforger.cc/p/http-status-codes/","title":"HTTP Status Codes"},{"content":" # 0. Header types Headers can be grouped according to their contexts:\nRequest headers contain more information about the resource to be fetched, or about the client requesting the resource. Response headers hold additional information about the response, like its location or about the server providing it. Representation headers contain information about the body of the resource, like its MIME type, or encoding/compression applied. Payload headers contain representation-independent information about payload data, including content length and the encoding used for transport. Learn more: HTTP headers - HTTP | MDN\n# 1. Content-Type Common values of Content-Type header can be application/json, application/x-www-form-urlencoded, multipart/form-data, the first two is usually used with posting form data to server, the third is used to upload file to the server.\napplication/x-www-form-urlencoded: the keys and values are encoded in key-value tuples separated by '\u0026amp;', with a '=' between the key and the value. The format of application/x-www-form-urlencoded is \u0026quot;username=davidzhu\u0026amp;password=778899a\u0026quot; . The format of application/json is '{\u0026quot;username\u0026quot;:\u0026quot;davidzhu\u0026quot;, \u0026quot;password\u0026quot;:\u0026quot;778899a\u0026quot;}'. multipart/form-data: each value is sent as a block of data (\u0026ldquo;body part\u0026rdquo;), with a user agent-defined delimiter (\u0026ldquo;boundary\u0026rdquo;) separating each part. The keys are given in the Content-Disposition header of each part. text/plain HTML provides no way to generate JSON from form data, learn more: Form Data \u0026amp; Query String - David\u0026rsquo;s Blog\n# 2. Access-Control-Allow-Origin Access-Control-Allow-Origin header allows servers to specify which origins are allowed to access their resources, even if they are from different origins.\nLearn more: Cross-origin Request HTTP - David\u0026rsquo;s Blog\n# 3. Location The Location response header indicates the URL to redirect a page to. It only provides a meaning when served with a 3xx (redirection) or 201 (created) status response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func (s *server) handleMkdir(w http.ResponseWriter, r *http.Request, currentPath string) error { // Parse form. if err := r.ParseForm(); err != nil { err = fmt.Errorf(\u0026#34;failed to parse folder name: %v\u0026#34;, err) return err } ... // clean url and redirect r.URL.RawQuery = \u0026#34;\u0026#34; w.Header().Set(\u0026#34;Location\u0026#34;, r.URL.String()) w.WriteHeader(http.StatusSeeOther) return nil } Note that Location is a response header.\nAjax call in js doesn\u0026rsquo;t need redirection to go back to the current page.\nStatus code 303: HTTP Status Codes - David\u0026rsquo;s Blog\n# 4. Cache-Control cache-control: no-cache: This directive means that cached versions of the requested resource cannot be used without first checking to see if there is an updated version. max-age=0 is a workaround for no-cache , because many old (HTTP/1.0) cache implementations don\u0026rsquo;t support no-cache.\ncache-control: private: A response with a private directive can only be cached by the client and never by an intermediary agent, such as a CDN or a proxy. These are often resources containing private data, such as a website displaying a user’s personal information.\ncache-control: public: Conversely, the public directive means the requested resource can be stored by any cache.\nWhen we access a web page, there may have multiple http requests being made, such as favicon.ico, xxx.js, xxx.json, xxx.png, for each of these resources there is a http request needed to be sent, and indeed there is a corresponding http response. These resourses are called the requested resources above we mentioned. So each of the repsonse have their own http headers:\nBrowser caching is a great way to both preserve resources and improve user experience on the Internet, but without cache-control, it would be very brittle. Every resource on every site would be bound by the same caching rules, meaning that sensitive information would be cached the same way as public information, and frequently-updated resources would be cached for the same amount of time as ones that rarely change.\nThe code below is written in Go which responsible for handle assets request from client, favicon.ico for example.\n1 2 3 4 5 func RenderAsset(w http.ResponseWriter, r *http.Request, assetPath string) { header := w.Header() header.Set(\u0026#34;Cache-Control\u0026#34;, \u0026#34;public, max-age=0\u0026#34;) http.ServeFile(w, r, filename) } Browser caching is when a web browser saves website resources so it doesn’t have to fetch them again from a server. For example, a background image on a website might be saved locally in cache so that when a user visits that page for the second time, the image will load from the user’s local files and the page will load much faster.\nCookies are HTTP Headers. The header is called Cookie:, and it contains your cookie.\nLearn more: What is cache-control? | Cache explained | Cloudflare\n# 5. Authorization \u0026amp; WWW-Authenticate HTTP supports the use of several authentication mechanisms to control access to pages and other resources. These mechanisms are all based around the use of the 401 status code and the WWW-Authenticate response header.\nThe most widely used HTTP authentication mechanisms are:\nBasic: the client sends the user name and password as unencrypted base64 encoded text. It should only be used with HTTPS, as the password can be easily captured and reused over HTTP.\nDigest: the client sends a hashed form of the password to the server. Although, the password cannot be captured over HTTP, it may be possible to replay requests using the hashed password.\nNTLM: this uses a secure challenge/response mechanism that prevents password capture or replay attacks over HTTP. However, the authentication is per connection and will only work with HTTP/1.1 persistent connections. For this reason, it may not work through all HTTP proxies and can introduce large numbers of network roundtrips if connections are regularly closed by the web server.\nIn this section, we will just discuss the Basic authentication mechanism:\nIf an HTTP receives an anonymous request for a protected resource it can force the use of Basic authentication by rejecting the request with a 401 (Access Denied) status code and setting the WWW-Authenticate response header as shown below:\n1 2 3 HTTP/1.1 401 Access Denied WWW-Authenticate: Basic realm=\u0026#34;My Server\u0026#34; Content-Length: 0 The word Basic in the WWW-Authenticate selects the authentication mechanism that the HTTP client must use to access the resource. The realm string can be set to any value to identify the secure area and may used by HTTP clients to manage passwords. Most web browsers will display a login dialog when this response is received, allowing the user to enter a username and password. This information is then used to retry the request with an Authorization request header:\n1 2 3 GET /securefiles/ HTTP/1.1 Host: www.httpwatch.com Authorization: Basic aHR0cHdhdGNoOmY= The Authorization specifies the authentication mechanism (in this case Basic) followed by the username and password. Although, the string aHR0cHdhdGNoOmY= may look encrypted it is simply a base64 encoded version of :. In this example, the un-encoded string \u0026ldquo;httpwatch:foo\u0026rdquo; was used and would be readily available to anyone who could intercept the HTTP request.\nThe HTTP Authorization request header can be used to provide credentials that authenticate a user agent with a server, allowing access to a protected resource.\nThe HTTP WWW-Authenticate response header defines the HTTP authentication methods (\u0026ldquo;challenges\u0026rdquo;) that might be used to gain access to a specific resource.\nLearn more: http://www.httpwatch.com/httpgallery/authentication/\nThe Authorization header is usually, but not always, sent after the user agent first attempts to request a protected resource without credentials. The server responds with a 401 Unauthorized message that includes at least one WWW-Authenticate header. This header indicates what authentication schemes can be used to access the resource (and any additional information needed by the client to use them).\nWarning: The \u0026ldquo;Basic\u0026rdquo; authentication scheme used in the diagram above sends the credentials encoded but not encrypted. This would be completely insecure unless the exchange was over a secure connection (HTTPS/TLS).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func(w http.ResponseWriter, r *http.Request) { username, password, ok := r.BasicAuth() if ok { usernameMatch := subtle.ConstantTimeCompare(usernameHash[:], expectedUsernameHash[:]) == 1 passwordMatch := subtle.ConstantTimeCompare(passwordHash[:], expectedPasswordHash[:]) == 1 if usernameMatch \u0026amp;\u0026amp; passwordMatch { ... return } } w.Header().Set(\u0026#34;WWW-Authenticate\u0026#34;, `Basic realm=\u0026#34;restricted\u0026#34;, charset=\u0026#34;UTF-8\u0026#34;`) http.Error(w, \u0026#34;Unauthorized\u0026#34;, http.StatusUnauthorized) } Basic realm=\u0026quot;restricted\u0026quot;, what does this mean?\nIn short, endpoints in the same realm should share credentials. If your credentials work for a endpoint with the realm \u0026quot;restricted\u0026quot;, it should be assumed that the same username and password combination should work for another endpoint with the same realm.\nHow to group pages (endpoints) with realm?\nrealm value doesn\u0026rsquo;t have magic, you still need apply the auth middleware for each endpoint at server side. But different value of realm will instruct browser use different credentials (username, password) for the different request url automatically. The concept of realms in the context of authentication is primarily used to instruct the client (typically a web browser) to send different credentials for different request URLs automatically. The server-side implementation still requires applying the authentication middleware to each endpoint.\nCode credit to: Go BasicAuth\nHTTP authentication framework: HTTP authentication - HTTP | MDN\nLearn more: Authorization - HTTP | MDN\n# 6. cookie There are two headers related to cookie, one is Set-Cookie header another is Cookie header.\nAfter receiving an HTTP request, a server can send one or more Set-Cookie headers with the response. The browser usually stores the cookie and sends it with requests made to the same server inside a Cookie HTTP header.\nFor example, a response from server contains cookie headers may looks like this:\n1 2 3 4 5 6 HTTP/2.0 200 OK Content-Type: text/html Set-Cookie: yummy_cookie=choco Set-Cookie: tasty_cookie=strawberry [page content] A HTTP request may looks like this below:\n1 2 3 GET /sample_page.html HTTP/2.0 Host: www.example.org Cookie: yummy_cookie=choco; tasty_cookie=strawberry Therefore, when I want set cookie manually for my http request, I\u0026rsquo;ll probably do something like this (I do this in Go):\n1 2 3 4 5 6 req, _ := http.NewRequest(http.MethodPost, \u0026#34;/chat\u0026#34;, your_encoded_message) // set content-type header req.Header.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/x-www-form-urlencoded\u0026#34;) // set cookie header (cookie is a key value data) req.Header.Set(\u0026#34;Cookie\u0026#34;, \u0026#34;session_id=xxxxxxxxxxx\u0026#34;) ... If I want get cookie from repsonse, I\u0026rsquo;ll probably retrieve the cookie like this:\n1 2 response := makeRequest(...) my_cooke := response.Header().Get(\u0026#34;Set-Cookie\u0026#34;) Cookie is just a header which having no doubt resides in the header of HTTP mesages, don\u0026rsquo;t overthinking.\nReference: https://developer.mozilla.org/en-US/docs/Web/HTTP/Cookies#creating_cookies\n","date":"2023-10-26T09:58:10Z","permalink":"https://blog.yorforger.cc/p/http-headers-1/","title":"HTTP Headers (1)"},{"content":"I choose a sqilte3 library which uses cgo, the Dockerfile:\n1 2 3 4 5 6 7 8 FROM golang:alpine WORKDIR /app COPY ./ ./ RUN go mod download RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o /server . CMD [\u0026#34;./server\u0026#34;] The CGO_ENABLED=0 is to disable cgo. GOOS=linux and GOARCH=amd64 is used for cross compilation in Go. Because I build this on my macOS arm64 machine, and I want build for Ubuntu amd64 machine, so I choose GOOS=linux GOARCH=amd64.\nLearn more:\nStatic Linking Go Programs - David\u0026rsquo;s Blog\nCross Compilation - Go - David\u0026rsquo;s Blog\nAfter build successfully and run image:\n1 2 3 $ docker run -p 80:80 shwezhu/file-station:v1 2023/10/10 02:15:15 /app/main.go:12 [error] failed to initialize database, got error Binary was compiled with \u0026#39;CGO_ENABLED=0\u0026#39;, go-sqlite3 requires cgo to work. This is a stub As you can see, apparently I will get an error, because my Go code use the go-sqlite3 package which implemented by pure cgo, if I disable cgo with CGO_ENABLED=0, this will wrong. Then I change the dockerfile to:\n1 2 3 4 5 6 ... # Install gcc to compile cgo RUN apk add --no-cache --update go gcc g++ RUN go build -o /server . CMD [\u0026#34;./server\u0026#34;] And build image with command:\n1 $ docker build -t shwezhu/file-station:v2 . There is an error when run the image on EC2 server:\n1 2 $ docker run -p 80:80 shwezhu/file-station:v1 WARNING: The requested image\u0026#39;s platform (linux/arm64) does not match the detected host platform (linux/amd64) and no specific platform was requested Because my local machine is arm64, which means the image will be built to arm64 by default, but my EC2 server is linux/amd64, so there is an error occurred. With --platform, you can specify the platform this image built for:\n1 $ docker build --platform linux/amd64 -t shwezhu/file-station:v2 . Go is a statically compiled language. To execute a Go binary on a machine, it must be compiled for the matching operating system and processor architecture. So there is cross-compilation in Go. --platform is used to build multi-platform docker images, not build Go for another platform. You should know the difference between these concepts.\n","date":"2023-10-21T10:24:22Z","permalink":"https://blog.yorforger.cc/p/cgo-compile-error-building-docker-image/","title":"CGO Compile Error Building Docker Image"},{"content":" # 1. ISO the International Organization for Standardization To clarify the relationship between ISO, standards and the standard library, ISO publishes standards, which essentially become the standard library. ISO discusses and develops language standards every year, resulting in standards for languages like C99 and C11. These standards primarily cover two main aspects: the functionality of the language itself and the standard library associated with that language. However, it\u0026rsquo;s important to note that these standards are essentially specifications. The actual implementations of these standards are carried out by other entities; for instance, glibc and MSVCRT are implementations of the C standard library.\nFor example, the outcome of their discussions in 1999 is the C99 standard, formally known as ISO/IEC 9899:1999(E) \u0026ndash; Programming Languages \u0026ndash; C, The C99 standard comprises two main parts:\nthe C/C++ features and functionalities;\nthe C/C++ API — a collection of classes, functions and macros that developers use in their C/C++ programs. It is called the Standard Library.\nNo implementation, just a specifications.\n# 2. Implementation of standard library There are functions for memory allocation, creating threads, and input/output operations (such as those in stdio.h) in C language. All of these functions rely on system calls. Therefore, when third-party manufacturers implement the standard library of C language, they must create different versions for the different OS because each OS has its own set of system calls.\n# 2.1. glibc - the linux implementation The GNU C Library and glibc are synonyms, which are the runtime library/standard library for the C programming language.\nIt\u0026rsquo;s important to clarify that the runtime library includes both static and dynamic libraries. The term \u0026ldquo;runtime library\u0026rdquo; is used for a general term.\nThe term library (runtime library) and header are not same. Library are the implementations of the header, which exist as binary files (the static library .a/.lib or the dynamic library .so/.dll ), whereas headers are .h files.\nTherefore, we usually cannot find the source code of the implementation of C standard library, such as function printf(), the implementation of these functions are provided as compiled binary files. But you can find the glibc\u0026rsquo;s implementation of printf() on the internet, because glibc is open source.\nThe printf() function is part of the C Standard Library and is typically provided by the GNU C Library (glibc) on Linux systems. The implementation of printf() in glibc is open source and can be found in the glibc source code.\nprintf() is part of C standard library, which is called \u0026ldquo;standard\u0026rdquo; because it is linked by default by C compiler. Standard library is typically provided by operating system or compiler. On most linux systems, it is located in libc.so, whereas on MS Windows C Library is provided by Visual C runtime file msvcrt.dll.\nhttps://stackoverflow.com/a/37154724/16317008\nLearn more: The GNU C Library\n# 2.2. Mac and iOS Implementation On Mac and iOS the C Standard Library implementation is part of libSystem, a core library located in /usr/lib/libSystem.dylib. LibSystem includes other components such as the math library, the thread library and other low-level utilities.\nC header file on MacOS: /Library/Developer/CommandLineTools/SDKs/MacOSX12.3.sdk/usr/include\nLearn more: https://www.internalpointers.com/post/c-c-standard-library\n# 2.3. Windows Implementation On Windows the implementation of the Standard Libraries has always been strictly bound to Visual Studio, the official Microsoft compiler. They use to call it C/C++ Run-time Library (CRT) and it covers both implementations.\nLearn more: https://www.internalpointers.com/post/c-c-standard-library\n# 3. glibc vs libc As of today glibc is the most widely used C library on Linux. However, during the ‘90s there was for a while a glibc competitor called Linux libc (or just libc), born from a fork of glibc 1.x. For a while, Linux libc was the standard C library in many Linux distributions.\nAfter years of development, glibc turned out to be way superior to Linux libc and all Linux distributions that had been using it switched back to glibc. So don\u0026rsquo;t worry if you find a file in your disk named libc.so.6: it\u0026rsquo;s the modern glibc. The version number got incremented to 6 in order to avoid any confusion with the previous Linux libc versions (they couldn\u0026rsquo;t name it glibc.so.6: all Linux libraries must start with the lib prefix).\n1 2 3 4 $ ls /usr/lib/x86_64-linux-gnu | grep libc libc.a libc.so libc.so.6 Learn more: https://www.internalpointers.com/post/c-c-standard-library\n# 4. glibc vs gcc A few things:\ngcc and glibc are two different things. gcc is the compiler, glibc are the runtime libraries. Pretty much everything needs glibc to run. .a files are static libraries, .so means shared object and is the Linux equivalent of a DLL Most things DON\u0026rsquo;T link against libc.a, they link against libc.so Hope that clears it up for you. As for the location, it\u0026rsquo;s almost certainly going to be in /usr/lib/libc.a and / or /usr/lib/libc.so.\nSource: https://stackoverflow.com/a/5925691/16317008\n# 5. Location of the implementation of library If you are looking for libc.a:\n1 2 3 4 5 $ gcc --print-file-name=libc.a /usr/lib/gcc/x86_64-linux-gnu/11/../../../x86_64-linux-gnu/libc.a $ gcc --print-file-name=libc.so /usr/lib/gcc/x86_64-linux-gnu/11/../../../x86_64-linux-gnu/libc.so Source: https://stackoverflow.com/a/36103882/16317008\n# 6. libc.a va libc.so The size of libc.a is 5.8 MB which is huge for codes, libc.a is a static library, also known as a \u0026ldquo;archive\u0026rdquo; library, It contains compiled object code that gets linked into the final executable at compile time.\n1 2 $ ls -lh /usr/lib/x86_64-linux-gnu/libc.a -rw-r--r-- 1 root root 5.8M Sep 25 14:45 /usr/lib/x86_64-linux-gnu/libc.a 1 2 3 4 5 6 # Don\u0026#39;t archive libc.a directly, archive it on a different folder $ ar -x libc.a $ ls | grep printf printf.o sprintf.o ... 为什么静态运行库里面一个目标文件只包含一个函数？比如libc.a里面printf.o只有printf()函数、strlen.o只有strlen()函数，为什么要这样组织？\n链接器在链接静态库的时候是以目标文件为单位的, 比如我们引用了printf()`函数, 如果进行静态链接的话, 那么链接器就只会把库中包含printf()函数的那个目标文件链接进来, 由于运行库有成百上千个函数, 如果把这些函数都放在一个目标文件中就会很大\u0026hellip;\n如果把整个链接过程比作一台计算机, 那么ld链接器就是计算机的CPU, 所有的目标文件、库文件就是输入, 链接结果输出的可执行文件就是输出, 而链接控制脚本正是这台计算机的“程序”, 它控制CPU的运行, 以“程序”要求的方式将输入加工成所须要的输出结果.\nlibc.so is a shared library, often referred to as a \u0026ldquo;dynamic link library.\u0026rdquo; It contains compiled code that is loaded into memory at runtime, allowing multiple programs to share the same code in memory.\nBoth libc.a and libc.so are implementations of the C library, but they differ in their form and how they are linked to programs.\nWhen we staticlly compile a source file, then libc.a will be used at compiled time, if we dynamically compile a source file (compile with dynamically linked) then libc.so will be used at runtime.\n1 2 3 4 5 6 7 $ gcc -static -o main main.c $ file main main: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, BuildID[sha1]=7fd47f129d345aa2ef6c44b06ffa01be4174d098, for GNU/Linux 3.2.0, not stripped $ ls -lh main -rwxrwxr-x 1 ubuntu ubuntu 880K Oct 18 00:51 main 1 2 3 4 5 6 7 $ gcc -o main main.c $ file main main: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=f14bf2e15cabc179d82a09a2de5bf15da6e5b75c, for GNU/Linux 3.2.0, not stripped $ ls -lh main -rwxrwxr-x 1 ubuntu ubuntu 16K Oct 18 00:54 main As you can see, the dynamically linked binary is very small just 16k compared with the statically linked binary 880K.\n# 7. Conclusion 程序如何使用操作系统提供的API(system call)? 在一般的情况下，一种语言的开发环境往往会附带有语言库（Language Library也可以说是标准库,运行时库）。这些库就是对操作系统的API的包装，比如我们经典的C语言版“Hello World”程序，它使用C语言标准库的“printf”函数来输出一个字符串，“printf”函数对字符串进行一些必要的处理以后，最后会调用操作系统提供的API。各个操作系统下，往终端输出字符串的API都不一样，在Linux下，它是一个“write”的系统调用，而在Windows下它是“WriteConsole”系统API。标准库函数(运行库)依赖的是system call。库里面还带有那些很常用的函数，比如C语言标准库里面有很常用一个函数取得一个字符串的长度叫strlen()，该函数即遍历整个字符串后返回字符串长度，这个函数并没有调用任何操作系统的API，也没有做任何输入输出。但是很大一部分库函数(运行库)都是要调用操作系统的API的.\n“Any problem in computer science can be solved by another layer of indirection.”\n每个层次之间都须要相互通信，既然须要通信就必须有一个通信的协议，我们一般将其称为接口（Interface），接口的下面那层是接口的提供者，由它定义接口；接口的上面那层是接口的使用者，它使用该接口来实现所需要的功能.\n运行时库(标准库, static library, dynamic library) 依赖 system call, 它提供头文件(stdio.h, math.h)供我们使用. 所以它很重要, 它在应用层和操作系统中间. 我们使用它提供的接口(printf())和操作系统进行交流(通过system call).\n我们的软件体系中，位于最上层的是应用程序，比如我们平时用到的网络浏览器、Email客户端、多媒体播放器、图片浏览器等。从整个层次结构上来看，开发工具与应用程序是属于同一个层次的，因为它们都使用一个接口，那就是操作系统应用程序编程接口（Application Programming Interface, 就是标准库的头文件）。应用程序接口(头文件)的提供者是运行库，什么样的运行库提供什么样的API，比如Linux下的Glibc库提供POSIX的API；Windows的运行库提供Windows API，最常见的32位Windows提供的API又被称为Win32。\n运行库使用操作系统提供的系统调用接口（System call Interface），系统调用接口在实现中往往以软件中断（Software Interrupt）的方式提供，比如Linux使用0x80号中断作为系统调用接口，Windows使用0x2E号中断作为系统调用接口（从Windows XP Sp2开始，Windows开始采用一种新的系统调用方式）。\n操作系统内核层对于硬件层来说是硬件接口的使用者，而硬件是接口的定义者，硬件的接口定义决定了操作系统内核，具体来讲就是驱动程序如何操作硬件，如何与硬件进行通信。这种接口往往被叫做硬件规格（Hardware Specification），硬件的生产厂商负责提供硬件规格，操作系统和驱动程序的开发者通过阅读硬件规格文档所规定的各种硬件编程接口标准来编写操作系统和驱动程序。\n\u0026mdash;程序员的自我修养：链接、装载与库\n","date":"2023-10-17T17:58:57Z","permalink":"https://blog.yorforger.cc/p/intros-of-c-standard-library/","title":"Intros of C Standard Library"},{"content":" # 1. /etc/hosts and /etc/resolv.conf On Linux or a Mac, if you add this to /etc/hosts, facebook no longer exists:\n1 127.0.0.1 facebook.com /etc/hosts is used to resolve hostnames to IP addresses on a local machine. They\u0026rsquo;re looked at first.\nNow\u0026hellip; If you don\u0026rsquo;t have an entry for a host in your host file, you need to ask someone what the IP is. That comes from a resolver.\n# 2. Local resolver \u0026amp; recursive resolver Recursive resolver usually located at remote acts as a DNS server, whereas, a DNS stub resolver running on client devices.\nMost Internet users use a recursive resolver provided by their ISP, but there are other options available; for example Cloudflare\u0026rsquo;s 1.1.1.1 or 8.8.8.8 provided by Google.\n1 2 3 4 $ cat /etc/resolv.conf # This is /run/systemd/resolve/stub-resolv.conf managed by man:systemd-resolved(8). # Do not edit. nameserver 127.0.0.53 127.0.0.53 is the DNS server address, you can also manually change it to the IP of any DNS server (for example, change it to the famous Google DNS 8.8.8.8).\n127.x.x.x are loopback addresses that point to the local machine and are bound to the \u0026ldquo;lo\u0026rdquo; (loopback) network device. So who is this DNS server 127.0.0.53?\n1 2 3 $ sudo netstat -anp | grep 127.0.0.53 tcp 0 0 127.0.0.53:53 0.0.0.0:* LISTEN 96729/systemd-resol udp 0 0 127.0.0.53:53 0.0.0.0:* 96729/systemd-resol From the output, we can see that the process uses 127.0.0.53:53 called systemd-resolve. In /etc/resolv.conf it says that it\u0026rsquo;s maintained by the systemd-resolved service. So we can try to check its status with systemctl:\n1 2 3 4 5 $ systemctl status systemd-resolved ● systemd-resolved.service - Network Name Resolution Loaded: loaded (/lib/systemd/system/systemd-resolved.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2023-10-10 01:56:13 UTC; 3 days ago .... We have know that the current DNS server on this machine is systemd-resolve which a DNS stub (client). So, what is the IP address of the actual DNS server? We can use the following command to check:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # $ systemd-resolve --status | grep \u0026#34;DNS Servers\u0026#34; # In systemd 239 \u0026#39;systemd-resolve\u0026#39; has been renamed to \u0026#39;resolvectl\u0026#39; $ resolvectl status | grep \u0026#39;DNS Servers\u0026#39; DNS Servers: 172.31.0.2 $ resolvectl status Global Protocols: -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported resolv.conf mode: stub Link 2 (eth0) Current Scopes: DNS Protocols: +DefaultRoute +LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported Current DNS Server: 172.31.0.2 DNS Servers: 172.31.0.2 DNS Domain: us-east-2.compute.internal DNS stub resolver used to initiate DNS queries, and it sends the DNS query to the recursive resolver, which then performs the actual DNS resolution process (usually provided by an internet service provider or a public DNS resolver like Google\u0026rsquo;s 8.8.8.8).\nReferences: https://zhuanlan.zhihu.com/p/101275725\n","date":"2023-10-13T14:03:57Z","permalink":"https://blog.yorforger.cc/p/dns-stub-and-recursive-resolver-config-files/","title":"DNS Stub and Recursive Resolver - Config Files"},{"content":" # 1. WORKDIR 1 WORKDIR /path/to/workdir The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn\u0026rsquo;t exist, it will be created even if it\u0026rsquo;s not used in any subsequent Dockerfile instruction.\nThe WORKDIR instruction can be used multiple times in a Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:\n1 2 3 WORKDIR /a WORKDIR b RUN pwd The output of the final pwd command in this Dockerfile would be /a/b.\nIf not specified, the default working directory is /. In practice, if you aren\u0026rsquo;t building a Dockerfile from scratch (FROM scratch), the WORKDIR may likely be set by the base image you\u0026rsquo;re using.\nTherefore, to avoid unintended operations in unknown directories, it is best practice to set your WORKDIR explicitly.\n# 2. COPY We usually the following code in Dockerfile:\n1 2 3 ... WORKDIR /app COPY ./test ./ The COPY ./test ./ will be executed when building the image, what COPY ./test ./ does is copy all the files under ./test on your computer into the /app folder of the container. Because ./ means current folder, and we set our current folder to /app by using WORKDIR /app in Dockerfile.\nNote that .dockerignore file is not considered here, if you add .dockerignore file, then before execute COPY ./test ./, docker will check the .dockerignore file first, if the file in the ./test folder machs the file list in .dockerignore file, then COPY ./test ./ will skip that file.\nAnd note that the build command docker build -t docker-learning:v1 ., the last . sign is used to specify the context, in COPY ./test ./, the ./test is the test folder under the context.\n# 3. RUN vs CMD vs ENTRYPOINT # 3.1. Example ENTRYPOINT sets the process to run, while CMD supplies default arguments to that process.\n1 2 3 4 5 6 7 WORKDIR /app COPY ./ ./ RUN go mod download RUN go build -o server ENTRYPOINT [\u0026#34;./server\u0026#34;] CMD [\u0026#34;-p\u0026#34;, \u0026#34;80\u0026#34;] When run image:\n1 docker run -p 80:80 --rm file-server:v1.0 The command ./server -p 80 will run in the container. By the way, thr CMD could be overwritten by docker run:\n1 2 3 4 ... ENTRYPOINT [\u0026#34;./server\u0026#34;] CMD [\u0026#34;-p\u0026#34;, \u0026#34;9000\u0026#34;] When run image:\n1 sudo docker run -p 8080:8080 --rm file-server:v1.0 -port 8080 The command ./server -port 8080 will run, not ./server -p 9000\n# 3.2. Concepts ENTRYPOINT is the process that’s executed inside the container. Images can only have one ENTRYPOINT. If you repeat the Dockerfile instruction more than once, the last one will apply. When an image is created without an ENTRYPOINT, Docker defaults to using /bin/sh -c. CMD is the default set of arguments that are supplied to the ENTRYPOINT process. There can only be one CMD instruction in a Dockerfile. If you list more than one CMD then only the last CMD will take effect. CMD - command triggers while we launch the created docker image. RUN - command triggers while we build the docker image. 1 2 3 ENTRYPOINT [\u0026#34;./server\u0026#34;] CMD [\u0026#34;-p\u0026#34;, \u0026#34;80\u0026#34;] # same as: CMD [\u0026#34;./server\u0026#34;, \u0026#34;-p\u0026#34;, \u0026#34;80\u0026#34;] # 4. docker run vs ENTRYPOINT The docker run command starts a new container using a specified image. When no further arguments are given, the process that runs in the container will exactly match the ENTRYPOINT and CMD defined in the image.\n1 2 # Executes /usr/bin/my-app help $ docker run my-image:latest Technically, it is possible to override the ENTRYPOINT using docker run by setting its --entrypoint flag. Although this is rarely required, the technique can be useful if you want to launch a shell inside a container, such as to inspect the contents of an image’s filesystem:\n1 2 # Executes bash -c \u0026#34;ls /\u0026#34; $ docker run --entrypoint bash my-image:latest -c \u0026#34;ls /\u0026#34; Note that there is no bash under the /bin folder of alpine:latest linux system which our container based on, there just two shells: ash and sh. So the command above will get an error.\n1 docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \u0026#34;bash\u0026#34;: executable file not found in $PATH: unknown. You can try:\n1 2 3 4 $ docker run --entrypoint /bin/sh docker-learning:v1 -c \u0026#34;cat /etc/shells\u0026#34; # valid login shells /bin/sh /bin/ash Or\n1 2 3 4 $ docker run docker-learning:v1 cat /etc/shells # valid login shells /bin/sh /bin/ash Both of these are same, because the default entrypoint is /bin/sh -c.\nReference:\nDockerfile reference | Docker Docs Docker ENTRYPOINT and CMD : Differences \u0026amp; Examples ","date":"2023-10-11T12:27:35Z","permalink":"https://blog.yorforger.cc/p/dockerfile-synatax/","title":"Dockerfile Synatax"},{"content":" # 1. Docker architecture Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface.\nLearn more about the Cocker client(docker), Docker daemon(dockerd) and Docker objects (images, containers): https://docs.docker.com/get-started/overview/\n# 2. Docker build architecture Docker Build implements a client-server architecture, where:\nBuildx is the client and the user interface for running and managing builds BuildKit is the server, or builder, that handles the build execution. As of Docker Engine 23.0 and Docker Desktop 4.19, Buildx is the default build client.\nIn newer versions of Docker Desktop and Docker Engine, you\u0026rsquo;re using Buildx by default when you invoke the docker build command. In earlier versions, to build using Buildx you would use the docker buildx build command.\nSourcce: Docker Build architecture | Docker Docs\nLearn more: Builders | Docker Docs\n","date":"2023-10-11T10:38:35Z","permalink":"https://blog.yorforger.cc/p/docker-architecture/","title":"Docker Architecture"},{"content":"Previous post: Statically Linking in C - David\u0026rsquo;s Blog\n# 1. Static linking on linux Go creates static binaries by default unless you use cgo to call C code, in which case it will create a dynamically linked binary. The easiest way to check if your program is statically compiled is to run file on it.\nstandard packages os/user and net use cgo, so importing either (directly or indirectly) will result in a dynamic binary.\nNote that net use cgo does\u0026rsquo;t mean that all its implementation is cgo, cgo is just used for Name Resolution and some teivial features. https://pkg.go.dev/net#section-documentation\nI do this test on my Ubuntu server firstly without cgo:\n1 2 3 4 5 6 7 package main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;hello\u0026#34;) } 1 2 3 4 5 6 7 8 9 10 # ubuntu @ ip-172-31-12-228 in ~/codes [19:40:23] $ go build -o server main.go # ubuntu @ ip-172-31-12-228 in ~/codes [19:40:31] $ file server server: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, Go BuildID=hjbIteBvAg_rZ86av_gy/k1xYD8duMhRTtrThDrrX/5yBtTaOBDsf4F2IOwADX/U1b5vnivY9rWcRUWpC_A, with debug_info, not stripped # ubuntu @ ip-172-31-12-228 in ~/codes [19:40:36] $ ldd server not a dynamic executable As you can see, I just use fmt package, and the executable file is statically linked.\nAnd then I change the go code to:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { srv := http.NewServeMux() srv.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { _, _ = fmt.Fprintln(w, \u0026#34;hello world\u0026#34;) }) fmt.Println(\u0026#34;running...\u0026#34;) _ = http.ListenAndServe(\u0026#34;:8080\u0026#34;, srv) } Then build it on Ubtutu machine:\n1 2 3 4 5 6 7 8 9 10 11 12 # ubuntu @ ip-172-31-12-228 in ~/codes [19:47:06] $ go build -o server main.go # ubuntu @ ip-172-31-12-228 in ~/codes [19:47:31] $ file server server: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, Go BuildID=KCFaacb5_zSot7hqkTv8/oYQa-0nbl_Gq2_YxF6JO/BnF2hmfFNgVx-UHRKxMt/Oj91sMcK9_or35yi4Xd0, with debug_info, not stripped # ubuntu @ ip-172-31-12-228 in ~/codes [19:47:39] $ ldd server linux-vdso.so.1 (0x00007fff10cfb000) libc.so.6 =\u0026gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f4cd6200000) /lib64/ld-linux-x86-64.so.2 (0x00007f4cd6612000) The binary file is dynamically linked as we expected.\n# 1.1. Disable dynamically linking with CGO_ENABLED=0 1 2 3 4 5 6 # ubuntu @ ip-172-31-12-228 in ~/codes [19:48:57] $ CGO_ENABLED=0 go build -o server main.go # ubuntu @ ip-172-31-12-228 in ~/codes [20:11:01] $ file server server: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, Go BuildID=wGRY1RH-HeASVOwzThcj/lQNxgqzqGUe1P8n_WjN7/cEcN362GspK8XKl2L0AG/F7hVHMJfVIyYcLM6Jhz1, with debug_info, not stripped Note that the CGO_ENABLED=0 is to disable cgo. It is disabled by default when cross-compiling. You can control this by setting the CGO_ENABLED environment variable when running the go tool: set it to 1 to enable the use of cgo, and to 0 to disable it.\nIf CGO_ENABLED=0 is set, the Go net package will not use cgo, and instead, it will use a pure Go implementation for its networking functionality.\nLearn more: https://go-review.googlesource.com/c/go/+/12603/2/src/cmd/cgo/doc.go\n# 2. Static linking on osx On Mac, the behavior is totoally different, even don\u0026rsquo;t use cgo the final executable will be dynamically linked.\n1 2 3 4 5 6 7 package main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;hello\u0026#34;) } Build on MacOS machine:\n1 2 3 4 5 6 7 8 9 10 11 $ go build -o server main.go $ file server server: Mach-O 64-bit executable arm64 # otool is similar to \u0026#39;ldd\u0026#39; on linux # -L print shared libraries used $ otool -L server server: /usr/lib/libSystem.B.dylib (compatibility version 0.0.0, current version 0.0.0) /usr/lib/libresolv.9.dylib (compatibility version 0.0.0, current version 0.0.0) CGO_ENABLED=0 won\u0026rsquo;t help on MaxOS. And I found something could be explain this:\nI think this won\u0026rsquo;t work on macOS, where fully static builds are not allowed/supported by Apple. Binaries should always go through libSystem, which is also why we changed the way Go calls the kernel in Go 1.12. So, pure Go binaries are already as static as they can be, as far as I can tell.\nI propose that on macOS go build -static simply tries to statically link cgo libraries, so that the final binary doesn\u0026rsquo;t depend on third-party. So but just on system libraries. To do this, unfortunately, it looks like it\u0026rsquo;s not sufficient to add the output of pkg-config --static --libs to LDFLAGS because that output still refers to each library as -L/path/to -lfoo (as this is the correct syntax when --static is passed to the linker, which we are not going to do in macOS). So, the output of pkg-config should be rewritten as /path/to/libfoo.a (using a similar library path search algorithm that the linker does).\nSource: https://github.com/golang/go/issues/26492#issuecomment-525527016\nYes, I agree with Volker\u0026rsquo;s comment that some systems don\u0026rsquo;t really allow static binaries.\nSource: https://stackoverflow.com/a/61324538/16317008\nNote that fully static builds are not allowed/supported by Apple doesn\u0026rsquo;t mean we cannot create statically linked binaries on Mac, we can build the binary executable on other platforms, linux, for example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { srv := http.NewServeMux() srv.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { _, _ = fmt.Fprintln(w, \u0026#34;hello world\u0026#34;) }) fmt.Println(\u0026#34;running...\u0026#34;) _ = http.ListenAndServe(\u0026#34;:8080\u0026#34;, srv) } Build on MacOS machine with some flags:\n1 2 3 4 $ GOOS=linux go build -o server main.go $ file server server: ELF 64-bit LSB executable, ARM aarch64, version 1 (SYSV), statically linked, Go BuildID=qAraNfnU-cYn-2KsoFx7/rrJYFJTR911CeHr08Y4E/uoFKsYV1LH9as_7QdMc7/fJ71_ARZOiNg7b4tLPIt, with debug_info, not stripped As you can see, even we use the net package which uses cgo, it still can be statically with GOOS=linux flag, this is called cross compilation, learn more: Cross Compilation - Go - David\u0026rsquo;s Blog\nBut the arch is arm64, not amd64, if you want build binary gonna runs on amd64, you should add GOARCH=amd64 :\n1 $ GOOS=linux GOARCH=amd64 go build -o server main.go # 3. \u0026quot;-extldflags=-static\u0026quot; In previous part, we know that CGO_ENABLED=0 will disable cgo, if we use net and we want statically linking, it\u0026rsquo;s fine we just pass CGO_ENABLED=0 to go build, then we will use pure go implementation of net. But what if we use a third party package that only implemented by cgo, mattn/go-sqlite3 for example, and we want make it linked statically, apparently we can\u0026rsquo;t use CGO_ENABLED=0 to disable cgo.\nA simle cgo:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package main // typedef int (*intFunc) (); // // int // bridge_int_func(intFunc f) // { //\treturn f(); // } // // int fortytwo() // { //\treturn 42; // } import \u0026#34;C\u0026#34; import \u0026#34;fmt\u0026#34; func main() { f := C.intFunc(C.fortytwo) fmt.Println(int(C.bridge_int_func(f))) // Output: 42 } Then compile it with CGO_ENABLED=0 on Ubuntu machine:\n1 2 3 4 5 6 7 8 9 $ CGO_ENABLED=0 go build -o server main.go go: no Go source files # ubuntu @ ip-172-31-12-228 in ~/codes [20:58:50] C:1 $ go build -o server main.go # ubuntu @ ip-172-31-12-228 in ~/codes [21:00:15] $ file server server: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=da0674602632e4f540cfe58f0be1ffa261f2eefe, for GNU/Linux 3.2.0, with debug_info, not stripped Then we can use -ldflags to tell the C linker to statically link with -extldflags:\n1 -ldflags=\u0026#34;-extldflags=-static\u0026#34; 1 2 3 4 5 6 # ubuntu @ ip-172-31-12-228 in ~/codes [21:00:18] $ go build -ldflags=\u0026#34;-extldflags=-static\u0026#34; -o server main.go # ubuntu @ ip-172-31-12-228 in ~/codes [21:01:41] $ file server server: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, BuildID[sha1]=4031763b8f09ffcd1455840afe89c4644eca0088, for GNU/Linux 3.2.0, with debug_info, not stripped Now it\u0026rsquo;s statically linked.\nI hope you can understand the difference between CGO_ENABLED=0 and -ldflags=\u0026quot;-extldflags=-static\u0026quot; flag, and when you should use which one. Besides, you should know the different behavior on MacOS and Linux for statically linking in Go. And the two common command ldd, otool, file will help you.\nLearn more:\nStatically compiling Go programs #!bash blog/2014/04/linking-golang-statically/ cgo | Dave Cheney Matt Turner - Statically Linking Go in 2022 ","date":"2023-10-10T12:09:35Z","permalink":"https://blog.yorforger.cc/p/static-linking-go-programs/","title":"Static Linking Go Programs"},{"content":" # 1. What is cross compilation? Cross-compilation is the process of compiling code for runing on a different OS.\n# 2. An example There is a question on Stackoverflow:\nIt\u0026rsquo;s said that Golang is the compiled language, but what does it mean by compiled? If golang application is compiled to machine code, why can\u0026rsquo;t I just distribute the binary (of course on corresponding arch and platform) instead of go install stuff?\nOnce you compile a binary you can distribute it onto machines with the same architecture. You don\u0026rsquo;t need go real time envorionment such as go install, go run, etc, which are just necessary for compilation.\nThe go code:\n1 2 3 4 5 6 7 8 func main() { srv := http.NewServeMux() srv.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { _, _ = fmt.Fprintln(w, \u0026#34;hello world\u0026#34;) }) fmt.Println(\u0026#34;running...\u0026#34;) _ = http.ListenAndServe(\u0026#34;:8080\u0026#34;, srv) } Compile go program on my local machine MacOS:\n1 2 3 4 5 $ go build -o server main.go $ file server server: Mach-O 64-bit executable arm64 $ uname -m arm64 Then copy this file to my Ubuntu server and try to run it:\n1 2 3 4 5 6 7 8 9 10 11 # ubuntu @ ip-172-31-12-228 in ~ [13:38:04] $ uname -m x86_64 # ubuntu @ ip-172-31-12-228 in ~ [13:38:09] $ file server server: Mach-O 64-bit arm64 executable, flags:\u0026lt;|DYLDLINK|PIE\u0026gt; # ubuntu @ ip-172-31-12-228 in ~ [13:38:17] $ ./server zsh: exec format error: ./serve Then I tried add GOOS=linux when compile it on my Mac, and it works:\n1 $ GOOS=linux GOARCH=amd64 go build -o server main.go Run on my Ubuntu server:\n1 2 3 4 5 6 # Note the output \u0026#39;statically linked\u0026#39; $ file server server: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, Go BuildID=UPJ7jqIdP4OxbRX0df1Y/Xoh0f7edlCBhoN_dKDuV/xIAf8LzXqSyYE4r7v3Rq/Jq_3l1_5WQhsiIkApqgj, with debug_info, not stripped $ ./server running... So once you compile a binary you can distribute it onto machines with the same architecture. You don\u0026rsquo;t need go real time envorionment to run it. This is the power of compiled language.\n# 3. Cross-compilation I found a post, share it here:\nThe Bash shell and the Python interpreter are available on most Linux servers of any architecture. Hence, everything had worked well before.\nHowever, now I was dealing with a compiled language, Go, which produces an executable binary. The compiled binary consists of opcodes or assembly instructions that are tied to a specific architecture. That\u0026rsquo;s why I got the format error. Since the Arm64 CPU (where I ran the binary) could not interpret the binary\u0026rsquo;s x86-64 instructions, it errored out. Previously, the shell and Python interpreter took care of the underlying opcodes or architecture-specific instructions for me.\nI checked the Golang docs and discovered that to produce an Arm64 binary, all I had to do was set two environment variables when compiling the Go program before running the go build command.\nGOOS refers to the operating system (Linux, Windows, BSD, etc.), while GOARCH refers to the architecture to build for.\n1 $ env GOOS=linux GOARCH=arm64 go build -o prepnode_arm64 # 3.1. What about other architectures? x86 and Arm are two of the five architectures I test software on. I was worried that Go might not support the other ones, but that was not the case. You can find out which architectures Go supports with:\n1 2 3 4 5 6 7 8 9 10 11 $ go tool dist list aix/ppc64 android/386 android/amd64 android/arm android/arm64 darwin/amd64 darwin/arm64 windows/386 windows/amd64 ..... Generatiing binaries for all of the architectures under my test is as simple as writing a tiny shell script from my x86 laptop:\n1 2 3 4 5 6 7 #!/usr/bin/bash archs=(amd64 arm64 ppc64le ppc64 s390x) for arch in ${archs} do env GOOS=linux GOARCH=${arch} go build -o prepnode_${arch} done ","date":"2023-10-10T11:09:35Z","permalink":"https://blog.yorforger.cc/p/cross-compilation-go/","title":"Cross Compilation - Go"},{"content":" # 1. Dockerfile example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 FROM golang:alpine WORKDIR /app COPY ./ ./ RUN go mod download RUN go build -o server ENTRYPOINT [\u0026#34;./server\u0026#34;] CMD [\u0026#34;-p\u0026#34;, \u0026#34;80\u0026#34;] # docker build [--platform linux/amd64] -t shwezhu/file-server:v1.0 . # [docker push shwezhu/file-server:v1.0] # [docker pull davidzhu/file-server:v1.0] # sudo docker run -d -p 80:80 --rm shwezhu/file-server:v1.0 # 2. Docker commands 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # check the output of the program running in the container $ docker logs container_name # volumes, mount the folder on the machine \u0026#39;~/root\u0026#39; to the \u0026#39;/app/root\u0026#39; dir of the container $ docker run --name file-server --rm -d -p 80:80 -v ~/root:/app/root shwezhu/file-server:v1.0 ./server -p 80 # file-server is the container name, not image $ docker exec -it file-server bash $ docker exec -it file-server /bin/sh #------------- show -------------- $ docker container ls -a $ docker image ls -a #-------------- build and delete -------------- $ docker build -t davidzhu/go-learning:v1.0 . $ docker rmi shwezhu/file-station:v1\t# if docker is in-use, delete with -f $ docker rm container_id #------------------ run image ---------------------- # --rm automatically removes the container when it exits, # -d: Run container in background $ docker run -d -p 80:80 --rm davidzhu/go-learning:v1.0 # specify the container name $ docker run --name mysql-volume ... # publish and pull image from repo $ docker push shwezhu/file-station:v1 $ docker pull davidzhu/file-station:v1 # 3. Build image The docker build command is used to build a Docker image from a Dockerfile and a \u0026ldquo;context\u0026rdquo;.\nDuring the build, those files (build context) are sent to the Docker daemon so that the image can use them as files. The build context is usually at the current folder.\n1 2 3 # -t image_name:tag_name # . specifies the build context as the current directory $ docker build -t go-learning:1 . Learn more: Multi-stage builds | Docker Docs\n# 4. .dockerignore File During the build, those files (build context) are sent to the Docker daemon so that the image can use them as files.\nWhen you build a Docker image, Docker takes all the files and directories in the build context and sends them to the Docker daemon, which then processes and includes them in the image. By using a .dockerignore file, you can specify patterns of files and directories that should be ignored during the build process.\nBy excluding unnecessary files and directories, you can significantly reduce the size of the Docker image. This is particularly important when building images for production environments or when transferring images across networks.\n# 5. Image vs container A container is an isolated place where an application runs without affecting the rest of the system and without the system impacting the application. Because they are isolated, containers are well-suited for securely running software like databases or web applications that need access to sensitive resources without giving access to every user on the system. However, containers can be much more efficient than virtual machines because they don’t need the overhead of an entire operating system. They share a single kernel with other containers and boot in seconds instead of minutes.\nImages are read-only templates containing instructions for creating a container. A Docker image creates containers to run on the Docker platform. Docker images are immutable, so you cannot change them once they are created. If you need to change something, create another container with your changes, then save those as another image. Or, just run your new container using an existing image as a base and change that one.\nAn image is composed of multiple stacked layers, like layers in a photo editor, each changing something in the environment. Images contain the code or binary, runtimes, dependencies, and other filesystem objects to run an application. The image relies on the host operating system (OS) kernel. For example, to build a web server image, start with an image that includes Ubuntu Linux (a base OS). Then, add packages like Apache and PHP on top.\nThink of a Docker container as a running image instance. You can create many containers from the same image, each with its own unique data and state. Although images are not the only way to create containers, they are a common method.\nSource: Docker image vs container: What are the differences? | CircleCI\n# 6. Image with cross platform Docker images are typically built for a specific CPU architecture, such as x86-64 (64-bit Intel/AMD processors). By default, Docker images are built for the architecture of the system where the image is built. However, it is possible to build and run Docker images for different architectures using a technique called multi-architecture or cross-platform support.\nDocker images can support multiple platforms, which means that a single image may contain variants for different architectures, and sometimes for different operating systems, such as Windows. When you run an image with multi-platform support, Docker automatically selects the image that matches your OS and architecture.\nMost of the Docker Official Images on Docker Hub provide a variety of architecturesopen_in_new. For example, the busybox image supports amd64, arm32v5, arm32v6, arm32v7, arm64v8, i386, ppc64le, and s390x. When running this image on an x86_64 / amd64 machine, the amd64 variant is pulled and run.\nLearn more: Multi-platform images | Docker Docs\n","date":"2023-10-09T21:09:35Z","permalink":"https://blog.yorforger.cc/p/docker-basics/","title":"Docker Basics"},{"content":" # 1. ufw command # 1.1. Commonly used ufw commands 1 2 3 4 5 6 7 # ubuntu @ ip-172-31-12-228 $ sudo ufw status Status: inactive # ubuntu @ ip-172-31-12-228 $ sudo ufw enable Command may disrupt existing ssh connections. Proceed with operation (y|n)? n Aborted Note: use sudo ufw enable carefully, because it may disrupt your ssh connection.\n1 2 3 4 5 6 7 8 9 10 ufw enable: Enables the firewall, which starts enforcing the configured rules. ufw disable: Disables the firewall, allowing all network traffic. ufw status: Displays the current status of the firewall and the rules that are in effect. ufw default allow: Sets the default policy to allow all incoming and outgoing traffic. ufw default deny: Sets the default policy to deny all incoming and outgoing traffic. ufw allow \u0026lt;port\u0026gt;: Opens a specific port for incoming traffic. ufw deny \u0026lt;port\u0026gt;: Blocks incoming traffic on a specific port. ufw delete \u0026lt;rule\u0026gt;: Deletes a specific rule from the firewall. # e.g., sudo ufw delete allow 80 # 1.2. ufw app list 1 2 3 4 5 6 7 # ubuntu @ ip-172-31-12-228 $ sudo ufw app list Available applications: Nginx Full Nginx HTTP Nginx HTTPS OpenSSH The output of sudo ufw app list only shows the available application profiles, not their actual status or whether they have been allowed or denied by ufw. It simply provides a list of predefined profiles so that you can use conveniently when configuring firewall rules. You don\u0026rsquo;t need to remember which port for each application, you can simply use like this:\n1 2 3 4 5 6 # sudo ufw allow \u0026#39;Profile Name\u0026#39; $ sudo ufw allow \u0026#39;OpenSSH\u0026#39; Rules updated Rules updated (v6) $ sudo ufw allow \u0026#39;OpenS\u0026#39; ERROR: Could not find a profile matching \u0026#39;OpenS\u0026#39; rather than\n1 sudo ufw allow 22 So the command below will allow both port 80 and 443:\n1 sudo ufw allow \u0026#39;Nginx Full\u0026#39; To check the status of your firewall rules and verify whether Nginx HTTP, Nginx HTTPS, or OpenSSH have been allowed or denied by ufw, you can use the sudo ufw status command. This command will display the current status of the firewall and the active rules.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ sudo ufw allow \u0026#39;OpenSSH\u0026#39; Rules updated Rules updated (v6) $ sudo ufw enable Command may disrupt existing ssh connections. Proceed with operation (y|n)? y Firewall is active and enabled on system startup $ sudo ufw status Status: active To Action From -- ------ ---- 5060/udp ALLOW Anywhere OpenSSH ALLOW Anywhere 5060/udp (v6) ALLOW Anywhere (v6) OpenSSH (v6) ALLOW Anywhere (v6) $ sudo ufw disable Firewall stopped and disabled on system startup # 2. Ubuntu Firewall (ufw) vs AWS Security Groups A firewall like UFW is running at the OS level, while Amazon Security Groups are running at the instance level. Traffic coming into the EC2 would first pass through the SG, and then be evaluated by UFW.\nI strongly recommend you to use only \u0026ldquo;SG(Security Group)\u0026rdquo; on EC2 even though we can use both \u0026ldquo;SG\u0026rdquo; and \u0026ldquo;UFW. \u0026ldquo;SG\u0026rdquo; is a firewall same as \u0026ldquo;UFW\u0026rdquo;.\nWhen only \u0026ldquo;SG\u0026rdquo; allowed \u0026ldquo;SSH 22\u0026rdquo; and \u0026ldquo;UFW\u0026rdquo; didn\u0026rsquo;t allow \u0026ldquo;SSH 22\u0026rdquo; then I logged out ubuntu, I couldn\u0026rsquo;t log in to ubuntu forever, then I terminated ubuntu.\nEven though \u0026ldquo;SG\u0026rdquo; allowed \u0026ldquo;SSH 22\u0026rdquo;, I couldn\u0026rsquo;t log in to ubuntu because \u0026ldquo;UFW\u0026rdquo; didn\u0026rsquo;t allow \u0026ldquo;SSH 22\u0026rdquo;. So if either of them doesn\u0026rsquo;t allow \u0026ldquo;SSH 22\u0026rdquo;, \u0026ldquo;SSH 22\u0026rdquo; doesn\u0026rsquo;t work. If both \u0026ldquo;SG\u0026rdquo; and \u0026ldquo;UFW\u0026rdquo; allow \u0026ldquo;SSH 22\u0026rdquo;, \u0026ldquo;SSH 22\u0026rdquo; works, then we can log in to ubuntu.\nI also experimented with \u0026ldquo;HTTP 80\u0026rdquo;. When only \u0026ldquo;SG\u0026rdquo; allowed \u0026ldquo;HTTP 80\u0026rdquo; and \u0026ldquo;UFW\u0026rdquo; didn\u0026rsquo;t allow \u0026ldquo;HTTP 80\u0026rdquo;, \u0026ldquo;HTTP 80\u0026rdquo; didn\u0026rsquo;t work. When \u0026ldquo;SG\u0026rdquo; and \u0026ldquo;UFW\u0026rdquo; allowed \u0026ldquo;HTTP 80\u0026rdquo;, \u0026ldquo;HTTP 80\u0026rdquo; worked.\nJust remember like \u0026ldquo;If both allow, it works\u0026rdquo; and \u0026ldquo;If only either of them allow, it doesn\u0026rsquo;t work\u0026rdquo;. Actually, using both of them makes complication and some trobles. So again, I really recommend you to use only \u0026ldquo;SG\u0026rdquo; on EC2 which is simpler than using both of them.\nSource: https://stackoverflow.com/questions/57436758/does-ubuntu-ufw-overrides-amazon-ec2s-security-groups-and-rules\n","date":"2023-10-09T13:53:57Z","permalink":"https://blog.yorforger.cc/p/ufw-vs-aws-security-group/","title":"ufw vs  AWS Security Group"},{"content":" OpenSSL is an all-around cryptography library that offers an open-source application of the TLS protocol. It allows users to perform various SSL-related tasks, including CSR (Certificate Signing Request) and private keys generation, and SSL certificate installation. You can use OpenSSL\u0026rsquo;s commands to generate, install and manage SSL certificates on various servers. What Is OpenSSL and How Does It Work?\nOpenSSL is a library, so there should be API interfaces. However, the more common use of OpenSSL is as a command-line tool, which can be entered on a computer\n1 2 $ openssl version LibreSSL 3.3.6 Why the output is LibreSSL? LibreSSL is another version of SSL implementation, which exists alongside OpenSSL. Learn more: Why You Should Use LibreSSL Instead of OpenSSL\nOpenSSL is indeed a library, and HTTPS is based on SSL. When establishing a secure connection using SSL, operations such as SSL handshake and identity verification are required. For example, we write a program to make an HTTPS request to a server, do TLS handshake and verifying digital signatures on our own would be challenging. This is where OpenSSL comes in—it handles these complex operations.\nIt\u0026rsquo;s possible that some people may have noticed that when sending HTTP requests using Python, OpenSSL was not explicitly used. There are two reasons for this:\nIt could be because you were sending an HTTP request instead of an HTTPS request. You can check if the destination port is set to 80, which is the default port for HTTP.\nThe HTTP library you used in Python may have called the OpenSSL library at its lower levels to handle the complex operations on your behalf. However, you might not have noticed this because it happens behind the scenes. For example, the OpenAI package you mentioned that you recently used internally utilizes OpenSSL, but you wouldn\u0026rsquo;t be able to detect it without examining the source code:\n1 2 3 4 5 6 7 8 9 10 11 import openai openai.api_key = \u0026#34;Your Key\u0026#34; # make https request here response = openai.ChatCompletion.create( model=\u0026#34;gpt-3.5-turbo\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Who won the 2018 FIFA world cup?\u0026#34;} ] ) print(response[\u0026#39;choices\u0026#39;][0][\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;]) When I run the code above, there is an error:\n1 ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the \u0026#39;ssl\u0026#39; module is compiled with LibreSSL 2.8.3. See: https://github.com/urllib3/urllib3/issues/2168 openai package utilizes a Python package called urllib3, which is used for sending HTTP requests. urllib3 relies on a built-in Python library called ssl, which internally calls OpenSSL\u0026rsquo;s related interfaces for tasks such as session creation and certificate verification.\nHowever, from your Python code\u0026rsquo;s perspective, you\u0026rsquo;re just using the ssl module\u0026rsquo;s high-level API. You don\u0026rsquo;t have to interact directly with OpenSSL - it\u0026rsquo;s an implementation detail hidden by the SSLContext and socket wrapping methods. So in short, the Python SSL library uses the OpenSSL library under the hood to actually perform SSL handshakes, key generation, encryption, etc. But from a Python programmer\u0026rsquo;s point of view, you just import ssl and call its APIs to establish encrypted SSL connections.\nSo, the error message above is stating that urllib3 version 2.0 only supports OpenSSL 1.1.1+ but the SSL library used on my computer is LibreSSL 2.8.3. (Remember, openssl version earlier, the output is LibreSSL) Since urllib3 version 2.0 does not support the LibreSSL on your computer, the solution would be to switch to a different version of urllib3.\n1 2 pip3 uninstall urllib3 pip3 install \u0026#39;urllib3\u0026lt;2.0\u0026#39; Alternatively, I could consider upgrading or changing the version of LibreSSL on my computer to a version that is compatible with urllib3 2.0:\n1 brew install openssl@1.1 ","date":"2023-10-07T12:01:25Z","permalink":"https://blog.yorforger.cc/p/urllib3-not-compatible-with-openssl-issue-python/","title":"urllib3 not compatible with openssl issue - python"},{"content":" # 1. HTTP vs HTTPS Strictly speaking, HTTPS is not a separate protocol, but refers to use of ordinary HTTP over an encrypted SSL/TLS connection.\nPort 80 is typically used for unencrypted HTTP traffic while port 443 is the common port used for encrypted HTTP traffic which is called HTTPS.\nNote that TLS is the successor of SSL, you can simply think they are same thing.\nSource: https://en.wikipedia.org/wiki/HTTPS#Network_layers\n# 2. What is TLS/SSL SSL (Secure Sockets Layer) and its successor, TLS (Transport Layer Security), are protocols for establishing authenticated and encrypted links between networked computers.\nHTTPS, HTTP, and TLS are all protocols. HTTPS utilizes the encryption and digital authentication provided by SSL/TLS, while SSL/TLS utilizes some cryptographic algorithms within the protocol in different phases, such as RSA is used at session key exchange stage, AES is used during data transfer. Encryption can be further divided into two types:\nSymmetric Encryption Algorithms: AES, etc.\nAsymmetric Encryption Algorithms (public key cryptography): RSA, ECC, etc.\n# 3. The process of establishing a HTTPS connection When we click a link on our browser will send a or multiple http requets to the target server, then the server will responds us with html file or some images or other resources. But transfer data there are other things needed to do under the hood:\nA tcp connection needed to be established (envolves three way handshake). Make a TLS handshake After TLS handshake, the secure communication begins (client makes http request, server makes response). During the TLS handshake, the client generates a session key and encrypts it with the public key of the server and then send the encrypted session key string to the server, then the server decrypt this string to get the actual session key. Then they make communication with this session key. Now you should understand why I say TLS/SSL use both RSA and AES encryption algorithms at different phrases in previous part.\nNote that SSL/TLS is a stateful protocol, whereas HTTP/HTTPS is a stateless protocol.\nTLS/SSL is stateful. The web server and the client (browser) cache the session including the cryptographic keys to improve performance and do not perform key exchange for every request. Source\n# 4. Details in TLS handshake - avoid man-in-middle attack I have talked man-in-middle attack in other post, when a ssh connection is being established at the first time, it will notify us the fingerprint of the server which enables us can make sure to we are connecting the right server. But it\u0026rsquo;s a little diffenent in SSL/TLS (HTTPS). The authenciation happens in the TLS handshake, the authenciation here means to prevent man-in-the-middle attack by verifying the identity of the remote server.\nOnce the client and server have agreed to use TLS, they negotiate a stateful connection by using a handshaking procedure (see TLS handshake). The protocols use a handshake with an asymmetric cipher to establish not only cipher settings but also a session-specific shared key with which further communication is encrypted using a symmetric cipher. During this handshake, the client and server agree on various parameters used to establish the connection\u0026rsquo;s security:\nThe handshake begins when a client connects to a TLS-enabled server requesting a secure connection and the client presents a list of supported cipher suites (ciphers and hash functions). From this list, the server picks a cipher and hash function that it also supports and notifies the client of the decision. The server usually then provides identification in the form of a digital certificate. The certificate contains the server name, the trusted certificate authority (CA) that vouches for the authenticity of the certificate, and the server\u0026rsquo;s public encryption key. (The digital certificate here is know as SSL/TLS certificate) The client confirms the validity of the certificate before proceeding. (The client verifies the identity of the remote server by check the digital certificate which is called SSL/TLS certificate here) To generate the session keys used for the secure connection, the client either: encrypts a random number (PreMasterSecret) with the server\u0026rsquo;s public key and sends the result to the server (which only the server should be able to decrypt with its private key); both parties then use the random number to generate a unique session key for subsequent encryption and decryption of data during the session, or uses Diffie–Hellman key exchange (or its variant elliptic-curve DH) to securely generate a random and unique session key for encryption and decryption that has the additional property of forward secrecy: if the server\u0026rsquo;s private key is disclosed in future, it cannot be used to decrypt the current session, even if the session is intercepted and recorded by a third party. This concludes(ends) the handshake and begins the secured connection, which is encrypted and decrypted with the session key until the connection closes. If any one of the above steps fails, then the TLS handshake fails and the connection is not created.\nSource: https://en.wikipedia.org/wiki/Transport_Layer_Security\nLearn more: https://www.cloudflare.com/learning/ssl/what-happens-in-a-tls-handshake/\n# 5. Two ways to get SSL/TLS certificate There are several ways to obtain an SSL/TLS certificate:\nPurchase from a Certificate Authority (CA): Trusted CAs offer various types of certificates, such as domain validation (DV), organization validation (OV), and extended validation (EV). A CA is an outside organization, a trusted third party, that generates and gives out SSL certificates. The CA will also digitally sign the certificate with their own private key, allowing client devices to verify it. Once the certificate is issued, it needs to be installed and activated on the website\u0026rsquo;s origin server.\nTechnically, anyone can create their own SSL certificate by generating a public-private key pairing and including all the information mentioned above . Such certificates are called self-signed certificates because the digital signature used, instead of being from a CA, would be the website\u0026rsquo;s own private key. While self-signed certificates provide encryption for your website or application, they are not trusted by default by web browsers or other client applications. Therefore, visitors accessing your site will typically see a warning message stating that the certificate is not trusted. Learn more: How to generate a self-signed SSL certificate using OpenSSL?\n# 6. Is HTTPS secure enough? Does an established HTTPS connection mean the line is really secure?\nIt\u0026rsquo;s important to understand what SSL does and does not do, especially since this is a very common source of misunderstanding.\nIt encrypts the channel It applies integrity checking It provides authentication So, the quick answer should be: \u0026ldquo;yes, it is secure enough to transmit sensitive data\u0026rdquo;. However, things are not that simple. There are a few issues here, the major one being authentication. Both ends need to be sure they are talking to the right person or institution and no man-in-the-middle attack or CSRF attacks.\nHTTPS is secure in encryption. HTTPS is secure itself but if we can totally trust HTTPS connection when exhcange privacy data is another thing. Although no one can decrept the data without the session key, there probably have man-in-the-middle attck or CSRF attck needs to be considered which make the hackers get your money without getting your sensitive data . If you can make sure the client is really that people you want talk as a server or you can make sure the server is the correct server you want to get, then https is safe. Can you make sure the server itself is a bad company? Which will sell your personal data to other perople. But this is another topic, haha, In the last I\u0026rsquo;ll share a answer here which is very comprehensive:\nQuestion: Consider a scenario, where user authentication (username and password) is entered by the user in the page\u0026rsquo;s form element, which is then submitted. The POST data is sent via HTTPS to a new page (where the php code will check for the credentials). If a hacker sits in the network, and say has access to all the traffic, is the Application layer security (HTTPS) enough in this case ?\nAnswer 1: Yes. In an HTTPS only the handshake is done unencrypted, but even the HTTP GET/POST query\u0026rsquo;s are done encrypted.\nIt is however impossible to hide to what server you are connecting, since he can see your packets he can see the IP address to where your packets go. If you want to hide this too you can use a proxy (though the hacker would know that you are sending to a proxy, but not where your packets go afterwards).\nAnswer 2: HTTPS is sufficient \u0026ldquo;if\u0026rdquo; the client is secure. Otherwise someone can install a custom certificate and play man-in-the-middle.\nReferences:\nDoes an established HTTPS connection mean a line is really secure? - Information Security Stack Exchange php - POST data encryption - Is HTTPS enough? - Stack Overflow ","date":"2023-10-07T08:30:26Z","permalink":"https://blog.yorforger.cc/p/https-ssl-tls/","title":"HTTPS SSL TLS"},{"content":" # 1. CORS issue My frontend application is deployed on http://localhost:5173, and backend application is deployed on http://localhost:8080. The frontend application sends HTTP requests to the backend application through fetch, but it fails with the following error:\n1 2 3 4 # 显然没有服务器处理 OPTIONS 请求 Access to fetch at \u0026#39;http://localhost:8080/\u0026#39; from origin \u0026#39;http://localhost:5173\u0026#39; has been blocked by CORS policy: Response to preflight request doesn\u0026#39;t pass access control check: It does not have HTTP ok status. Access to fetch at \u0026#39;http://localhost:8080/api/chat\u0026#39; from origin \u0026#39;http://localhost:5173\u0026#39; has been blocked by CORS policy: Request header field content-type is not allowed by Access-Control-Allow-Headers in preflight response. This is the famous CORS issue, and it is caused by the same-origin policy (SOP) enforced by web browsers.\nCORS is a feature built into browsers for added security. It prevents any random website from using your authenticated cookies to send an API request to your bank\u0026rsquo;s website and do stuff like secretly withdraw money.\n想象一下, 如果你点进了一个恶意网站, 这个网站有个JS脚本使用比如 fetch 向你银行 /api/transfers 发送了一个请求, 如果浏览器没有 same-origin policy 限制, 那么这个请求就会被发送出去, 而且会自动带上你的银行网站的 cookie (一些请求头), 这样恶意网站就可以做一些你不知道的事情, 比如转账, 提现等等.\n但是, 有了 same-origin policy 限制, 那么这个请求就不会被浏览器发送出去, 因为这个请求的 origin 和你银行网站的 origin 不一样, 所以浏览器会阻止这个请求. Origin 是浏览器自动添加的请求头, 你不能修改它. Origin 包括三部分: 协议, 域名, 端口. 上面的例子 origin 就是 http://localhost:5173 和 http://localhost:8080, 前端页面的 js 代码是在 http://localhost:5173 运行的, 所以它的 origin 是 http://localhost:5173, 而后端的 API 是在 http://localhost:8080 运行的, 所以它的 origin 是 http://localhost:8080.\n所以理论上来说, 如果你的后端服务器不允许CORS, 那么除了跟你后端服务器同源(Origin)的前端页面, 其他的前端页面都不能在浏览器访问你的后端 API. 当然你可以在终端使用 curl 命令访问你的后端 API, 因为 curl 命令不是浏览器, 它不会自动添加 origin 请求头. 另外 React 的 create-react-app 也有一个 proxy 功能, 可以让你在开发环境下绕过 CORS 限制, 但是这个代理功能只在开发环境下有效, 生产环境下还是要你自己配置后端服务器的 CORS.\n# 2. How to fix CORS issue # 2.1. 简单场景 解决的办法很简单, 在后端 API 的响应头里添加 Access-Control-Allow-Origin: * 就可以了. 但这仅限于一些简单的场景, 如 GET 请求. 可参考: Golang CORS Guide: What It Is and How to Enable It\n# 2.2. 复杂场景 如果你的 API 是 POST 请求, 并且需要带上一些请求头, 那么你可能还需要在响应头里添加 Access-Control-Allow-Headers: *, 这样才能让浏览器发送 POST 请求, 并且带上你需要的请求头. (具体规定可参考: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) 这也是为什么在有的前端JS fetch代码里, 你后端简单设置 Access-Control-Allow-Origin: * 就可以了, 但是有的并不会成功.\n除了设置上面两个响应头, 你还需要处理 OPTIONS 请求, 这是因为浏览器在发送跨域请求时, 会先发送一个 OPTIONS 请求, 用来询问服务器是否允许跨域请求, 如果服务器不允许, 那么浏览器就不会发送真正的请求.\n比如我的后端 API 需要客户端带上一个请求头 Content-Type: application/json 和 Content-Type: xxxx, 那么我就需要在响应头里添加 Access-Control-Allow-Headers: Content-Type, Content-Type, 或者 Access-Control-Allow-Headers: *, 这样浏览器发送HTTP请求时才会带上这两个请求头.\n使用 Golang 处理的话, 大致逻辑如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func (s *Server) ServeHTTP(w http.ResponseWriter, r *http.Request) { enableCors(\u0026amp;w) // Handle CORS preflighted request sent by browser. if (*r).Method == \u0026#34;OPTIONS\u0026#34; { return } // 真正的处理逻辑 s.mux.ServeHTTP(w, r) } func enableCors(w *http.ResponseWriter) { (*w).Header().Set(\u0026#34;Access-Control-Allow-Origin\u0026#34;, \u0026#34;*\u0026#34;) (*w).Header().Set(\u0026#34;Access-Control-Allow-Methods\u0026#34;, \u0026#34;POST, GET, OPTIONS\u0026#34;) // We need to allow the Authorization header to be sent to the backend. (*w).Header().Set(\u0026#34;Access-Control-Allow-Headers\u0026#34;, \u0026#34;*\u0026#34;) (*w).Header().Set(\u0026#34;Access-Control-Max-Age\u0026#34;, \u0026#34;86400\u0026#34;) } # 3. CORS vs SOP The Same-Origin Policy (SOP) is a security feature enforced by web browsers that restricts web pages (javascript) from interacting with resources (such as making requests or accessing data) from different origins.\nCORS allows servers to specify which origins are allowed to access their resources, even if they are from different origins. It provides a set of HTTP headers that the server includes in its responses to explicitly permit cross-origin requests from specific origins.\nReferences:\nCross-Origin Resource Sharing (CORS) - HTTP | MDN\nSame-origin policy - Web security | MDN\n","date":"2023-10-06T20:28:50Z","permalink":"https://blog.yorforger.cc/p/cross-origin-request-http/","title":"Cross-origin Request HTTP"},{"content":"Why would you get an error from Close() but not an earlier Write() call? To answer that we need to take a brief, high-level detour into the area of computer architecture.\nGenerally speaking, as you move outside and away from your CPU, actions get orders of magnitude slower. Writing to a CPU register is very fast. Accessing system RAM is quite slow in comparison. Doing I/O on disks or networks is an eternity.\nIf every Write() call committed the data to the disk synchronously, the performance of our systems would be unusably slow. While synchronous writes are very important for certain types of software (like databases), most of the time it’s overkill.\nThe pathological case is writing to a file one byte at a time. Hard drives – brutish, mechanical devices – need to physically move a magnetic head to the position on the platter and possibly wait for a full platter revolution before the data could be persisted. SSDs, which store data in blocks and have a finite number of write cycles for each block, would quickly burn out as blocks are repeatedly written and overwritten.\nFortunately this doesn’t happen because multiple layers within hardware and software implement caching and write buffering. When you call Write(), your data is not immediately being written to media. The operating system, storage controllers and the media itself are all buffering the data in order to batch smaller writes together, organizing the data optimally for storage on the medium, and deciding when best to commit it. This turns our writes from slow, blocking synchronous operations to quick, asynchronous operations that don’t directly touch the much slower I/O device. Writing a byte at a time is never the most efficient thing to do, but at least we are not wearing out our hardware if we do it.\nOf course, the bytes do have to be committed to disk at some point. The operating system knows that when we close a file, we are finished with it and no subsequent write operations are going to happen. It also knows that closing the file is its last chance to tell us something went wrong.\nOn POSIX systems like Linux and macOS, closing a file is handled by the close system call. The BSD man page for close(2) talks about the errors it can return:\n1 2 3 4 5 6 7 8 9 ERRORS The close() system call will fail if: [EBADF] fildes is not a valid, active file descriptor. [EINTR] Its execution was interrupted by a signal. [EIO] A previously-uncommitted write(2) encountered an input/output error. EIO is exactly the error we are worried about. It means that we’ve lost data trying to save it to disk, and our Go programs should absolutely not return a nil error in that case.\nOn Twitter, Ben Johnson suggested that Close() may be safe to run multiple times on files, like so:\n1 2 3 4 5 6 7 8 9 10 11 12 func doSomething() error { f, err := os.Create(\u0026#34;foo\u0026#34;) if err != nil { return err } defer f.Close() if _, err := f.Write([]byte(\u0026#34;bar\u0026#34;); err != nil { return err } return f.Close() } The docs for *os.File on its behavior saying: Close will return an error if it has already been called. But since we are ignoring the error from the defer, this doesn’t matter.\nSource: Don\u0026rsquo;t defer Close() on writable files – joe shaw\n","date":"2023-10-05T18:52:56Z","permalink":"https://blog.yorforger.cc/p/dont-defer-close-on-writable-files-go-notes/","title":"Don't Defer Close() on Writable Files - Go Notes"},{"content":" # 1. Index concepts Learn more: https://www.oreilly.com/library/view/high-performance-mysql/0596003064/ch04.html\n# 2. When will index be created Primary Key: By default, when you define a primary key constraint on a column or set of columns using the PRIMARY KEY keyword, MySQL automatically creates an index on that column(s).\n1 2 3 4 5 6 7 # index will created on user_id column create table `user` ( `user_id` smallint not null auto_increment, `username` varchar(20) not null, `password` varchar(20) not null, primary key (`user_id`) ); Unique Constraints: When you define a unique constraint on a column or set of columns using the UNIQUE keyword, MySQL automatically creates an index to enforce uniqueness.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # index will be created on username column create table `user` ( `user_id` smallint not null, `username` varchar(20) not null, `password` varchar(20) not null, unique (`username`) ); # there will be two indexes # index will be created on both user_id and username column create table `user` ( `user_id` smallint not null auto_increment, `username` varchar(20) not null, `password` varchar(20) not null, primary key (`user_id`), unique (`username`) ); # 3. Why use index Indexes are used to find rows with specific column values quickly. Without an index, MySQL must begin with the first row and then read through the entire table to find the relevant rows. The larger the table, the more this costs. If the table has an index for the columns in question, MySQL can quickly determine the position to seek to in the middle of the data file without having to look at all the data. This is much faster than reading every row sequentially.\nMost MySQL indexes (PRIMARY KEY, UNIQUE, INDEX, and FULLTEXT) are stored in B-trees. Exceptions: Indexes on spatial data types use R-trees; MEMORY tables also support hash indexes; InnoDB uses inverted lists for FULLTEXT indexes.\nThe best way to improve the performance of SELECT operations is to create indexes on one or more of the columns that are tested in the query. The index entries act like pointers to the table rows, allowing the query to quickly determine which rows match a condition in the WHERE clause, and retrieve the other column values for those rows. All MySQL data types can be indexed.\nAlthough it can be tempting to create an indexes for every possible column used in a query, unnecessary indexes waste space and waste time for MySQL to determine which indexes to use. Indexes also add to the cost of inserts, updates, and deletes because each index must be updated. You must find the right balance to achieve fast queries using the optimal set of indexes.\nSource: MySQL :: MySQL 8.0 Reference Manual :: 8.3 Optimization and Indexes\nLearn when index will be used in MySQL operations: https://dev.mysql.com/doc/refman/8.0/en/mysql-indexes.html\n# 4. Table types MyISAM and InnoDB are two different storage engines in MySQL, each with its own characteristics and features.\n# 4.1. InnoDB tables InnoDB tables provide B-tree indexes. The indexes provide no packing or prefix compression. In addition, InnoDB also requires a primary key for each table. As with BDB, though, if you don’t provide a primary key, MySQL will supply a 64-bit value for you.\nThe indexes are stored in the InnoDB tablespace, just like the data and data dictionary (table definitions, etc.). Furthermore, InnoDB uses clustered indexes. That is, the primary key’s value directly affects the physical location of the row as well as its corresponding index node. Because of this, lookups based on primary key in InnoDB are very fast. Once the index node is found, the relevant records are likely to already be cached in InnoDB’s buffer pool.\nWith clustered indexes, the primary key and the record itself are “clustered” together, and the records are all stored in primary-key order. InnoDB table uses clustered indexes.\n# 4.2. MyISAM tables MySQL’s default table type provides B-tree indexes, and as of Version 4.1.0, it provides R-tree indexes for spatial data.\nWith MyISAM tables, the indexes are kept in a completely separate file that contains a list of primary (and possibly secondary) keys and a value that represents the byte offset for the record. These ensure MySQL can find and then quickly skip to that point within the database to locate the record. MySQL has to store the indexes this way because the records are stored in essentially random order.\nNote that the difference between MyISAM table and InnoDB table is that the records are stored in random order, another tables\u0026rsquo;s record stored according to clustered indexe (primary key).\nWith a standard MyISAM table index, there are two lookups, one to the index, and a second to the table itself via the location specified in the index. With clustered indexes in InnoDB table, there’s a single lookup that points directly to the record in question. Therefore, when your data is almost always searched on via its primary key, clustered indexes can make lookups incredibly fast.\n# 5. Clustered vs secondary indexes Each InnoDB table has a special index called the clustered index that stores row data. Typically, the clustered index is synonymous with the primary key. To get the best performance from queries, inserts, and other database operations, it is important to understand how InnoDB uses the clustered index to optimize the common lookup and DML operations.\nWhen you define a PRIMARY KEY on a table, InnoDB uses it as the clustered index. A primary key should be defined for each table. If there is no logical unique and non-null column or set of columns to use a the primary key, add an auto-increment column. Auto-increment column values are unique and are added automatically as new rows are inserted. If you do not define a PRIMARY KEY for a table, InnoDB uses the first UNIQUE index with all key columns defined as NOT NULL as the clustered index. If a table has no PRIMARY KEY or suitable UNIQUE index, InnoDB generates a hidden clustered index named GEN_CLUST_INDEX on a synthetic column that contains row ID values. The rows are ordered by the row ID that InnoDB assigns. The row ID is a 6-byte field that increases monotonically as new rows are inserted. Thus, the rows ordered by the row ID are physically in order of insertion. Source: https://dev.mysql.com/doc/refman/8.0/en/innodb-index-types.html\nSecondary indexes point to the primary key rather than the row. Therefore, if you index on a very large value and have several secondary indexes, you will end up with many duplicate copies of that primary index, first as the clustered index stored alongside the records themselves, but then again for as many times as you have secondary indexes pointing to those clustered indexes. With a small value as the primary key, this may not be so bad, but if you are using something potentially long, such as a URL, this repeated storage of the primary key on disk may cause storage issues.\nSome operations render clustered indexes less effective. For instance, consider when a secondary index is in use. Going back to our phone book example, suppose you have last_name set as the primary index and phone_number set as a secondary index, and you perform the following query:\n1 SELECT * FROM phone_book WHERE phone_number = \u0026#39;555-7271\u0026#39; MySQL scans the phone_number index to find the entry for 555-7271, which contains the primary key entry Zawodny because phone_book’s primary index is the last name. MySQL then skips to the relevant entry in the database itself.\nIn other words, lookups based on your primary key happen exceedingly fast, and lookups based on secondary indexes happen at essentially the same speed as MyISAM index lookups would.\nAnother less common but equally problematic condition happens when the data is altered such that the primary key is changed on a record. This is the most costly function of clustered indexes. A number of things can happen to make this operation a more severe performance hit:\nAlter the record in question according to the query that was issued. Determine the new primary key for that record, based on the altered data record. Relocate the stored records so that the record in question is moved to the proper location in the tablespace. Update any secondary indexes that point to that primary key. As you might imagine, if you’re altering the primary key for a number of records, that UPDATE command might take quite some time to do its job, especially on larger tables. Choose your primary keys wisely. Use values that are unlikely to change, such as a Social Security account number instead of a last name, serial number instead of a product name, and so on.\nSource: https://www.oreilly.com/library/view/high-performance-mysql/0596003064/ch04.html\n# 6. Should each table have a primary key Short answer: yes, each table should have a PRIMARY KEY so that you could distinguish two records.\nIn MySQL, the InnoDB storage engine always creates a primary key if you didn\u0026rsquo;t specify it explicitly, thus making an extra column you don\u0026rsquo;t have access to.\nLearn more: https://stackoverflow.com/a/840182/16317008\n# 7. Unique indexes versus primary keys If you’re coming from other relational databases, you might wonder what the difference between a primary key and a unique index is in MySQL. As usual, it depends. In MyISAM tables, there’s almost no difference. The only thing special about a primary key is that it can’t contain NULL values. The primary key is simply a NOT NULL UNIQUE INDEX named PRIMARY. MyISAM tables don’t require that you declare a primary key.\nInnoDB and BDB tables require primary keys for every table. There’s no requirement that you specify one, however. If you don’t, the storage engine automatically adds a hidden primary key for you. In both cases, the primary keys are simply incrementing numeric values, similar to an AUTO-INCREMENT column. If you decide to add your own primary key at a later time, simply use ALTER TABLE to add one. Both storage engines will discard their internally generated keys in favor of yours. Heap tables don’t require a primary key but will create one for you. In fact, you can create Heap tables with no indexes at all.\n# 8. Index adds places and time overhead Time overhead is easy to understand, indexes add to the cost of inserts, updates, and deletes because each index must be updated.\nFor taking up place, there is an example: try to think our book or dictionary which have index (table of contents) which helps us to find the content quickly, the table of contents usually has multiple pages, just like this, index in a table also will take up some place which makes a table bigger.\nGiven that creating an index requires additional disk space (277,778 blocks extra from the above example, a ~28% increase), and that too many indices can cause issues arising from the file systems size limits, careful thought must be used to select the correct fields to index. Learn more: https://stackoverflow.com/a/1130/16317008\nSince indices are only used to speed up the searching for a matching field within the records, it stands to reason that indexing fields used only for output would be simply a waste of disk space and processing time when doing an insert or delete operation, and thus should be avoided.\nWhat does \u0026ldquo;indexing fields used only for output\u0026rdquo; mean?\nFor example, imagine a table of products. Some fields like \u0026ldquo;name\u0026rdquo; or \u0026ldquo;price\u0026rdquo; are used for searching and filtering, while other fields like \u0026ldquo;description\u0026rdquo; or \u0026ldquo;image_url\u0026rdquo; are used only to provide additional information in the output. Creating an index on fields used only for output is not helpful and can waste resources.\nIndexes trade space for performance. Because MySQL needs to maintain a separate list of indexes’ values and keep them updated as your data changes, you really don’t want to index every column in a table. Indexes are a trade-off between space and time. You’re sacrificing some extra disk space and a bit of CPU overhead on each INSERT, UPDATE, and DELETE query to make most (if not all) your queries much faster. Source: https://www.oreilly.com/library/view/high-performance-mysql/0596003064/ch04.html\n","date":"2023-10-03T12:06:36Z","permalink":"https://blog.yorforger.cc/p/index-and-primary-key/","title":"Index and Primary Key"},{"content":" # 1. Session Session is a server-side state management technique that allows the storage and retrieval of user-specific data across multiple requests. It provides a way to maintain state for individual users during their interaction with a web application. Here are some key points about session:\nSession data is stored on the server, usually in memory or in a session store. Each user is assigned a unique session ID, typically stored as a cookie or in the URL. Session data is private to each user and cannot be accessed by other users. Session data can be accessed and modified throughout the user\u0026rsquo;s session on any page of the application. Session data is available only as long as the session is active. It expires after a certain period of inactivity or can be explicitly cleared. Sessions are commonly used to store user-specific information such as user\u0026rsquo;s authorization state, user\u0026rsquo;s privilege, user preferences, and temporary data needed during the user\u0026rsquo;s session. And recently, I\u0026rsquo;m writing a chatgpt bot which needs remember user\u0026rsquo;s chat history, this is just a temporary data which need isolated with each user and not that important, session is perfect to store it.\nLearn more: Session Management - AWS\n# 2. Caching Caching is a mechanism used to temporarily store frequently accessed or expensive-to-compute data in order to improve application performance and reduce database or server load. It involves storing data in memory or another fast-access storage location. Here are some key points about caching:\nCached data is stored on the server or in a distributed cache, such as Redis or Memcached. Cached data is typically shared among all users of an application and can be accessed across multiple requests. Cached data is often used to store static or infrequently changing data that is expensive to compute or retrieve from a database. Cached data can be set to expire after a certain period or manually invalidated to ensure fresh data is retrieved when necessary. Caching can help optimize application performance by reducing the need to fetch data from slower data sources, such as databases or external APIs. It is commonly used to cache database query results, rendered views, frequently accessed configuration settings, or other expensive computations.\nDon’t use cache to store anything you can’t regenerate. Cache can be deleted in the middle of the session, or even in the middle of a non-atomic operation. Learn more: https://www.v2ex.com/t/109856\n# 3. Additional: session management There are various ways to manage user sessions including storing those sessions locally to the node responding to the HTTP request or designating a layer in your architecture which can store those sessions in a scalable and robust manner. Common approaches used include utilizing Sticky sessions or using a Distributed Cache for your session management.\n# 3.1. Sticky Sessions with Local Session Caching Sticky sessions, also known as session affinity, allow you to route a site user to the particular web server that is managing that individual user’s session. The session’s validity can be determined by a number of methods, including a client-side cookies or via configurable duration parameters that can be set at the load balancer which routes requests to the web servers.\nSome advantages with utilizing sticky sessions are that it’s cost effective due to the fact you are storing sessions on the same web servers running your applications and that retrieval of those sessions is generally fast because it eliminates network latency. A drawback for using storing sessions on an individual node is that in the event of a failure, you are likely to lose the sessions that were resident on the failed node. In addition, in the event the number of your web servers change, for example a scale-up scenario, it’s possible that the traffic may be unequally spread across the web servers as active sessions may exist on particular servers. If not mitigated properly, this can hinder the scalability of your applications.\n# 3.2. Distributed Session Management In order to address scalability and to provide a shared data storage for sessions that can be accessible from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution to for this is to leverage an In-Memory Key/Value store such as Redis and Memcached.\nWhile Key/Value data stores are known to be extremely fast and provide sub-millisecond latency, the added network latency and added cost are the drawbacks. An added benefit of leveraging Key/Value stores is that they can also be utilized to cache any data, not just HTTP sessions, which can help boost the overall performance of your applications.\nA consideration when choosing a distributed cache for session management is determining how many nodes may be needed in order to manage the user sessions. Generally speaking, this decision can be determined by how much traffic is expected and/or how much risk is acceptable. In a distributed session cache, the sessions are divided by the number of nodes in the cache cluster. In the event of a failure, only the sessions that are stored on the failed node are affected. If reducing risk is more important than cost, adding additional nodes to further reduce the percent of stored sessions on each node may be ideal even when fewer nodes are sufficient.\nAnother consideration may be whether or not the sessions need to be replicated or not. Some key/value stores offer replication via read replicas. In the event of a node failure, the sessions would not be entirely lost. Whether replica nodes are important in your individual architecture may inform as to which key/value store should be used. ElastiCache offerings for In-Memory key/value stores include ElastiCache for Redis, which can support replication, and ElastiCache for Memcached which does not support replication.\nThere are a number of ways to store sessions in Key/Value stores. Many application frameworks provide libraries which can abstract some of the integration plumbing required to GET/SET those sessions in memory. In other cases, you can write your own session handler to persist the sessions directly.\nSource: https://aws.amazon.com/caching/session-management/\n# 4. Conclusion Sessions are used to store user-specific data across requests, while caching is used to store frequently accessed or expensive-to-compute data to improve application performance, they’re just unrelated things. Both mechanisms have their own specific use cases and can be used together to enhance the functionality and performance of application.\nSession data could be stored in many places, using a MySQL database for example might be acceptable if that is your existing backend. Or you can store session in a central cache server, such as Redis. As you can see, cache is just a way to store data, session can be cached with Redis server or cached just in loacl memory, cache and session is like the car and high way, they are not same thing. But you should note that adding a cache layer will make your application complexed:\n“The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming.” - Donald Knuth\nTherefore, premature optimization is always a disaster. It\u0026rsquo;s not too late to add caching when actual bottlenecks are discovered after deployment or users\u0026rsquo; feedback.\nReference: What is the Difference between session and caching?\n","date":"2023-10-02T09:37:58Z","permalink":"https://blog.yorforger.cc/p/caching-session-management/","title":"Caching \u0026 Session Management"},{"content":"Previous post: https://davidzhu.xyz/post/database/redis/004-session-vs-cache/\nA paragraph I like, and share it here with you:\n“The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming.” - Donald Knuth\nPremature optimization is always a disaster. It\u0026rsquo;s not too late to add caching when actual bottlenecks are discovered after deployment or users\u0026rsquo; feedback.\n# 1. What should be cached Generally you want to cache:\nMetadata/configuration data that does not change frequently. E.g. country/state lists, external resource addresses, logic/branching settings, product/price/tax definitions, etc. Data that is costly to retrieve or generate and that does not need to frequently change. E.g. historical data sets for reports. Data that is unique to the current user\u0026rsquo;s session. The last item above is where you need to be careful as you can drastically increase your app\u0026rsquo;s memory usage, by adding a few megabytes to the data for every active session. It implies different levels of caching \u0026ndash; application wide, user session, etc.\nGenerally you should NOT cache data that is under active change.\nThis is a really broad question, and the answer depends heavily on the specific application/system you are building. Caching is only a performance optimization technique, and as with any optimization, measure first before making substantial changes, to avoid wasting time optimizing the wrong thing. Maybe you don\u0026rsquo;t need much caching, and it would only complicate your app. Maybe the data you are thinking of caching can be retrieved in a faster way, or less of it can be retrieved at once.\nSource: https://stackoverflow.com/a/13519206/16317008\ne.g.,\nEach user on my server has a credit balance which needs to be changed frequently, obviously we shouldn\u0026rsquo;t cache it, instead we should fecth it from database directly, a comment from Reddit:\nIf the credit balance (which sounds financial) query is simple and quick then pulling it directly from the backend storage should be fine and would eliminate the complexities of using a caching layer and trying to keep it in sync with the persistent storage layer.\n# 2. Where cache should sit In larger systems you also need to think about where the cache(s) will sit. Is it possible to have one central cache server (Redis), or is it good enough for each server/process to handle its own caching (local memory)?\nGenerally, Redis is used for distributed caching. But sometimes local caching like Guava Cache and Caffeine can also be considered. There are some disadvantages to using local caching: it cannot perform large-scale data storage, and the cache will become invalid when the application process restarts.\nHowever, using caching brings up issues that need to be considered: how to ensure consistency between redis and database, cache penetration, cache breakdown and cache avalanche, clustering.\ne.g.,\nA app is small enough that we haven\u0026rsquo;t had to scale a single instance yet. The three caching avenues I\u0026rsquo;m looking into are:\nin-memory - use HttpContext.Session for user data caching, with sticky sessions on the server. This is the one I tried out because it\u0026rsquo;s simplest. (This belongs to the server handle its own caching above, local memory caching) Redis - Add on MemoryDB or ElastiCache in our AWS stack. (Use Redis as a central cache server) Database caching- Use a DB table to keep that user data. I\u0026rsquo;d rather avoid this, the whole point is to avoid that round trip to the DB every pageload Source: https://www.reddit.com/r/dotnet/comments/vp873j/whats_your_preferred_session_data_caching/\n# 3. Basic Caching Strategies To maximize the advantages of the Go-Redis connection, it’s imperative to employ the right caching strategy. Each strategy has its own set of merits and potential pitfalls, making the choice critical based on specific application needs. Let’s deep-dive into some basic caching strategies and their respective implementation in Go.\n# 3.1. Cache-aside (Lazy Loading) Cache-aside, often termed lazy loading, is a caching pattern where the application code is responsible for loading data into the cache, updating, and invalidating cache entries.\n# 3.1.1. Implementation: Check the Cache: Initially, the application checks the cache to determine if the desired data is present. Database Call: If not present in the cache, the application fetches the data from the primary data store (e.g., a database) and then places it in the cache. Go Code: 1 2 3 4 5 6 7 8 9 10 value, err := client.Get(ctx, \u0026#34;key\u0026#34;).Result() if err == redis.Nil { // Cache miss // Fetch data from database data := fetchDataFromDB(\u0026#34;key\u0026#34;) client.Set(ctx, \u0026#34;key\u0026#34;, data, time.Hour) // Store in cache } else if err != nil { panic(err) } else { fmt.Println(\u0026#34;Data from cache:\u0026#34;, value) } # 3.1.2. Advantages \u0026amp; Disadvantages: Advantages: Minimal Initial Load: Since data is loaded on-demand, the initial loading time is reduced. Always Updated: Data in the cache is guaranteed to be current since it’s fetched only when needed. Disadvantages: Latency: The first-time data is fetched (cache miss) has an additional latency since it requires a database call. Stale Data: If not managed correctly, there can be periods where stale data remains in the cache. # 3.2. Write-through Cache In a write-through caching strategy, every write to the application data also writes to the cache. The cache is always updated with fresh data.\n# 3.2.1. Implementation: Write Operation:\nEvery time there’s a write operation to the primary data store, the same data is written to the cache. Go Code:\n1 2 3 4 5 // Data update function func updateData(key string, value string) { updateDatabase(key, value) // Update primary data store client.Set(ctx, key, value, time.Hour) // Update cache } # 3.2.2 Advantages \u0026amp; Disadvantages: Advantages: Data Consistency: The cache always contains the latest data. Read Speed: Read operations are fast as data is always available in the cache. Disadvantages: Write Penalty: Every write operation comes with an overhead of updating the cache. Resource Intensive: It can be resource-intensive for write-heavy applications. # 3.3. Write-behind (or Write-back) Cache Here, the application writes directly to the cache, which then periodically updates the primary data store. This reduces the latency associated with every write operation.\n# 3.3.1. Implementation: Buffered Writes:\nWrites are buffered in the cache and are periodically flushed to the primary data store. Go Code:\n1 2 3 4 5 // Buffered data update function func bufferedUpdate(key string, value string) { client.Set(ctx, key, value, time.Hour) // Update cache // A separate routine or process will flush the cache to the primary data store } # 3.3.2. Advantages \u0026amp; Disadvantages: Advantages: Fast Writes: Write operations are speedy since they only update the cache initially. Batch Processing: Periodic flushing can leverage batch processing for efficiency. Disadvantages: Data Loss: If the cache fails before a flush, data can be lost. Complexity: Implementing a reliable flushing mechanism adds complexity. # 4. Eviction Policies: Keeping Your Cache Optimized Redis provides several eviction policies, ensuring optimal use of memory:\nNo Eviction: Redis returns errors when the memory limit is reached. AllKeys LRU: Evicts least recently used keys first. AllKeys Random: Evicts random keys. Volatile LRU: Evicts least recently used keys, but only among those set to expire. Volatile Random: Random eviction, but only among keys with an expiration set. Volatile TTL: Evicts the keys with the nearest expiration time first. In Go, you can set the desired eviction policy using:\n1 client.ConfigSet(ctx, \u0026#34;maxmemory-policy\u0026#34;, \u0026#34;allkeys-lru\u0026#34;) # 5. Best Practices for Go-Redis Caching # 5.1. Cache key naming conventions Choosing an appropriate naming convention for your cache keys can significantly improve cache manageability, readability, and prevent key collisions.\nDescriptive Names: A key should provide hints about the data it holds.\nBad: k1234 Good: user:profile:1234 Namespacing: Use colons : for separating different parts of your keys to simulate namespaces.\nExample: post:comments:4567 Versioning: When your data structure changes, you can use versioning in your keys to avoid conflicts.\nExample: v2:user:profile:1234 Keep It Concise: While descriptiveness is essential, long keys take more memory.\nExample in Go:\n1 key := fmt.Sprintf(\u0026#34;post:%d:comments\u0026#34;, postID) # 5.2. Handling Cache Misses Efficiently Cache misses can be expensive. Here’s how to manage them wisely:\nImplementing a Loading Strategy: On a cache miss, fetch the data from the primary data source and load it into the cache for future requests. This can be implemented with the Cache-aside (Lazy Loading) pattern we discussed earlier. Avoid Cache Stampede: This occurs when multiple clients try to read a key that’s missing from the cache, causing them all to hit the database simultaneously. One way to avoid this is by using a mutex or a semaphore to ensure only one client fetches from the database. Set Reasonable TTLs (Time-To-Live): For infrequently changed data, longer TTLs are apt, while frequently changed data benefits from shorter TTLs. # 6. Common Pitfalls in Go-Redis Caching and How to Avoid Them # 6.1. Not Considering Serialization Costs The Pitfall: Overlooking the time and CPU overhead of serializing and deserializing data when caching complex data structures. The Solution: Choose efficient serialization libraries and formats. In Go, libraries like encoding/gob or third-party solutions like msgpack can be considered. Test serialization strategies to see which works best for your specific data types and access patterns. e.g.,\nSerialization of cache items (encoding/gob?)\nI switched our in-memory caching to Redis and used encoding/gob to serialize the cache items. It seemed nice considering it carries type info and gave my \u0026ldquo;generic\u0026rdquo; caches a nice api (as I could just keep the redis store as an interchangeable Store interface, easily switched out for an in-memory cache without serialization or whatever), and benchmarked quite well as compared to json. But now I\u0026rsquo;m seeing a pretty massive CPU usage on our servers, and having read a bit more it might not have been a great use case for gob, which apparently is better suited for streams of data where one encoder instance is \u0026ldquo;paired\u0026rdquo; to one decoder instance. I end up having to initialize GobEncoder and GobDecoder on every ser/deser, and I suspect it\u0026rsquo;s taking a pretty heavy toll on CPU.\nIs there any way around this while still using gob? I tried keeping an encoder/decoder alive with a buffer along with a mutex, and clearing the buffer with buffer.Reset() on every ser/deser, but that fails. There seems to be more internal state to gob than that.\nWhat other serialization formats do people normally use for storing stuff in redis or equivalent?\nComments:\nOP: The thing I\u0026rsquo;m unsure about is whether gob is a viable solution for these \u0026ldquo;one off\u0026rdquo; ser/deserializations? Since it\u0026rsquo;s primarily meant for streams of data, and I end up using something like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 func Serialize(item Item) ([]byte, error) { b := new(bytes.Buffer) if err := gob.NewEncoder(b).Encode(item); err != nil { return nil, err } return b.Bytes(), nil } func Deserialize(data []byte) (*Item, error) { b := bytes.NewBuffer(data) var item Item return \u0026amp;item, gob.NewDecoder(b).Decode(\u0026amp;item) } Someone: Yeah, gob isn\u0026rsquo;t the best fit for storing singular values.\nhttps://golang.org/pkg/encoding/gob/#Encoder.Encode\n\u0026hellip; guaranteeing that all necessary type information has been transmitted first.\nThis could be the gotcha you\u0026rsquo;re facing, https://golang.org/src/encoding/gob/decoder.go#L153 will likely end up running for every single value (as each stored value is in effect a complete stream with a single element).\nedit: https://github.com/alecthomas/go_serialization_benchmarks may be helpful\nSomeone: You\u0026rsquo;ll also find that gob uses more space than many other encodings when storing singular values - it\u0026rsquo;s just not suited to this task.\nOP: Ended up using msgpack as it performed way, way better for this single value scenario. Gob is just meant for streaming I suppose.\n# 6.2. Cache Invalidation Woes The Pitfall: Not invalidating or updating the cache when the underlying data changes, leading to stale data being served. The Solution: Implement a robust cache invalidation strategy. This might include setting appropriate TTLs, using write-through caching, or manually invalidating keys when data changes. # 6.3. Not Preparing for Redis Failures The Pitfall: Failing to consider scenarios where the Redis server might become unavailable. The Solution: Implement redundancy using Redis Sentinel or Redis Cluster. On the application side, ensure that your Go application can handle Redis downtimes gracefully, potentially serving stale data or reverting to the primary data source. Learn more:\nComplete Guide to Redis in Go - From Installation to Advanced Features | Master Redis with Golang Go-Redis Caching: Strategies, Best Practices \u0026amp; Common Pitfalls ","date":"2023-10-01T17:56:58Z","permalink":"https://blog.yorforger.cc/p/caching-strategies-and-what-should-be-cached/","title":"Caching Strategies and What should be Cached"},{"content":"To prevent data race, store.Get() always creates a new session (or make a copy) and returns the session to user.\n1 2 3 4 5 6 7 // Get a session. // A copied one or new session. session, _ = store.Get(req, \u0026#34;session-key\u0026#34;) // Add a value. session.Values[\u0026#34;foo\u0026#34;] = \u0026#34;bar\u0026#34; // Save. _ = sessions.Save(req, rsp) Let\u0026rsquo;s check the source code, the Get() method of Redistore:\n1 2 3 func (s *RediStore) Get(r *http.Request, name string) (*sessions.Session, error) { return sessions.GetRegistry(r).Get(s, name) } It gets a registry associated with the request, then get the session by the registry. The code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // GetRegistry returns a registry instance for the current request. func GetRegistry(r *http.Request) *Registry { var ctx = r.Context() registry := ctx.Value(registryKey) if registry != nil { return registry.(*Registry) } newRegistry := \u0026amp;Registry{ request: r, sessions: make(map[string]sessionInfo), } *r = *r.WithContext(context.WithValue(ctx, registryKey, newRegistry)) return newRegistry } For each request they are different, even that two request from a same client. So for each new request when the server call store.Get() there will be a registry created. Only when you call store.Get() more than once in a handler, the registry := ctx.Value(registryKey) will return a non-nil registry:\n1 2 3 4 5 6 7 8 9 10 // Get a session. session, _ = store.Get(req, \u0026#34;session-key\u0026#34;) // Add a value. session.Values[\u0026#34;foo\u0026#34;] = \u0026#34;bar\u0026#34; // Save. _ = sessions.Save(req, rsp) // Get session again // because the req is same, the Registry // will be the old one created druing the first store.Get() call. session, _ = store.Get(req, \u0026#34;session-key\u0026#34;) After gets the registry, then its Registry.Get() will be called:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Get registers and returns a session for the given name and session store. // // It returns a new session if there are no sessions registered for the name. func (s *Registry) Get(store Store, name string) (session *Session, err error) { if !isCookieNameValid(name) { return nil, fmt.Errorf(\u0026#34;sessions: invalid character in cookie name: %s\u0026#34;, name) } // because Registry is always created as a new one for each request // ok always equals to false // if you call store.Get() at a same handler twice or more, // the Registry value will be the old one which created during the first store.Get() call if info, ok := s.sessions[name]; ok { session, err = info.s, info.e } else { // for each request, there will be a new session // a brand new or a copied one session, err = store.New(s.request, name) session.name = name s.sessions[name] = sessionInfo{s: session, e: err} } session.store = store return } Let\u0026rsquo;s check how store.New work:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func (s *RediStore) New(r *http.Request, name string) (*sessions.Session, error) { var err error session := sessions.NewSession(s, name) // make a copy options := *s.Options session.Options = \u0026amp;options session.IsNew = true if c, errCookie := r.Cookie(name); errCookie == nil { // verify the cookie err = securecookie.DecodeMulti(name, c.Value, \u0026amp;session.ID, s.Codecs...) // if the cookie is correct which means there is a corresponding session in the store // therefore, load from redistore // load() will makes a deep copy, which prevent data race // user can use the returned session safely if err == nil { ok, err := s.load(session) session.IsNew = !(err == nil \u0026amp;\u0026amp; ok) // not new if no error and data available } } return session, err } Let\u0026rsquo;s check store.load()\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // load reads the session from redis. // returns true if there is a sessoin data in DB func (s *RediStore) load(session *sessions.Session) (bool, error) { conn := s.Pool.Get() defer conn.Close() if err := conn.Err(); err != nil { return false, err } data, err := conn.Do(\u0026#34;GET\u0026#34;, s.keyPrefix+session.ID) if err != nil { return false, err } if data == nil { return false, nil // no data was associated with this key } b, err := redis.Bytes(data, err) if err != nil { return false, err } return true, s.serializer.Deserialize(b, session) } Source code: https://github.com/boj/redistore\n","date":"2023-10-01T12:12:50Z","permalink":"https://blog.yorforger.cc/p/gorilla-sessions-redisstore-source-code-analysis/","title":"Gorilla Sessions RedisStore Source Code Analysis"},{"content":" # 1. Redis data types Redis, an acronym for Remote Dictionary Server, is an open-source, in-memory data structure store. It can be employed as a database, cache, and even a message broker. Unlike traditional databases that read and write data to disk, Redis operates primarily in memory, which is one of the key reasons behind its lightning-fast data retrieval capabilities.\n# 1.1. String Redis Strings is one of the most versatile of Redis’ building blocks, a binary-safe data structure, binary-safe means the strings in Redis can be any binary data. In Redis, strings are multipurpose. They can store data as simple as an integer or as complex as a JPEG image file.\nStrings is an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. It can store any data-a string, integer, floating point value, JPEG image, serialized Ruby object, or anything else you want it to carry.\n1 2 3 4 redis\u0026gt; set mykey \u0026#34;This is a string\u0026#34; OK redis\u0026gt; get mykey \u0026#34;This is a string\u0026#34; # 1.2. Hash A Redis hash is a data type that represents a mapping between a string field and a string value. There is no integer or float value in hash, just string.\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; hset bike:1 model Deimos type \u0026#39;Enduro bikes\u0026#39; price 4972 (integer) 3 \u0026gt; hget bike:1 model \u0026#34;Deimos\u0026#34; \u0026gt; hget bike:1 price \u0026#34;4972\u0026#34; \u0026gt; hgetall bike:1 1) \u0026#34;model\u0026#34; 2) \u0026#34;Deimos\u0026#34; 3) \u0026#34;type\u0026#34; 4) \u0026#34;Enduro bikes\u0026#34; 5) \u0026#34;price\u0026#34; 6) \u0026#34;4972\u0026#34; Note that the output of \u0026gt; hget bike:1 price is \u0026quot;4972\u0026quot; not 4972, which is a string.\nOn the surface, think of a hash data type as an enclosed set of key/value pairs inside the value of another key. The embedded keys are called fields. So a string is stored inside a key and inside a particular field within that key.\nFigure 3 shows the basics of how a hash is stored. The key is associated with a hash object. That hash object can have multiple fields (i.e., subkeys) associated with individual strings. This allows you to store more complex data types.\nSource:\nRedis Data Types: The Basics - The New Stack\nhttps://youtu.be/-KdITaRkQ-U?si=aAQDQfZvUhqGn800\nLearn more:\nRedis Data Types: The Basics - The New Stack\nData Structures | Redis\nRedis data types | Redis\n# 2. Key-value database All data in Redis are stored as key-value pairs, Redis keys are always strings, Redis values can be strings, hashes, lists, sets or sorted sets.\nA good practice for naming keys is to create and stick to a particular schema. For example, a common scheme for identifying objects is to use an object-type:id schema. So an object representing user id#1000 would be attached to the key name user:1000. An object representing a purchase order with an order number of 1234 might be attached to a key named order:1234.\nAdditionally, keys related to the base object can use suffixes on the key. For example, while the user id#1000 can use the key user:1000, a queue associated with that user could use the key user:1000:queue.\nSource: https://thenewstack.io/redis-data-types-the-basics/\n# 2.1. Key is always string When you store a hash object in Redis, the key is still a string, and the value is a hash data structure:\n1 2 3 4 5 6 7 127.0.0.1:6379\u0026gt; hset user:1 name Alice age 6 (integer) 2 127.0.0.1:6379\u0026gt; set os Ubuntu OK 127.0.0.1:6379\u0026gt; keys * 1) \u0026#34;user:1\u0026#34; 2) \u0026#34;os\u0026#34; As you can see there we created two key-value data above, the first one\u0026rsquo;s value is a hash and second one\u0026rsquo;s value is a string, that\u0026rsquo;s the only difference. You may think the first key \u0026quot;user:1\u0026quot; a littler strange, but it\u0026rsquo;s just a string, you can name it as user-1, user1 or other form, \u0026quot;user:1\u0026quot; is just a hash\u0026rsquo;s key naming convention in Redis.\nAnd we use key to retrieve data:\n1 2 3 4 5 6 7 8 9 127.0.0.1:6379\u0026gt; get os \u0026#34;Ubuntu\u0026#34; 127.0.0.1:6379\u0026gt; hget user:1 age \u0026#34;6\u0026#34; 127.0.0.1:6379\u0026gt; hgetall user:1 1) \u0026#34;name\u0026#34; 2) \u0026#34;Alice\u0026#34; 3) \u0026#34;age\u0026#34; 4) \u0026#34;6\u0026#34; Retrieving hash value in Redis is a little complited, as you can see, we use string \u0026quot;user:1\u0026quot; to retrieve the hash value {\u0026quot;name\u0026quot;:\u0026quot;Alice\u0026quot;, \u0026quot;age\u0026quot;, \u0026quot;6\u0026quot;}, then we use the string \u0026quot;age\u0026quot; to get the string value \u0026quot;6\u0026quot;.\n# 2.2. Don\u0026rsquo;t forget all is string Note that we are in the Redis command line, therefore we don\u0026rsquo;t need make the key or value string explicitly, what I mean is that the both codes belwo is fine:\n1 2 3 4 5 6 127.0.0.1:6379\u0026gt; set \u0026#34;greeting\u0026#34; \u0026#34;helloworld\u0026#34; OK 127.0.0.1:6379\u0026gt; set greeting helloworld OK 127.0.0.1:6379\u0026gt; hset user:1 name Alice age 12 127.0.0.1:6379\u0026gt; hset \u0026#34;user:1\u0026#34; \u0026#34;name\u0026#34; \u0026#34;Alice\u0026#34; \u0026#34;age\u0026#34; \u0026#34;12\u0026#34; Bu you should know that they are string!\nIn Redis, a key-value pair is a fundamental data structure used for storing and retrieving data. The \u0026ldquo;key\u0026rdquo; is a unique identifier that is used to access the associated \u0026ldquo;value\u0026rdquo;.\nA Redis hash is a data type that represents a mapping between a string field and a string value.\n# 3. Redis features Atomic Operations: Redis operations are atomic, ensuring data integrity and consistency, even in concurrent environments, a feature particularly complementary to Go’s concurrent capabilities.\nPersistence: Even though Redis is an in-memory store, it offers optional durability. This means you can periodically save the data in memory to disk without compromising performance.\nRedis provides different methods to persist data on disk without sacrificing much performance. Configuring the right persistence option can make a significant impact. RDB Snapshots: RDB persistence offers point-in-time snapshots of your dataset at specified intervals. Append-Only File (AOF): Logs every write operation received by the server, allowing complete data recovery. Versatility: Redis isn’t just a simple key-value store. It supports various data structures like strings, hashes, lists, sets, and more, making it adaptable to diverse caching needs.\nScalability: With features like replication, partitioning, and clustering, Redis is designed for high availability and horizontal scaling, catering to applications of all sizes.\nHorizontal Scaling: Using Redis Cluster, you can distribute data across multiple machines. Vertical Scaling: Increasing the hardware resources (CPU, Memory) of your existing Redis server. Replication: Setting up master-slave replication to distribute read queries among multiple nodes. Time-to-Live (TTL): Redis supports the assignment of expiration times to keys, making it conducive to caching scenarios.\nSource:\nGo-Redis Caching: Strategies, Best Practices \u0026amp; Common Pitfalls\nComplete Guide to Redis in Go - From Installation to Advanced Features | Master Redis with Golang\n# 4. Use cases # 4.1. Session Store There are many ways to save sessions, in-memory, file, database and Redis, if you don\u0026rsquo;t know session, please refer to: Cookie \u0026amp; Session\n# 4.2. Caching Learn more:\nReferences:\nInstall Redis on macOS | Redis Redis CLI | Redis Redis: In-memory database. How it works and Why you should use it | The Home of Redis Developers ","date":"2023-10-01T10:40:58Z","permalink":"https://blog.yorforger.cc/p/introduce-redis-in-memory-database/","title":"Introduce Redis - In-memory Database"},{"content":" # 1. Install \u0026amp; run redis server Install:\n1 $ brew install redis Run Redis in the foreground:\n1 $ redis-server As an alternative to running Redis in the foreground, you can also start the process in the background:\n1 2 3 $ brew services start redis $ brew services stop redis # 2. Connect to redis server Once Redis is running, you can connect it by running Redis Command Line Interface - redis-cli:\n1 2 # run a client at terminal to connect Redis server $ redis-cli By default, redis-cli connects to the server at the address 127.0.0.1 with port 6379. To specify a different host name or an IP address:\n1 $ redis-cli -h redis15.localnet.org -p 6390 By default, redis-cli uses a plain TCP connection to connect to Redis. You may enable SSL/TLS using the --tls option, along with --cacert or --cacertdir to configure a trusted root certificate bundle or directory.\nIf the target server requires authentication using a client side certificate, you can specify a certificate and a corresponding private key using --cert and --key.\n# 3. redis-cli MONITOR All commands received by the active Redis instance will be printed to the standard output:\n1 2 3 4 5 $ redis-cli MONITOR OK 1692367745.525689 [0 127.0.0.1:49963] \u0026#34;set\u0026#34; \u0026#34;name\u0026#34; \u0026#34;jack\u0026#34; 1692368601.032173 [0 127.0.0.1:49963] \u0026#34;set\u0026#34; \u0026#34;name\u0026#34; \u0026#34;john\u0026#34; 1692368645.284030 [0 127.0.0.1:49963] \u0026#34;get\u0026#34; \u0026#34;name\u0026#34; This means you will get all the commands that your redis server received from clients, recall that redis-cli will connects to the server at the address 127.0.0.1 with port 6379.\nAnd if your application C1 use a Redis server S to save data, your another application C2 that connects to server S can get that data too. When you run redis-cli, it start a client, and you can input keys * to query all key saved on your Redis server.\nOf course using Redis just from the command line interface(redis-cli) is not enough as the goal is to use it from your application. In order to do so you need to download and install a Redis client library for your programming language. You\u0026rsquo;ll find a full list of clients for different languages in this page.\n","date":"2023-09-30T16:40:58Z","permalink":"https://blog.yorforger.cc/p/useful-redis-commands/","title":"Useful Redis Commands"},{"content":" # 1. Benchmark # 1.1. b.N loop Functions of the form\n1 func BenchmarkXxx(*testing.B) are considered benchmarks, and are executed by the go test command when its -bench flag is provided. Benchmarks are run sequentially.\nA sample benchmark function looks like this:\n1 2 3 4 5 func BenchmarkRandInt(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { rand.Int() } } The benchmark function must run the target code b.N times. During benchmark execution, b.N is adjusted until the benchmark function lasts long enough to be timed reliably. The output\n1 BenchmarkRandInt-8 68453040\t17.8 ns/op means that the loop ran 68453040 times at a speed of 17.8 ns per loop.\n# 1.2. Ignore expensive setup with timer If a benchmark needs some expensive setup before running, the timer may be reset:\n1 2 3 4 5 6 7 8 func BenchmarkBigLen(b *testing.B) { big := prepare() // note this will reset the timer b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { big.Len() } } Or something like this:\n1 2 3 4 5 6 7 8 func BenchmarkFib(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { b.StopTimer() // pause timer prepare() // prepare, initial setup b.StartTimer() // continue counting funcUnderTest() // the function was tested } } # 1.3. RunParallel() If a benchmark needs to test performance in a parallel setting, it may use the RunParallel helper function; such benchmarks are intended to be used with the go test -cpu flag:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func main() { testing.Benchmark(func(b *testing.B) { templ := template.Must(template.New(\u0026#34;test\u0026#34;).Parse(\u0026#34;Hello, {{.}}!\u0026#34;)) b.RunParallel(func(pb *testing.PB) { // Each goroutine has its own bytes.Buffer. var buf bytes.Buffer for pb.Next() { // The loop body is executed b.N times total across all goroutines. buf.Reset() templ.Execute(\u0026amp;buf, \u0026#34;World\u0026#34;) } }) }) } The body function will be run in each goroutine. It should set up any goroutine-local state and then iterate until pb.Next returns false. It should not use the StartTimer, StopTimer, or ResetTimer functions, because they have global effect. It should also not call Run.\n# 1.4. Skip test TestSomething(t *testing.T) this is called test function func BenchmarkXxx(*testing.B) this is called benchmark function go test run your benchmark function and does not disable run the test functions in the package. If you want to skip the test function, you can do so by passing a regex to the -run flag that will not match anything. I usually use\n1 go test -run=XXX -bench=. Learn more: Unit Test Basic Commands - Go - David\u0026rsquo;s Blog\n# 1.5. A note on compiler optimisations Before concluding I wanted to highlight that to be completely accurate, any benchmark should be careful to avoid compiler optimisations eliminating the function under test and artificially lowering the run time of the benchmark.\n1 2 3 4 5 6 7 8 9 10 11 12 13 var result int func BenchmarkFibComplete(b *testing.B) { var r int for n := 0; n \u0026lt; b.N; n++ { // always record the result of Fib to prevent // the compiler eliminating the function call. r = Fib(10) } // always store the result to a package level variable // so the compiler cannot eliminate the Benchmark itself. result = r } References:\nhttps://pkg.go.dev/testing#hdr-Benchmarks https://pkg.go.dev/testing#B.RunParallel How to write benchmarks in Go | Dave Cheney ","date":"2023-09-30T09:58:56Z","permalink":"https://blog.yorforger.cc/p/test-benchmark-go/","title":"Test \u0026 Benchmark - Go"},{"content":"In previous post we know that don\u0026rsquo;t mediate the access to shared memory with locks and mutexes instead share that memory by communicating. However, is this always true? Sometimes you may want mutex and lock not channels, it depends on the situation. Channel is a way help mutiple groutines communicate with each other. I find some posts and share it here:\nUse a sync.Mutex or a channel?\nOne of Go\u0026rsquo;s mottos is \u0026ldquo;Share memory by communicating, don\u0026rsquo;t communicate by sharing memory.\u0026rdquo;\nThat said, Go does provide traditional locking mechanisms in the sync package. Most locking issues can be solved using either channels or traditional locks. So which should you use?\nUse whichever is most expressive and/or most simple.\nA common Go newbie mistake is to over-use channels and goroutines just because it\u0026rsquo;s possible, and/or because it\u0026rsquo;s fun. Don\u0026rsquo;t be afraid to use a sync.Mutex if that fits your problem best. Go is pragmatic in letting you use the tools that solve your problem best and not forcing you into one style of code.\nAs a general guide, though:\nChannel Mutex passing ownership of data, distributing units of work, communicating async results caches, state If you ever find your sync.Mutex locking rules are getting too complex, ask yourself whether using channel(s) might be simpler.\nWait Group\nAnother important synchronisation primitive is sync.WaitGroup. These allow co-operating goroutines to collectively wait for a threshold event before proceeding independently again.\nChannel communication, mutexes and wait-groups are complementary and can be combined.\nSource: MutexOrChannel · golang/go Wiki\nMutexes are faster than channels. My rule of thumb is, if you can do what you need to do with one mutex, there\u0026rsquo;s no problem with that. For instance, concurrent map access, or some integer you need atomically incremented, or other such simple things.\nHowever, you want to never try to take two locks. This includes bearing in mind that if you take a lock in your function, and call something else that takes a lock, that\u0026rsquo;s two locks. Threading hell is not caused by a single lock, it is caused by trying to use more than one at a time. At that point, you generally want channels and select. They don\u0026rsquo;t magically solve the problem. Technically you can still deadlock with channels and select.\nAlso, performance isn\u0026rsquo;t everything, but I do find it helpful to bear in mind that when using any of these primitives, they aren\u0026rsquo;t free, even if they are cheap, and they need to pay their way. In general the way you do that sort of thing is try to ensure that when you do take a lock or send a message over a channel, you do what you can to send as big a task or chunk of information as possible. Even in a local OS process, you need to be a bit careful not to design internal APIs in a way that two processes are constantly interacting over mutexes or channels. For instance, rather than a loop where you ask the user server for a user\u0026rsquo;s real name one at a time, create an API where the user server will accept the full list and return the full set in one shot. Even 13.2ns is a couple hundred cycles, and that number is also the best case, it can get worse if there is contention.\nSource: https://www.reddit.com/r/golang/comments/d85d0l/comment/f17m5e8/?utm_source=share\u0026utm_medium=web2x\u0026context=3\nThis is just an example for demonstrating passing behavior with function, don\u0026rsquo;t over-use channel:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 func newChannelStore() *Store { s := \u0026amp;Store{ops: make(chan func(map[string]*Session))} go s.loop() return s } type Store struct { ops chan func(map[string]*Session) } func (s *Store) addSession(session *Session) { s.ops \u0026lt;- func(m map[string]*Session) { // if the key has existed in map, change the value of the key. // if key doesn\u0026#39;t exist, create a new one m[session.id] = session } } func (s *Store) getSession(id string) *Session { result := make(chan *Session, 1) s.ops \u0026lt;- func(m map[string]*Session) { // everything copied by value, session is a copy of m[id] // you should consider if session has pointer field session, ok := m[id] if !ok { result \u0026lt;- nil return } result \u0026lt;- session } // wait ops finish return \u0026lt;-result } func (s *Store) loop() { sessions := make(map[string]*Session) for op := range s.ops { op(sessions) } } The code below has a better performance compared with code above which achieves in channel style:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 func newMuxStore() *store { return \u0026amp;store{ mu: sync.RWMutex{}, sessions: make(map[string]*Session), } } type store struct { mu sync.RWMutex sessions map[string]*Session } func (m *store) addSession(session *Session) { m.mu.Lock() defer m.mu.Unlock() m.sessions[session.id] = session } func (m *store) getSession(id string) *Session { m.mu.RLock() defer m.mu.RUnlock() session, ok := m.sessions[id] if !ok { return nil } return session } session.go:\n1 2 3 4 5 6 7 8 9 10 11 12 13 func newSession(id string, value int) *Session { return \u0026amp;Session{ id: id, value: value, expiry: time.Now().Add(time.Duration(10) * time.Second).Unix(), } } type Session struct { id string value int expiry int64 } ","date":"2023-09-29T20:44:55Z","permalink":"https://blog.yorforger.cc/p/dont-overuse-channel-go/","title":"Don't Overuse Channel - Go"},{"content":" # 1. bytes \u0026amp; strings packages Package strings implements simple functions to manipulate UTF-8 encoded strings.\nPackage bytes implements functions for the manipulation of byte slices. It is analogous to the facilities of the strings package.\n# 1.1. Make http request When send POST request with json body, you can write code like this:\n1 2 3 4 5 jsonBytes := []byte(`{\u0026#34;username\u0026#34;:\u0026#34;coco\u0026#34;, \u0026#34;password\u0026#34;:\u0026#34;778899a\u0026#34;}`) r, _ := http.NewRequest(\u0026#34;POST\u0026#34;, \u0026#34;/login\u0026#34;, bytes.NewReader(jsonBytes)) r.Header.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) w := httptest.NewRecorder() ... And at the server side, decode the request body:\n1 2 3 4 5 6 7 8 9 func handleLogin(w http.ResponseWriter, r *http.Request) { user := struct { Username string `json:\u0026#34;username\u0026#34;` Password string `json:\u0026#34;password\u0026#34;` }{} decoder := json.NewDecoder(r.Body) _ = decoder.Decode(\u0026amp;user) fmt.Println(user) } Because the third parameter ofhttp.NewRequest() required to be an io.Reader, we use bytes.NewReader() to wrap jsonBytes at client side.\n1 func NewRequest(method, url string, body io.Reader) (*Request, error) You can use other type which implements the io.Reader at client to call http.NewRequest():\n1 2 3 4 jsonString := `{\u0026#34;username\u0026#34;:\u0026#34;coco\u0026#34;, \u0026#34;password\u0026#34;:\u0026#34;778899a\u0026#34;}` r, _ := http.NewRequest(\u0026#34;POST\u0026#34;, \u0026#34;/login\u0026#34;, strings.NewReader(jsonString)) r.Header.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) ... As you can see the package bytes and strings are just different for processing byte slices and strings, strings.NewReader() wraps a string value into an io.Reader whereas bytes.NewReader converts a byte slice into an io.Reader.\nstrings.NewRecoder():\n1 func NewReader(s string) *Reader bytes.NewReader:\n1 func NewReader(b []byte) *Reader # 2. bytes.Buffer type A Buffer is a variable-sized buffer of bytes with Read and Write methods. This means it has implemented io.Reader and io.Writer interface.\n1 2 3 4 5 6 7 // A Buffer is a variable-sized buffer of bytes with Read and Write methods. // The zero value for Buffer is an empty buffer ready to use. type Buffer struct { buf []byte // contents are the bytes buf[off : len(buf)] off int // read at \u0026amp;buf[off], write at \u0026amp;buf[len(buf)] lastRead readOp // last read operation, so that Unread* can work correctly. } # 3. bufio package Buffered I/O is a technique that allows a program to read or write data in chunks rather than one byte at a time. This is useful because it allows the program to read or write data more efficiently. It also allows the program to read or write data more predictably.\n# 3.1. bufio.Reader \u0026amp; io.Reader To create a buffered reader (bufio.Reader), you can use the bufio.NewReader function. This function takes an io.Reader as an argument. This means that you can pass in any type that implements the io.Reader interface. This includes os.File, strings.Reader, and bytes.Buffer.\nNote that until we introdeced two Readesr: io.Reader and bufio.Reader. bufio.Reader just wraps an io.Reader and provides additional buffering functionality, such as ReadLine(), ReadString(), and ReadBytes().\n1 func (b *Reader) ReadLine() (line []byte, isPrefix bool, err error) ReadLine() is a low-level line-reading primitive. Most callers should use ReadBytes('\\n') or ReadString('\\n') instead or use a bufio.Scanner.\n1 func (b *Reader) ReadString(delim byte) (string, error) ReadString() reads until the first occurrence of delim in the input, returning a string containing the data up to and including the delimiter.\nThe following program reads the content of a file line-by-line delimited with value '\\n' :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func main() { file, err := os.Open(\u0026#34;./planets.txt\u0026#34;) if err != nil { fmt.Println(err) os.Exit(1) } defer file.Close() reader := bufio.NewReader(file) for { line, err := reader.ReadString(\u0026#39;\\n\u0026#39;) if err != nil { if err == io.EOF { break } else { fmt.Println(err) os.Exit(1) } } fmt.Print(line) } } // credit: https://medium.com/learning-the-go-programming-language/streaming-io-in-go-d93507931185 # 3.2. bufio vs. io package The main difference between buffered I/O and normal I/O is that buffered I/O reads or writes data in chunks rather than one byte at a time. While on the other side normal I/O reads or writes data one byte at a time. This might not seem like a big difference but it can make a big difference in performance.\nIn a case where you are reading or writing a lot of data, buffered I/O can be much faster than normal I/O. To see this, we can compare the performance of buffered I/O and normal I/O using benchmarks.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 func funcToWithIO() { file, err := os.Open(\u0026#34;file.txt\u0026#34;) if err != nil { fmt.Println(err) return } defer file.Close() data := make([]byte, 100) for { _, err := file.Read(data) if err == io.EOF { break } if err != nil { fmt.Println(err) return } } } func funcToWithBufio() { file, err := os.Open(\u0026#34;file.txt\u0026#34;) if err != nil { fmt.Println(err) return } defer file.Close() reader := bufio.NewReader(file) data := make([]byte, 100) for { _, err := reader.Read(data) if err == io.EOF { break } if err != nil { fmt.Println(err) return } } } func createFile() { file, err := os.Create(\u0026#34;file.txt\u0026#34;) if err != nil { fmt.Println(err) return } defer file.Close() for i := 0; i \u0026lt; 1000000; i++ { file.Write([]byte(\u0026#34;Hello World!\u0026#34;)) } } func main() { createFile() funcToWithIO() funcToWithBufio() } Test file:\n1 2 3 4 5 6 7 8 9 10 11 func BenchmarkFuncToWithIO(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { funcToWithIO() } } func BenchmarkFuncToWithBufio(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { funcToWithBufio() } } 1 2 3 4 5 6 7 $ go test -bench \u0026#39;BenchmarkFuncToWithIO|BenchmarkFuncToWithBufio\u0026#39; goos: darwin goarch: arm64 pkg: leetcode BenchmarkFuncToWithIO-8 18 62718294 ns/op BenchmarkFuncToWithBufio-8 469 2531219 ns/op PASS Source: https://www.kelche.co/blog/go/golang-bufio/\n","date":"2023-09-26T17:03:55Z","permalink":"https://blog.yorforger.cc/p/useful-types-and-packages-for-io-go/","title":"Useful Types and Packages for IO - Go"},{"content":"Connecting to my aws server with ssh client is very slow, and after connected, the session is very slow too.\nLike input a command in the ssh session and you have to wait seconds to get the output, lots of latency.\nThen I think maybe there is a \u0026ldquo;bad\u0026rdquo; process which takes a lot of cpu usages makes my server in high load, or there is a high network traffic. Then I use top command to check, then I got this:\n1 2 $ top 60841 asterisk -11 0 1025372 325376 9300 R 99.9 32.9 980:16.51 asterisk The asterisk which takes up 99.9% of my cup!!!\nThenk I kill it with\n1 sudo kill 60841 Then my server works fine.\n","date":"2023-09-25T14:30:22Z","permalink":"https://blog.yorforger.cc/p/ssh-session-vary-slow/","title":"ssh session vary slow"},{"content":" # 1. Content-Type header The Content-Type header can be application/json, application/x-www-form-urlencoded, multipart/form-data, the first two is usually used with posting form data to server, the third is used to upload file to the server.\nThe format of application/json is '{\u0026quot;username\u0026quot;:\u0026quot;davidzhu\u0026quot;, \u0026quot;password\u0026quot;:\u0026quot;778899a\u0026quot;}'.\nThe format of application/x-www-form-urlencoded is \u0026quot;username=davidzhu\u0026amp;password=778899a\u0026quot; .\n# 2. Parse query string and x-www-form-urlencoded data In previous post I have talked that r.ParseForm() will try to parse both query string and request body in Go, but there is a condition: the Content-Type header of the reuqest must be application/x-www-form-urlencoded and you have the correct format data in the request body. Otherwise, the request body will be ignored by r.ParseForm(), it just tries to parse query string resides in the URL. Talk is cheap let\u0026rsquo;s show the codes,\nFirst, there is a simple handler which parses form data or query string:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func handlePostForm(w http.ResponseWriter, r *http.Request) { // r.ParseForm() parses username and password in form or query string. // And puts the parsed data into r.Form which is a map value. if err := r.ParseForm(); err != nil { http.Error(w, fmt.Sprintf(\u0026#34;failed to parse username and password: %v\u0026#34;, err), http.StatusMethodNotAllowed) return } username := r.Form.Get(\u0026#34;username\u0026#34;) password := r.Form.Get(\u0026#34;password\u0026#34;) if username == \u0026#34;\u0026#34; || password == \u0026#34;\u0026#34; { http.Error(w, \u0026#34;failed to parse username and password: no username or password\u0026#34;, http.StatusMethodNotAllowed) return } _, _ = fmt.Fprint(w, fmt.Sprintf(\u0026#34;username:%v password:%v\\n\u0026#34;, username, password)) } func main() { mux := http.NewServeMux() // Add pattern and its handdler into mux mux.HandleFunc(\u0026#34;/postform\u0026#34;, handlePostForm) // http.ListenAndServe will call mux.ServerHTTP(w, r) _ = http.ListenAndServe(\u0026#34;:8080\u0026#34;, mux) } And the request below will get same result:\n1 2 $ curl -X POST \u0026#34;localhost:8080/postform?username=david\u0026amp;password=778899a\u0026#34; $ curl -X POST localhost:8080/postform -d \u0026#34;username=davidzhu\u0026amp;password=778899a\u0026#34; But the code below won\u0026rsquo;t work, because the Content-Type header is not application/x-www-form-urlencoded:\n1 2 3 4 5 $ curl -X POST localhost:8080/postform -d \u0026#34;username=davidzhu\u0026amp;password=778899a\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; failed to parse username and password: no username or password # 3. Parse json data from request body The handler should be like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func handlePostBody(w http.ResponseWriter, r *http.Request) { // Parse username and password in form. type info struct { Username string `json:\u0026#34;username\u0026#34;` Password string `json:\u0026#34;password\u0026#34;` } user := info{} // note that r.Body is an io.Reader // which means decoder.Decode() method will consume data stroed in r.Body // you cannot read same data twice decoder := json.NewDecoder(r.Body) err := decoder.Decode(\u0026amp;user) if err != nil { http.Error(w, fmt.Sprintf(\u0026#34;failed to decode post body:%v\u0026#34;, err), http.StatusMethodNotAllowed) } _, _ = fmt.Fprint(w, fmt.Sprintf(\u0026#34;username:%v password:%v\\n\u0026#34;, user.Username, user.Password)) } The command below won\u0026rsquo;t work:\n1 2 3 4 5 $ curl -X POST localhost:8080/postbody -d \u0026#34;username=davidzhu\u0026amp;password=778899a\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; failed to decode post body:invalid character \u0026#39;u\u0026#39; looking for beginning of value username: password: Because the data format isn\u0026rsquo;t correct, this will work:\n1 $ curl localhost:8080/postbody -d \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;davidzhu\u0026#34;, \u0026#34;password\u0026#34;:\u0026#34;778899a\u0026#34;}\u0026#39; ","date":"2023-09-23T18:08:55Z","permalink":"https://blog.yorforger.cc/p/parse-form-data-in-go-server/","title":"Parse Form Data in Go Server"},{"content":" # 1. Intro of HTTP messages There are two types of messages: requests sent by the client to trigger an action on the server, and responses, the answer from the server.\nHTTP requests, and responses, share similar structure and are composed of:\nA start-line describing the requests to be implemented, or its status of whether successful or a failure. This start-line is always a single line. An optional set of HTTP headers specifying the request, or describing the body included in the message. A blank line indicating all meta-information for the request has been sent. An optional body containing data associated with the request (like content of an HTML form), or the document associated with a response. The presence of the body and its size is specified by the start-line and HTTP headers. The start-line and HTTP headers of the HTTP message are collectively known as the head of the requests, whereas its payload is known as the body.\n# 2. HTTP Requests # 2.1. Start line A start line contains three parts:\nTheir start-line contain three elements: An HTTP method, a verb (like GET, PUT or POST) or a noun (like HEAD or OPTIONS), that describes the action to be performed. GET 请求的结果可被浏览器和其他中间件(代理服务器)缓存，并且可能会被重复执行（例如，用户刷新页面时）。 如果你是在用户点击某个按钮后通过 GET 请求来处理点赞功能，那么理论上，仅当用户再次点击这个按钮时，该 GET 请求才会被重新发送。如果你的前端逻辑确保了每次点击都对应一个点赞操作，那么不会因为浏览器页面刷新而导致重复提交。然而，即使是这种情况，使用 GET 请求来处理点赞动作仍然不是最佳实践，原因包括： 非幂等性：点赞操作是非幂等的，因为每次操作都改变了服务器上的状态（即增加了点赞计数）。而 GET 请求应当是幂等的，即多次执行相同的请求应该具有相同的副作用。在这种情况下，多次点击按钮会导致多次点赞，违反了 GET 请求的幂等性原则。\nThe request target\nUsually a URL Sometimes url followed by a '?' and query string: http://example.com/?bar1=a\u0026amp;bar2=b The HTTP version\n1 POST www.example.com/test.html?username=alibaba HTTP/1.1 # 2.2. Headers An HTTP header consists of its case-insensitive name followed by a colon (:), then by its value. Whitespace before the value is ignored. This is an HTTP header:\n1 Content-Type: text/html; charset=utf-8 Usually, a request or response consist of many headers:\n1 2 3 4 5 6 7 8 GET /home.html HTTP/1.1 Host: developer.mozilla.org User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:50.0) Gecko/20100101 Firefox/50.0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Language: en-US,en;q=0.5 Accept-Encoding: gzip, deflate, br Referer: https://developer.mozilla.org/testpage.html Connection: keep-alive As you can see above, there are 7 headers in this request, a header like a key-value string, different header usually has different syntax and its own type. For example:\nThe type of Accept header is a Request header, and its syntax is below:\n1 2 3 4 5 6 Accept: \u0026lt;MIME_type\u0026gt;/\u0026lt;MIME_subtype\u0026gt; Accept: \u0026lt;MIME_type\u0026gt;/* Accept: */* // Multiple types, weighted with the quality value syntax: Accept: text/html, application/xhtml+xml, application/xml;q=0.9, image/webp, */*;q=0.8 The type of Content-Type header is Representation header, and its syntax:\n1 2 Content-Type: text/html; charset=utf-8 Content-Type: multipart/form-data; boundary=something The offcial way to indicate aobve is:\nMany different headers can appear in an HTTP request. They can be divided in several groups:\nGeneral headers, like Via, apply to the message as a whole. Request headers, like User-Agent or Accept, modify the request by specifying it further (like Accept-Language), by giving context (like Referer), or by conditionally restricting it (like If-None). Representation headers like Content-Type that describe the original format of the message data and any encoding applied (only present if the message has a body). # 2.3. Body The final part of the request is its body. Not all requests have one: requests fetching resources, like GET, HEAD, DELETE, or OPTIONS, usually don\u0026rsquo;t need one. Some requests send data to the server in order to update it: as often the case with POST requests (containing HTML form data).\nBodies can be broadly divided into two categories:\nSingle-resource bodies, consisting of one single file, defined by the two headers: Content-Type and Content-Length. Multiple-resource bodies, consisting of a multipart body, each containing a different bit of information. This is typically associated with HTML Forms. # 3. HTTP Responses HTTP Reponse also contains three parts:\nStatus line\nThree part: protocol version, a status code, a status text (A brief, purely informational, textual description of the status code to help a human understand the HTTP message.) HTTP/1.1 404 Not Found Headers\nSame as http request. Body\nSame as http request. Reference: HTTP Messages - HTTP | MDN\n","date":"2023-09-23T09:51:30Z","permalink":"https://blog.yorforger.cc/p/http-messages/","title":"HTTP Messages"},{"content":" # 1. POST data with Curl The Content-Type header can be application/json, application/x-www-form-urlencoded, multipart/form-data, the first two is usually used with posting form data to server, the third is used to upload file to the server.\nThe format of application/json is '{\u0026quot;username\u0026quot;:\u0026quot;davidzhu\u0026quot;, \u0026quot;password\u0026quot;:\u0026quot;778899a\u0026quot;}'.\nThe format of application/x-www-form-urlencoded is \u0026quot;username=davidzhu\u0026amp;password=778899a\u0026quot; .\ne.g.,\nPOST x-www-form-urlencoded format form data to server:\n1 2 # curl will set the request header to \u0026#39;application/x-www-form-urlencoded\u0026#39; by default curl localhost:8080/hello -d \u0026#34;username=davidzhu\u0026amp;password=778899a\u0026#34; POST json format data with curl:\n1 2 3 4 # you need specify the Content-Type header to application/json explictly curl -X POST [URL] -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;davidzhu\u0026#34;, \u0026#34;password\u0026#34;:\u0026#34;778899a\u0026#34;}\u0026#39; Note that we use single quote to wrap the double quote which makes double quote lose its special meaning, it\u0026rsquo;s just all about shell scripting, learn more: Shell Script Basic Syntax - David\u0026rsquo;s Blog\nBad example:\n1 2 # Bad example curl localhost:8080/hello -d \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;davidzhu\u0026#34;, \u0026#34;password\u0026#34;:\u0026#34;778899a\u0026#34;}\u0026#39; The server will parse it wrong, because the data format is json obviously, but curl will set the Content-Type header to application/x-www-form-urlencoded by default. Therefore, don\u0026rsquo;t write it wrong, otherwise, your server won\u0026rsquo;t parse data from the request correctly.\nThe content below form the docs of curl, hope it will help:\nTo make a POST request with Curl, you can run the Curl command-line tool with the -d or --data command-line option and pass the data as the second argument. Curl will automatically select the HTTP POST method and application/x-www-form-urlencoded content type if the method and content type are not explicitly specified. Source\nIf you submit data using Curl and do not explicitly specify the Content type, Curl uses the application/x-www-form-urlencoded content type for your data. Therefore, when sending JSON (or any other data type), you must specify the data type using the -H \u0026ldquo;Content-Type: application/json\u0026rdquo; command line parameter.\n1 2 3 $ curl localhost:8080/hello -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;Id\u0026#34;: 79, \u0026#34;status\u0026#34;: 3}\u0026#39; Learn more: application/x-www-form-urlencoded and multipart/form-data\nLearn more about http request header: https://davidzhu.xyz/post/cs-basics/018-http-messages/\n# 2. Query string with curl 1 2 3 $ curl -X POST \u0026#34;localhost:8080/postform?username=david\u0026amp;password=778899a\u0026#34; # The code below won\u0026#39; work (you have to make it with double quote): $ curl -X POST localhost:8080/postform?username=david\u0026amp;password=778899a # wrong ","date":"2023-09-22T22:58:30Z","permalink":"https://blog.yorforger.cc/p/curl-basics/","title":"Curl Basics"},{"content":"This article enlightened by:\nUnderstanding Mux and Handler concept GopherCon 2019: Mat Ryer - How I Write HTTP Web Services after Eight Years # 1. Hello world - 2 ways # 1.1. Implementing a Handler Interface - way 1 To run a server, we need to implement http.Handler interface:\n1 2 3 type Handler interface{ ServeHTTP(ResponseWriter, *Request) } To achieve that we need to create an empty struct and provide a method for it:\n1 2 3 4 5 6 7 8 9 10 11 type handler struct{} func (h *handler) ServeHTTP(w http.ResponseWriter, r *http.Request) { _, _ = fmt.Fprint(w, \u0026#34;hello world\\n\u0026#34;) } func main() { h := \u0026amp;handler{} // 1 http.Handle(\u0026#34;/\u0026#34;, h) // 3 _ = http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) //4 } Note that the signature of http.Handle():\n1 2 // Handle() registers the handler for the given pattern in the DefaultServeMux. func Handle(pattern string, handler Handler) the http.ListenAndServe()\n1 2 // The handler is typically nil, in which case the DefaultServeMux is used. func ListenAndServe(addr string, handler Handler) error Test:\n1 2 $ curl localhost:8080 hello world We creating an empty struct that implements http.Handler interface http.Handle() will register our handler for the given pattern, in our case, it is “/”. Why pattern and not URI? Because under the hood when your server is running and getting any request - it will find the closest pattern to the request path and dispatch the request to the corresponding handler. That means that if you will try to call \u0026lsquo;http://localhost:8000/some/other/path?value=foo\u0026rsquo; it will still be dispatched to our registered handler, even if it is registered under the “/” pattern. On the last line with http.ListenAndServe we starting server on the port 8000. Keep in mind the second argument, which is nil for now, but we will consider it in detail in a few moments. # 1.2. Using handler function - way 2 1 2 3 4 5 6 7 8 func handler(w http.ResponseWriter, r *http.Request) { _, _ = fmt.Fprint(w, \u0026#34;hello world\\n\u0026#34;) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) _ = http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } Note that the http.HandleFunc() above:\n1 2 // HandleFunc() registers the handler function for the given pattern in the DefaultServeMux. func HandleFunc(pattern string, handler func(ResponseWriter, *Request)) Test:\n1 2 $ curl localhost:8080 hello world This example looks very similar to the previous one, but it is a little bit shorter and easier to develop. We don’t need to implement any interface.\nNotice that under the hood http.HandleFunc() is just syntax sugar for avoiding the creation of http.Handler instances on each pattern. http.HandleFunc() is an adapter that converts a function to a struct with serveHTTP() method. So instead of this:\n1 2 3 4 5 6 7 8 9 10 11 12 type root struct{} func (t *root) ServeHTTP(w http.ResponseWriter, r *http.Request) {...} type home struct{} func (t *home) ServeHTTP(w http.ResponseWriter, r *http.Request) {...} type login struct{} func (t *login) ServeHTTP(w http.ResponseWriter, r *http.Request) {...} //... http.Handle(\u0026#34;/\u0026#34;, root) http.Handle(\u0026#34;/home\u0026#34;, home) http.Handle(\u0026#34;/login\u0026#34;, login) We can just use this approach:\n1 2 3 4 5 6 7 func root(w http.ResponseWriter, r *http.Request) {...} func home(w http.ResponseWriter, r *http.Request) {...} func login(w http.ResponseWriter, r *http.Request) {...} ... http.HandleFunc(\u0026#34;/\u0026#34;, root) http.HandleFunc(\u0026#34;/home\u0026#34;, home) http.HandleFunc(\u0026#34;/login\u0026#34;, login) # 2. ServeMux - the Router # 2.1. Register patterns for ServeMux You\u0026rsquo;ve probably noticed that above:\n1 2 3 4 5 6 7 8 9 // HandleFunc() registers the handler function for the given pattern in the DefaultServeMux. func HandleFunc(pattern string, handler func(ResponseWriter, *Request)) // Handle() registers the handler for the given pattern in the DefaultServeMux. func Handle(pattern string, handler Handler) // ListenAndServe() listens on the TCP network address addr and then calls Serve with handler to handle requests on incoming connections. // The handler is typically nil, in which case the DefaultServeMux is used. func ListenAndServe(addr string, handler Handler) error The comment of Handle() is quite confusing, what does \u0026ldquo;register\u0026rdquo; mean? We can check what this function does:\n1 2 3 // Handle registers the handler for the given pattern // in the DefaultServeMux. func Handle(pattern string, handler Handler) { DefaultServeMux.Handle(pattern, handler) } This is the definition of DefaultServeMux.Handle(pattern, handler) (I\u0026rsquo;ve deleted some codes):\n1 2 3 4 5 6 7 8 9 func (mux *ServeMux) Handle(pattern string, handler Handler) { // check if the pattern has existed if _, exist := mux.m[pattern]; exist { panic(\u0026#34;http: multiple registrations for \u0026#34; + pattern) } // add (register) the pattern and its corresponding hander into ServeMux e := muxEntry{h: handler, pattern: pattern} mux.m[pattern] = e } 1 2 3 4 5 6 7 8 9 10 11 type ServeMux struct { mu sync.RWMutex m map[string]muxEntry es []muxEntry // slice of entries sorted from longest to shortest. hosts bool } type muxEntry struct { h Handler pattern string } You can think ServeMux uses a map m to store the patterns and its corresponding hander. Functions http.Handle() and http.HandleFunc() will call ServeMux\u0026rsquo;s two method DefaultServeMux.HandleFunc(pattern, handler) and DefaultServeMux.Handle(pattern, handler) which will add (register) pattern into the default ServeMux.\nWe don\u0026rsquo;t need to call functions http.Handle() and http.HandleFunc() to register patterns for ServeMux, we can create our own ServeMux and we call its method HandleFunc() and Handle() directly.\nNote that there are four functions, first two is http.HandleFunc() and http.Handle() which were used above in the main code. Another two are the methods of ServeMux. They do different things, the first two just call the last two.\nA method contains a receiver, and a function does not contain a receiver.\n# 2.2 ServeMux - a request dispatcher (Router) ServeMux implements the function ServeHTTP() which means it implements http.Handler interface, ServeMux is a http.Handler itself:\n1 2 3 4 5 6 7 // ServeHTTP dispatches the request to the handler whose // pattern most closely matches the request URL. func (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) { // mux.Handler() returns the handler to use for the given request h, _ := mux.Handler(r) h.ServeHTTP(w, r) } Do you remember we said:\nNotice that under the hood http.HandleFunc() is just syntax sugar for avoiding the creation of http.Handler instances on each pattern. HandleFunc() is an adapter that converts a function to a struct with serveHTTP() method.\nActually it converts a function into another function type HandlerFunc which has implemented http.Handler, I\u0026rsquo;ll show you:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 func handler(w http.ResponseWriter, r *http.Request) { _, _ = fmt.Fprint(w, \u0026#34;hello world\\n\u0026#34;) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) _ = http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } //------------http.HandleFunc()-----------\u0026gt; // HandleFunc registers the handler function for the given pattern // in the DefaultServeMux. // The documentation for ServeMux explains how patterns are matched. func HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { DefaultServeMux.HandleFunc(pattern, handler) } //-------DefaultServeMux.HandleFunc(pattern, handler)-----------\u0026gt; // HandleFunc registers the handler function for the given pattern. func (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { if handler == nil { panic(\u0026#34;http: nil handler\u0026#34;) } mux.Handle(pattern, HandlerFunc(handler)) } //------------HandlerFunc--------------\u0026gt; // The HandlerFunc type is an adapter to allow the use of // ordinary functions as HTTP handlers. If f is a function // with the appropriate signature, HandlerFunc(f) is a // Handler that calls f. type HandlerFunc func(ResponseWriter, *Request) // ServeHTTP calls f(w, r). func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) } Now you know that when develop a web app we have to deal with http requests, and we do this by regisering some patterns (urls) with its handler which is an instance of http.Handler into a ServeMux. We can take advantage offunc HandleFunc(pattern string, handler func(ResponseWriter, *Request)) which helps us create an instance of http.Handler for the pattern.\nServeMux is an HTTP request multiplexer. It matches the URL of each incoming request against a list of registered patterns and calls the handler for the pattern that most closely matches the URL. ServeMux - Go\nKeep in mind that servemux, provided by Go will process each request in a separate goroutine.\n# 3. Creating own ServeMux At part one we create two hello wrold example and we pass nil to ListenAndServe():\n1 2 3 4 func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) _ = http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } We can check the source code:\n1 2 3 4 5 // The handler is typically nil, in which case the DefaultServeMux is used. func ListenAndServe(addr string, handler Handler) error { server := \u0026amp;Server{Addr: addr, Handler: handler} return server.ListenAndServe() } Therefore, we can create our own ServeMux and pass it to ListenAndServe(), and we can register patterns to ServeMux directly which means we don\u0026rsquo;t need to use http.HandleFunc() and ``http.Handle()` to help us register anymore.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 func newServer() *server { s := \u0026amp;server{ router: http.NewServeMux(), } s.routes() return s } // you can put dependencies here, like a db type server struct { router *http.ServeMux } // make our server as a http.Handler // so that we can pass it to ListenAndServe() // don\u0026#39;t hide any logic here, which means don\u0026#39;t write any other code // if you want do something before a request, valide request for example // use a middleware instead, // middleware: https://davidzhu.xyz/post/golang/practice/008-closures-go/#22-creating-middleware // learn more: https://youtu.be/rWBSMsLG8po?si=qwtUTKF3J4EtWRQC // I\u0026#39;ll explain why we call s.router.ServeHTTP(w, r) later func (s *server) ServeHTTP(w http.ResponseWriter, r *http.Request) { s.router.ServeHTTP(w, r) } // register patterns for ServeMux directly func (s *server) routes() { s.router.HandleFunc(\u0026#34;/\u0026#34;, s.handleIndex) s.router.HandleFunc(\u0026#34;/favicon.ico\u0026#34;, s.handleFavicon) } func (s *server) handleFavicon(_ http.ResponseWriter, _ *http.Request) {} func (s *server) handleIndex(w http.ResponseWriter, _ *http.Request) { _, _ = fmt.Fprint(w, \u0026#34;hello there\u0026#34;) } func main() { // As we have talked befor, don\u0026#39;t need to register patterns here // because we have registered patterns to ServeMux directly in routes() method s := newServer() // s is a http.Handler, so we can pass it here log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, s)) } The code above has this:\n1 2 3 func (s *server) ServeHTTP(w http.ResponseWriter, r *http.Request) { s.router.ServeHTTP(w, r) } Recall that ServeHTTP() method of ServeMux is used to dispatches the http requests to the handler function, we pass our server to http.ListenAndServe() which means http package will treat our server as the \u0026ldquo;DefaultServeMux\u0026rdquo;, which means that the ServeHTTP() method of our server will be called to \u0026ldquo;dispatche\u0026rdquo; the http request by the http packet. But as you can see, there is nothing but a call for s.router.ServeHTTP(w, r), yes, http packet calls the ServeHTTP() method of our server to dispatche http request, but actually the ServeHTTP() method of our server call the ServeHTTP() of our ServeMux to dispatche http request.\n1 2 3 4 5 6 7 // ServeHTTP dispatches the request to the handler whose // pattern most closely matches the request URL. func (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) { // mux.Handler() returns the handler to use for the given request h, _ := mux.Handler(r) h.ServeHTTP(w, r) } Actually we can delete the method ServeHTTP of our server and just pass our ServeMux to ListenAndServe():\n1 2 ... log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, s.router)) But this probably looks not as elegant as:\n1 log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, s)) Now you should understand the nature of ServeMux: a router of our web application.\n","date":"2023-09-19T23:49:55Z","permalink":"https://blog.yorforger.cc/p/the-router-of-go-web-app-servemux/","title":"The Router of Go Web App - ServeMux"},{"content":"Previous post: First-Class Functions - Go - David\u0026rsquo;s Blog\n# 1. What is closure Go functions may be closures. A closure is a function value that references variables from outside its body. The function may access and assign to the referenced variables; in this sense the function is \u0026ldquo;bound\u0026rdquo; to the variables.\nClosures - A Tour of Go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func adder() func(int) (int, *int) { sum := 0 return func(x int) (int, *int) { sum += x return sum, \u0026amp;sum } } func main() { pos, neg := adder(), adder() for i := 1; i \u0026lt;= 3; i++ { sum, addr := pos(i) fmt.Println(\u0026#34;pos: \u0026#34;, sum, \u0026#34; \u0026#34;, addr) sum, addr = neg(-i) fmt.Println(\u0026#34;neg: \u0026#34;, sum, \u0026#34; \u0026#34;, addr) } } //-------------------------- pos: 1 0x140000b0018 neg: -1 0x140000b0020 pos: 3 0x140000b0018 neg: -3 0x140000b0020 pos: 6 0x140000b0018 neg: -6 0x140000b0020 As we see above, when the anonymous function defined in the adder() access sum, it looks like accessing a global variable, even the function adder() has returned we still can access variable sum. However, there are \u0026ldquo;two\u0026rdquo; sum actually, they are isolated and each has its own address.\nYou should note that we return an anonymous function here, function is first-class citizen in golang just like javascript, it\u0026rsquo;s a gift don\u0026rsquo;t fear it.\n# 2. Use cases # 2.1. Pass behaviour 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 type Mux struct { mu sync.Mutex conns map[net.Addr]net.Conn } func (m *Mux) Add(conn net.Conn) { m.mu.Lock() defer m.mu.Unlock() m.conns[conn.RemoteAddr()] = conn } func (m *Mux) Remove(addr net.Addr) { m.mu.Lock() defer m.mu.Unlock() delete(m.conns, addr) } func (m *Mux) SendMsg(msg string) error { m.mu.Lock() defer m.mu.Unlock() for _, conn := range m.conns { _, err := io.WriteString(conn, msg) if err != nil { return err } } return nil } Is this what we call idiomatic in Go? Maybe. This is our first proverb don\u0026rsquo;t mediate the access to shared memory with locks and mutexes instead share that memory by communicating.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 type Mux struct { add chan net.Conn remove chan net.Addr sendMsg chan string } func (m *Mux) Add(conn net.Conn) { m.add \u0026lt;- conn } func (m *Mux) Remove(addr net.Addr) { m.remove \u0026lt;- addr } func (m *Mux) SendMsg(msg string) error { m.sendMsg \u0026lt;- msg return nil } func (m *Mux) loop() { // only one goroutine can access this map, // don\u0026#39;t need lock conns := make(map[net.Addr]net.Conn) for { select { case conn := \u0026lt;-m.add: conns[conn.RemoteAddr()] = conn case addr := \u0026lt;-m.remove: delete(conns, addr) case msg := \u0026lt;-m.sendMsg: for _, conn := range conns { _, _ = io.WriteString(conn, msg) } } } } We don\u0026rsquo;t need the mutex anymore because the shared sate this conns map is now local to the loop() function, it can\u0026rsquo;t be mutated by anybody else. There cannot be a (data) race because it only exists within the scope of that function.\nBut there\u0026rsquo;s still a lot of hard-coded logic in here, loop() only knows how to do three things, it only knows how to add, remove and sendMsg. If we wanted to extend our Mux, it would involve three things\nadding a channel adding a helper (function) to send data over that channel and then opening up the select logic inside loop and adding the knowledge of how to process that data But we can rewrite our Mux to use first-class functions to pass the behavior we want to be executed not the data to interpret.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 type Mux struct { ops chan func(map[net.Addr]net.Conn) } func (m *Mux) Add(conn net.Conn) { m.ops \u0026lt;- func(m map[net.Addr]net.Conn) { m[conn.RemoteAddr()] = conn } } func (m *Mux) Remove(addr net.Addr) { m.ops \u0026lt;- func(m map[net.Addr]net.Conn) { delete(m, addr) } } func (m *Mux) SendMsg(msg string) error { m.ops \u0026lt;- func(m map[net.Addr]net.Conn) { for _, conn := range m { _, _ = io.WriteString(conn, msg) } } return nil } func (m *Mux) loop() { conns := make(map[net.Addr]net.Conn) for op := range m.ops { op(conns) } } But there are a few little problems to fix, the lack of error handling inside sendMsg, if there\u0026rsquo;s an error of writing to one of the connections that\u0026rsquo;s just going to be discarded. To handle the error generated inside the anonymous function that we pass into loop we need to create a channel to communicate with the result of the operation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func (m *Mux) SendMsg(msg string) error { result := make(chan error, 1) m.ops \u0026lt;- func(m map[net.Addr]net.Conn) { for _, conn := range m { _, err := io.WriteString(conn, msg) if err != nil { result \u0026lt;- err return } } result \u0026lt;- nil } return \u0026lt;-result } func (m *Mux) PrivateMsg(addr net.Addr, msg string) (int, error) { result := make(chan net.Conn, 1) m.ops \u0026lt;- func(m map[net.Addr]net.Conn) { result \u0026lt;- m[addr] } conn := \u0026lt;-result if conn == nil { return 0, fmt.Errorf(\u0026#34;client %v not registered\u0026#34;, addr) } return io.WriteString(conn, msg) } I was wondering can I just replace the channel with just a error value in sendMsg() method, like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func (m *Mux) SendMsg(msg string) error { var result error m.ops \u0026lt;- func(m map[net.Addr]net.Conn) { for _, conn := range m { _, err := io.WriteString(conn, msg) if err != nil { result = err return } } result = nil } return result } The answer is no, I cann\u0026rsquo;t, the result has to be a channel here. Because the closure defined in SendMsg is sent somewhere else (via the m.ops chan) to get executed - I need to get the result from the channel to block the SendMsg at the last return \u0026lt;-result - the result channel is a synchronization point. source\nFirst class functions let you pass around behaviour, not just dead data that must be interpreted. First class functions aren\u0026rsquo;t new or novel. Like the other features, first class functions should be used with restraint. First class functions are something that every Go programmer should have in their toolbox. First class functions aren\u0026rsquo;t hard, just a little unfamiliar, and that is something that can be overcome with practice. Next time you define an API it has just one method I want you to ask yourself: should this really just be a function.\nDon\u0026rsquo;t overuse channel, learn more: https://davidzhu.xyz/post/golang/advance/008-do-not-overuse-cahnnel\nSource: dotGo 2016 - Dave Cheney - Do not fear first class functions\n# 2.2. Creating middleware Middleware is basically a fancy term for reusable function that can run code both before and after your code designed to handle a web requst. In Go these are typically accomplished with closures, but in different programming languages they may be achieved in other ways.\nMiddleware is very helpful in scenarios where we need to perform common tasks on incoming HTTP requests, such as authentication, authorization, request validation, and logging. Middleware allows us to apply these tasks consistently across our application, reducing code duplication and making it easier to maintain and modify our code.\nThere are several middleware functions that are commonly used in web applications, including:\nAuthentication middleware: This middleware checks whether the user is authenticated and authorized to access the requested resource. Logging middleware: This middleware logs information about incoming requests, including request method, URL, headers, and response status. Error handling middleware: This middleware catches errors that occur during request handling and returns an appropriate error response to the client. Request validation middleware: This middleware validates incoming requests to ensure that they meet certain criteria, such as HTTP method, headers, query parameters, and request body content. Caching middleware: This middleware caches responses to certain requests to improve performance and reduce server load. Request Tracing: This middleware is used to trace the path of a request through a web application. It captures relevant information about the request and logs it for monitoring and debugging purposes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func main() { http.HandleFunc(\u0026#34;/hello\u0026#34;, timed(hello)) http.ListenAndServe(\u0026#34;:3000\u0026#34;, nil) } func timed(f func(http.ResponseWriter, *http.Request)) func(http.ResponseWriter, *http.Request) { return func(w http.ResponseWriter, r *http.Request) { start := time.Now() f(w, r) end := time.Now() fmt.Println(\u0026#34;The request took\u0026#34;, end.Sub(start)) } } func hello(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026#34;\u0026lt;h1\u0026gt;Hello!\u0026lt;/h1\u0026gt;\u0026#34;) } Notice that our timed() function takes in a function that could be used as a handler function, and returns a function of the same type, but the returned function is different that the one passed it. The closure being returned logs the current time, calls the original function, and finally logs the end time and prints out the duration of the request. All while being agnostic to what is actually happening inside of our handler function.\nNow all we need to do to time our handlers is to wrap them in timed(handler) and pass the closure to the http.HandleFunc() function call.\nReferences:\n5 Useful Ways to Use Closures in Go - Calhoun.io Mastering Middleware in Go: Tips, Tricks, and Real-World Use Cases | by ansu jain | Medium Function Closures - A Tour of Go ","date":"2023-09-18T23:32:55Z","permalink":"https://blog.yorforger.cc/p/function-closures-go/","title":"Function Closures - Go"},{"content":"In Git, there are two main ways to integrate changes from one branch into another: the merge and the rebase.\n# 1. fast-forward vs three-way merge There are two type of merge:\nthree-way merge, happens when the branches\u0026rsquo; commit have diverged will result a merge commit fast-forward merge, happens when there is a liner structure in the commit history we usually prevent fast-forward merge, because there is no merge commit. can be suppressed with the --no-ff option The liner structure commit structure looks like this:\n1 2 3 A---B fixbug / ...--E master Branches\u0026rsquo; diverged commit structure looks like this:\n1 2 3 A---B fixbug / ...--E--F master Video: https://youtu.be/zOnwgxiC0OA\n# 2. git rebase The Golden Rule of Rebasing reads: “Never rebase while you’re on a public branch.”\nSource: https://www.gitkraken.com/blog/golden-rule-of-rebasing-in-git\nOn main branch:\n1 2 3 4 5 * commit 41bc8e89e60d75e21e034aaabcbd20103a61fca4 (HEAD -\u0026gt; main) | main: Fri 15 Sep 2023 13:01:02 ADT | * commit 2220b67f9c8ccf5f47e51bff7bd3a3fca6e141b6 main: Fri 15 Sep 2023 11:47:54 ADT On fixbug branch:\n1 2 3 4 5 6 7 8 * commit 0096a2dfa2b4e4c40011213b6cce12ee73833ca9 (HEAD -\u0026gt; fixbug) | fixbug: Fri 15 Sep 2023 11:48:53 ADT | * commit fd900fa0c05632757f45fd6fee236eb84c99bb94 | fixbug: Fri 15 Sep 2023 11:48:27 ADT | * commit 2220b67f9c8ccf5f47e51bff7bd3a3fca6e141b6 main: Fri 15 Sep 2023 11:47:54 AD Now the commit history of all branches looks like this:\n1 2 3 A---B fixbug / ...--E---F main Go to main branch and make rebase.\n1 2 3 $ git switch main $ git rebase fixbug Successfully rebased and updated refs/heads/main. Check the log, you can see, these two branches combined but the commit histroy structure is linear:\n1 2 3 4 5 6 7 8 9 10 11 * commit 918260ae4f0b7f7d573a9a640ce380ea0f861a6a (HEAD -\u0026gt; main) | main: Fri 15 Sep 2023 13:01:02 ADT | * commit 0096a2dfa2b4e4c40011213b6cce12ee73833ca9 (fixbug) | fixbug: Fri 15 Sep 2023 11:48:53 ADT | * commit fd900fa0c05632757f45fd6fee236eb84c99bb94 | fixbug: Fri 15 Sep 2023 11:48:27 ADT | * commit 2220b67f9c8ccf5f47e51bff7bd3a3fca6e141b6 main: Fri 15 Sep 2023 11:47:54 ADT Now the commit history of main branch looks like this (and you can delete fixbug branch):\n1 2 3 A---B---F\u0026#39; main / ...--E As you can see, rebase just make a fast-forward merge happen even the branches has diverged.\nAnd note that the F' mean it\u0026rsquo;s a copy of original commit F (git will do this internally), this is why we should not use rebase on a shared branch, main for example, because rebase will \u0026ldquo;change\u0026rdquo; the commit history of branch main.\n# 3. git rebase vs merge Merge preserves commit history. Use merge on a public branch. Rebase rewrites (makes copy) history. Use rebase on a private branch to catch up update form remote. Why rebase rewrite commit history: https://youtu.be/zOnwgxiC0OA?si=CwbvoPI35pHgJ1pn\u0026t=401 git push --force on shared branches is an absolute no-no. Note that we say use rebase on a private branch means we can use the commandgit rebase master on a private, please don\u0026rsquo;t use git rebase fixissue on master branch which apparently is a publick branch.\nLearn more: https://youtu.be/zOnwgxiC0OA?si=CwbvoPI35pHgJ1pn\u0026t=401\nReferences:\nLearn Git Rebase in 6 minutes // explained with live animations! Git MERGE vs REBASE: The Definitive Guide Git MERGE vs REBASE ","date":"2023-09-15T10:20:23Z","permalink":"https://blog.yorforger.cc/p/git-merge-vs-git-rebase/","title":"git merge vs git rebase"},{"content":" # 1. Three trees Git as a system manages and manipulates three trees in its normal operation:\nTree Role HEAD Last commit snapshot, next parent Index Proposed next commit snapshot Working Directory Sandbox The Git index is a critical data structure in Git. It serves as the “staging area” between the files you have on your filesystem and your commit history. When you run git add, the files from your working directory are hashed and stored as objects in the index, leading them to be “staged changes”. When you run git commit, the staged changes as stored in the index are used to create that new commit.\n# 2. HEAD # 2.1. HEAD is YOU HEADis a symbolic reference pointing to wherever you are in your commit history. It follows you wherever you go, whatever you do, like a shadow. If you make a commit, HEAD will move. If you checkout something, HEAD will move. Whatever you do, if you have moved somewhere new in your commit history, HEAD has moved along with you. To address one common misconception: you cannot detach yourself from HEAD. That is not what a detached HEAD state is. If you ever find yourself thinking: \u0026ldquo;oh no, i\u0026rsquo;m in detached HEAD state! I\u0026rsquo;ve lost my HEAD!\u0026rdquo; Remember, it\u0026rsquo;s your HEAD. HEAD is you. You haven\u0026rsquo;t detached from the HEAD, you and your HEAD have detached from something else.\nAcutally, HEAD is just a special pointer that points to the local branch you’re currently on.\n1 2 $ cat .git/HEAD ref: refs/heads/hugo-blog Sometimes you\u0026rsquo;ll get something like this:\n1 a3c485d9688e3c6bc14b06ca1529f0e78edd3f86 That\u0026rsquo;s what happens when HEAD points directly to a commit. This is called a detached HEAD, because HEAD is pointing to something other than a branch reference.\nIn Git, HEAD refers to the currently checked-out branch’s latest commit. However, in a detached HEAD state, the HEAD does not point to any branch, but instead points to a specific commit or the remote repository.\n# 2.2. What can HEAD attach to? HEAD can point to a commit, yes, but typically it does not. Let me say that again. Typically HEAD does not point to a commit. It points to a branch reference. It is attached to that branch, and when you do certain things (e.g., commit or reset), the attached branch will move along with HEAD.\nHEAD is you. It points to whatever you checked out, wherever you are. Typically that is not a commit, it is a local branch. If HEAD does point to a commit (or tag), even if it\u0026rsquo;s the same commit (or tag) that a branch also points to, you (and HEAD) have been detached from that branch. Since you don\u0026rsquo;t have a branch attached to you, the branch won\u0026rsquo;t follow along with you as you make new commits. The HEAD, however, will.\nSource:\nhttps://stackoverflow.com/a/54935492/16317008\nhttps://circleci.com/blog/git-detached-head-state/\nhttps://youtu.be/GN36mrrM12k?si=S6_VTBQDZgG_fHre\n","date":"2023-09-15T10:18:30Z","permalink":"https://blog.yorforger.cc/p/head-and-three-trees-git/","title":"HEAD and Three Trees - Git"},{"content":" # 1. git push -f is dangerous The commit history on my local and remote repo are same:\n1 2 second commit first commit Then I added a file docs.md driectly on github and make a commit so there is another new commit history on remote repo which is different form the commit history of my local repo.\nCommit history on remote repo:\n1 2 3 create docs.md second commit first commit Files on remote repo:\n1 2 3 README.md main.c docs.md Then I made some modifications on local repo and make a commit.\nCommit history on local repo:\n1 2 3 third commit second commit first commit Files on local repo:\n1 2 README.md main.c Now when I try to push my local changes to github:\n1 2 3 4 5 6 7 8 9 $ git push origin master To github.com:shwezhu/test_repo.git ! [rejected] master -\u0026gt; master (fetch first) error: failed to push some refs to \u0026#39;github.com:shwezhu/test_repo.git\u0026#39; hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: (e.g., \u0026#39;git pull ...\u0026#39;) before pushing again. hint: See the \u0026#39;Note about fast-forwards\u0026#39; in \u0026#39;git push --help\u0026#39; for details. This happens because the commit history of the master branch in local and remote repo are different.\nOne way to slolve this is to fetch the remote and merge the changes (commit history will be merged too), then push will succeed. Learn more: Push on Github get Rejected - Solved - David\u0026rsquo;s Blog\nAnother way is to use git push -f but this will overwirte your remote repo, which means **the commit history and the work place on remote rep will be overwritten **.\nAfter use git push -f, the commit history and workplace of the remote repo will looks exactly like the local:\n1 2 3 third commit second commit first commit Noew the file docs.md will miss which means git push -f is dangerous.\n# 2. –force-with-lease Git’s push --force is destructive because it unconditionally overwrites the remote repository with whatever you have locally, possibly overwriting any changes that a team member has pushed in the meantime. However there is a better way; the option –force-with-lease can help when you do need to do a forced push but still ensure you don’t overwrite other’s work.\n--force on shared branches is an absolute no-no.\nLearn more: -force considered harmful; understanding git\u0026rsquo;s -force-with-lease - Atlassian Developer Blog\n","date":"2023-09-14T10:00:23Z","permalink":"https://blog.yorforger.cc/p/git-push-with-force/","title":"Git Push with force"},{"content":" # 1. Structure # 2. Capture filter # 2.1. Selcet netowk interface en0 Physical network interface utun0~4 VIrtual netwrok interface used for tunneling, learn more: Working with TUN Device on MacOS - David\u0026rsquo;s Blog loopback interface: 127.0.0.1 When you run a echo server and client on your local machine, you should select the loopback interface, not the en0 interface. # 2.2. Specify filter rules Filter by port and protocol:\n1 port 9000 Wireshark can only capture some specific ports that for HTTP package by default, so if you gonna capture HTTP package, make sure use the correct ports or goto settings to change the default ports. If you ignore this, like to capture HTTP on port 9000, you probably jut get TCP package.\nYou can find the allowed HTTP port on Preferences-\u0026gt;Protocols-\u0026gt;HTTP\nLearn more: CaptureFilters - Wireshark\n# 3. Display filter Learn more: DisplayFilters\n# 4. Practical examples Client:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 func main() { // Three-way handshake included this step // Note that we\u0026#39;re connecting to port 9000 on the server, // not use port 9000 to connect the server. conn, err := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;:9000\u0026#34;) if err != nil { log.Fatalf(\u0026#34;couldn\u0026#39;t connect to the server: %v\u0026#34;, err) } buf := make([]byte, 15) // send data to server, the data will be copied into kernel space // and encapsulated into tcp segment -\u0026gt; ip packet -\u0026gt; ethernet frame if _, err := conn.Write([]byte(\u0026#34;Hi, I am Coco\\n\u0026#34;)); err != nil { log.Fatalf(\u0026#34;couldn\u0026#39;t send request: %v\u0026#34;, err) } else { // Read data from server, the data are copied from kernel space // what happens in kernel space (network stack): // ethernet frame -\u0026gt; ip packet -\u0026gt; tcp segment // the data will be forwarded to this program. // If the connection is closed, return error: io.EOF _, err = conn.Read(buf) if err != nil { log.Fatalf(\u0026#34;couldn\u0026#39;t read server response: %v\u0026#34;, err) } fmt.Println(string(buf)) } _ = conn.Close() } Server:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 func main() { // Obtain the port port := fmt.Sprintf(\u0026#34;:%s\u0026#34;, os.Args[1]) // Create a tcp listener on the given port listener, err := net.Listen(\u0026#34;tcp\u0026#34;, port) if err != nil { fmt.Println(\u0026#34;failed to create listener, err:\u0026#34;, err) os.Exit(1) } fmt.Printf(\u0026#34;listening on %s\\n\u0026#34;, listener.Addr()) // listen for new connections for { // Three-way handshake included // this connection will be assigned a new port (different from the port this server is listening) // for sending and receiving data conn, err := listener.Accept() if err != nil { fmt.Println(\u0026#34;failed to accept connection, err:\u0026#34;, err) continue } // Pass an accepted connection to a handler goroutine go handleConnection(conn) } } func handleConnection(conn net.Conn) { defer conn.Close() buf := make([]byte, 15) for { // read client request data, same as client side // if the connection is closed, return error: io.EOF _, err := conn.Read(buf) if err != nil { if err != io.EOF { log.Println(\u0026#34;failed to read data, err:\u0026#34;, err) } fmt.Println(\u0026#34;connection closed by client:\u0026#34;, conn.RemoteAddr()) return } fmt.Printf(\u0026#34;request: %s\u0026#34;, buf) line := fmt.Sprintf(\u0026#34;%s\u0026#34;, buf) // Same as on client side _, _ = conn.Write([]byte(line)) } } Run server and client:\n1 2 3 4 # server $ go run main.go 9000 # client $ go run main.go Wireshark:\nThe first three is the three-way handshake packet,\n1 2 3 [SYN] seq=0 len=0 [SYN] seq=0 ack=1 len=0 [SYN] seq=1 ack=1 len=0 Note that the length of the fifth packet:\n1 2 # ACK sent with data [PSH, ACK] ... len=14 len is the data\u0026rsquo;s length, the first there packets are just three-way handshake there is no data sent, so len=0.\nThe last four packets are TCP termination four-way hand-shake:\n1 2 3 4 5 # ACK sent with FIN, this ACK is used to # ack=16 confirms that client has received the 15 byte sent by server # and client expects seq=16 from server [FIN, ACK] seq=15 ack=16 len=0 [ACK] seq=16 ack=16 The TCP ACK flag is used to confirm the last received byte by receiver.\nThe PSH flag, on the other hand, is used to tell the server to push data to the application layer immediately.\n","date":"2023-09-13T16:52:30Z","permalink":"https://blog.yorforger.cc/p/wireshark-basics/","title":"Wireshark Basics"},{"content":" # 1. TUN on MacOS On macOS, the utun interface is a type of TUN device specifically designed for VPN connections to handle the network traffic within the VPN tunnel regardless of whether VPN is enabled.\nI\u0026rsquo;ll give you an example to demonstrate the realtionship between TUN device and utun interface, the code below written in Go is to create a TUN device:\n1 2 // New() creates a new TUN/TAP interface using config. ifce, err := water.New(water.Config{DeviceType: water.TUN}) Before run this code there are 4 utun interfaces on my mac:\n1 2 3 4 5 6 7 8 $ ifconfig utun0: flags=8051\u0026lt;UP,POINTOPOINT,RUNNING,MULTICAST\u0026gt; mtu 1380 inet6 fe80::652e:88dd:ddb0:ad93%utun0 prefixlen 64 scopeid 0xf nd6 options=201\u0026lt;PERFORMNUD,DAD\u0026gt; ... utun4: flags=8051\u0026lt;UP,POINTOPOINT,RUNNING,MULTICAST\u0026gt; mtu 1380 inet6 fe80::e305:5ba8:574a:a5ac%utun4 prefixlen 64 scopeid 0x13 nd6 options=201\u0026lt;PERFORMNUD,DAD\u0026gt; After I run the Go codes above with sudo, there are 5 utun interfaces but with no ip information:\n1 2 3 4 5 6 $ ifconfig ... utun4: flags=8051\u0026lt;UP,POINTOPOINT,RUNNING,MULTICAST\u0026gt; mtu 1380 inet6 fe80::e305:5ba8:574a:a5ac%utun4 prefixlen 64 scopeid 0x13 nd6 options=201\u0026lt;PERFORMNUD,DAD\u0026gt; utun5: flags=8051\u0026lt;UP,POINTOPOINT,RUNNING,MULTICAST\u0026gt; mtu 1500 utun* is a point-to-point interface, also called a tunnel or a peer-to-peer interface, It doesn\u0026rsquo;t behave like \u0026ldquo;shared medium\u0026rdquo; interfaces such as Wi-Fi or Ethernet, which connect you to multiple devices. Instead, it behaves like a cable that just has hosts on both ends.\nThere are no layer-2 headers, no MAC addresses, and no ARP on a point-to-point interface, because everything sent through it reaches the same destination (the \u0026ldquo;peer\u0026rdquo; host).\nSource: https://superuser.com/a/1446061/1689666\n# 2. utun is an instance of TUN device You can think utun* is an instance of TUN device on Mac, a TUN device can have many instances.\nOS treat virtual network interface (tun/tap devices) as same to the physical network interface, which means virtual network interface can have anything (including ip address) that physical network interface have.\nutun* is just a network interface similar to en0, lo0, when you input ifconfig command, they will listed together:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 $ ifconfig lo0: flags=8049\u0026lt;UP,LOOPBACK,RUNNING,MULTICAST\u0026gt; mtu 16384 options=1203\u0026lt;RXCSUM,TXCSUM,TXSTATUS,SW_TIMESTAMP\u0026gt; inet 127.0.0.1 netmask 0xff000000 inet6 ::1 prefixlen 128 inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 nd6 options=201\u0026lt;PERFORMNUD,DAD\u0026gt; en0: flags=8863\u0026lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST\u0026gt; mtu 1500 options=6463\u0026lt;RXCSUM,TXCSUM,TSO4,TSO6,CHANNEL_IO,PARTIAL_CSUM,ZEROINVERT_CSUM\u0026gt; ether d4:57:63:da:b6:98 inet6 fe80::475:ca98:8ecc:d86%en0 prefixlen 64 secured scopeid 0xc inet 192.168.2.15 netmask 0xffffff00 broadcast 192.168.2.255 inet6 fdd0:ed77:f347:4d69:859:d993:f358:5af9 prefixlen 64 autoconf secured nd6 options=201\u0026lt;PERFORMNUD,DAD\u0026gt; media: autoselect status: active en1: flags=8963\u0026lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST\u0026gt; mtu 1500 options=460\u0026lt;TSO4,TSO6,CHANNEL_IO\u0026gt; ether 36:6b:75:95:04:c0 media: autoselect \u0026lt;full-duplex\u0026gt; status: inactive utun0: flags=8051\u0026lt;UP,POINTOPOINT,RUNNING,MULTICAST\u0026gt; mtu 1380 inet6 fe80::652e:88dd:ddb0:ad93%utun0 prefixlen 64 scopeid 0xf nd6 options=201\u0026lt;PERFORMNUD,DAD\u0026gt; utun1: flags=8051\u0026lt;UP,POINTOPOINT,RUNNING,MULTICAST\u0026gt; mtu 2000 inet6 fe80::a13f:9a63:f8cb:4017%utun1 prefixlen 64 scopeid 0x10 nd6 options=201\u0026lt;PERFORMNUD,DAD\u0026gt; ... Find more about what these interface are: https://stackoverflow.com/a/55232331/16317008\ne.g.,\n1 2 utun3: flags=8051\u0026lt;UP,POINTOPOINT,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 10.8.0.18 --\u0026gt; 10.8.0.17 netmask 0xffffffff With \u0026ldquo;normal\u0026rdquo; interfaces, configuring an address with subnet mask like 192.168.1.3/24 on eth0 is really just shorthand for saying \u0026ldquo;My address is 192.168.1.3 and I also have an on-link route 192.168.1.0/24 dev eth0\u0026rdquo;. The on-link route is derived from combining the address \u0026amp; subnet mask.\nWith point-to-point interfaces, it\u0026rsquo;s actually the same idea. This example means \u0026ldquo;My address is 10.8.0.18 and I also have an on-link route 10.8.0.17/32 dev utun3.\u0026rdquo; In this case the autogenerated route is a /32, indicating only one host – the \u0026ldquo;peer\u0026rdquo;.\n(Note: My examples use Linux iproute2-style syntax.) So in the end, the difference between 10.8.0.17 netmask 0xffffffff and 10.8.0.17/32 styles is just that automatic route.\nSource: https://superuser.com/a/1446061/1689666\n# 3. Set up ip for utun interface 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ tldr ifconfig - View network settings of an Ethernet adapter: ifconfig eth0 - Display details of all interfaces, including disabled interfaces: ifconfig -a - Disable eth0 interface: ifconfig eth0 down - Enable eth0 interface: ifconfig eth0 up - Assign IP address to eth0 interface: ifconfig eth0 ip_address For example, if you have two machines, one we label \u0026ldquo;local\u0026rdquo; with a LAN IP address like 192.168.0.12 and another we label \u0026ldquo;remote\u0026rdquo; with a LAN IP address like 192.168.1.14, you can assign tunnel IP addresses thusly:\n1 ifconfig tun0 inet 10.0.0.1 10.0.0.2 up on the local system, and:\n1 ifconfig tun0 inet 10.0.0.2 10.0.0.1 up on the remote system. Note the reversed perspective on the remote machine. Do not set your point to point addresses to anything on an existing subnet; it will not route properly.\nNote, if you set a wrong interface, you can cancle it with sudo ifconfig utun2 delete 10.1.0.10 10.1.0.20 or ifconfig en1 delete 192.168.141.99 for differnt types of network interfaces.\nSource: https://stackoverflow.com/a/17511998/16317008\n# 4. Use TUN capture ip packets with Go on MacOS 1 go get -u github.com/songgao/water 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func main() { ifce, err := water.New(water.Config{DeviceType: water.TUN}) if err != nil { log.Fatal(err) } log.Printf(\u0026#34;Interface Name: %s\\n\u0026#34;, ifce.Name()) packet := make([]byte, 1500) for { n, err := ifce.Read(packet) if err != nil { log.Fatal(err) } log.Printf(\u0026#34;Packet Received: % x\\n\u0026#34;, packet[:n]) } } 1 2 # NOTE: replace utunx with the name printed on your go code above $ sudo ifconfig utun5 10.1.0.10 10.1.0.20 up Then :\n1 ping -c 1 10.1.0.20 If no data printed on your go codes, restart your go codes and change a pair of ip addresses for utun interface.\nLearn more:\nsonggao/water: A simple TUN/TAP library written in native Go. TUN Device \u0026amp; utun Interface ","date":"2023-09-12T08:31:59Z","permalink":"https://blog.yorforger.cc/p/working-with-tun-device-on-macos/","title":"Working with TUN Device on MacOS"},{"content":"原文:\n云原生虚拟网络 tun/tap \u0026amp; veth-pair - luozhiyun`s Blog 理解 Linux 虚拟网卡设备 tun/tap 的一切 | 骏马金龙 之前的相关文章:\nTUN Device \u0026amp; utun Interface - David\u0026rsquo;s Blog Tunneling Protocols - David\u0026rsquo;s Blog # 1. 物理网卡和虚拟网卡 # 1.1. 物理网卡收发数据的流程 物理网卡可以接收和发送数据：\n收：外界向该物理网卡发送数据时，外界发送到网卡的数据最终会传输到内核空间的网络协议栈中 发：本机要从物理网卡发送数据时，数据将从内核的网络协议栈传输到网卡，网卡负责将数据发送出去 现在的网卡具备 DMA 能力，所以网卡和网络协议栈之间的数据传输由网卡负责，而非由内核亲自占用 CPU 来执行读和写 一般来说，数据的起点和终点是用户程序，所以多数时候的数据需要在用户空间和内核空间 (网络协议栈) 再传输一次：\n当用户进程的数据要发送出去时，数据从用户空间写入内核的网络协议栈，再从网络协议栈传输到网卡，最后发送出去 当用户进程等待外界响应数据时，数据从网卡流入，传输至内核的网络协议栈，最后数据写入用户空间被用户进程读取 在这些过程中，内核和用户空间的数据传输由内核占用 CPU 来完成，内核和网卡之间的数据传输由网卡的 DMA 来完成，不需要占用过多的 CPU。\n# 1.2. 虚拟网卡和物理网卡的对比 和物理网卡对比一下，物理网卡是硬件网卡，它位于硬件层，虚拟网卡则可以看作是用户空间的网卡，就像用户空间的文件系统 (fuse) 一样。\n物理网卡和虚拟网卡唯一的不同点在于物理网卡本身的硬件功能：物理网卡以比特流的方式传输数据。\n也就是说，内核会公平对待物理网卡和虚拟网卡，物理网卡能做的配置，虚拟网卡也能做。比如可以为虚拟网卡接口配置 IP 地址、设置子网掩码，可以将虚拟网卡接入网桥等等。\n只有在数据流经物理网卡和虚拟网卡的那一刻，才会体现出它们的不同，即传输数据的方式不同：物理网卡以比特流的方式传输数据，虚拟网卡则直接在内存中拷贝数据 (即，在内核之间和读写虚拟网卡的程序之间传输)。\n正因为虚拟网卡不具备物理网卡以比特流方式传输数据的硬件功能，所以，绝不可能通过虚拟网卡向外界发送数据，外界数据也不可能直接发送到虚拟网卡上。能够直接收发外界数据的，只能是物理设备。\n虽然虚拟网卡无法将数据传输到外界网络，但却：\n可以将数据传输到本机的另一个网卡 (虚拟网卡或物理网卡) 或其它虚拟设备 (如虚拟交换机) 上 可以在用户空间运行一个可读写虚拟网卡的程序，该程序可将流经虚拟网卡的数据包进行处理，这个用户程序就像是物理网卡的硬件功能一样，可以收发数据 (可将物理网卡的硬件功能看作是嵌入在网卡上的程序)，比如 OpenVPN 就是这样的工具 很多人会误解这样的用户空间程序，认为它们可以对数据进行封装。比如认为 OpenVPN 可以在数据包的基础上再封装一层隧道 IP 首部，但这种理解是错的。\n一定请注意，用户空间的程序是无法对数据包做任何封装和解封操作的，所有的封装和解封都只能由内核的网络协议栈来完成。\n使用 OpenVPN 之所以可以对数据再封装一层隧道 IP 层，是因为 OpenVPN 可以读取已经封装过一次 IP 首部的数据，并将包含 ip 首部的数据作为普通数据通过虚拟网卡再次传输给内核。因为内核接收到的是来自虚拟网卡的数据，所以内核会将其当作普通数据从头开始封装 (从四层封装到二层封装)。当数据从网络协议栈流出时，就有了两层 IP 首部的封装。\n换句话说，每一次看似由用户空间程序进行的额外封装，都意味着数据要从内核空间到用户空间，再到内核空间。以 OpenVPN 为例：\n1 tcp/ip stack --\u0026gt; tun --\u0026gt; OpenVPN --\u0026gt; tcp/ip stack --\u0026gt; Phyical NIC 其中 tun 是 OpenVPN 创建的一个三层虚拟网卡，tun 设备在用户空间和内核空间之间传递数据。\n具体的 openvpn 数据封装和数据流向的细节，参考更详细的通过 openvpn 分析 tun 实现隧道的数据流程。\n# 1.3. 程序写入虚拟网卡时的注意事项 用户空间的程序不可随意向虚拟网卡写入数据，因为写入虚拟网卡的这些数据都会被内核网络协议栈进行解封处理，就像来自物理网卡的数据都会被解封一样。\n因此，如果用户空间程序要写 tun/tap 设备，所写入的数据需具有特殊结构：\n要么是已经封装了 PORT 的数据，即传输层的 tcp 数据段或 udp 数据报 要么是已经封装了 IP+PORT 的数据，即 ip 层数据包 要么是已经封装了 IP+PORT+MAC 的数据，即链路层数据帧 要么是其它符合 tcp/ip 协议栈的数据，比如二层的 PPP 点对点数据，比如三层的 icmp 协议数据 也就是说，程序只能向虚拟网卡写入已经封装过的数据。\n由于网络数据的封装都由内核的网络协议栈负责，所以程序写入虚拟网卡的数据实际上都原封不动地来自于上一轮的网络协议栈，用户空间程序无法对这部分数据做任何修改。\n也就是说，这时写虚拟网卡的用户空间程序仅充当了一个特殊的【转发】程序：要么转发四层 tcp/udp 数据，要么转发三层数据包，要么转发二层数据帧。\n这一段话可能不好理解，下面给个简单的示例分析。\n假如物理网卡 eth0 从外界网络接收了这么一段特殊的 ping 请求数据：\n这份数据会从物理网卡传输到内核网络协议栈，网络协议栈会对其解封，解封的内容只能是 tcp/ip 协议栈中的内容，即只能解封帧头部、IP 头部以及端口头部，网络协议栈解封后还剩下一段包含了内层 IP 头部 (tun 的 IP) 以及 icmp 请求的数据。\n内核会根据刚才解封的端口号找到对应的服务进程，并将解封剩下的数据传输给该进程，即传输给用户空间的程序。\n用户空间的程序不做任何修改地将读取到的包含了内层 IP 头部和 ICMP 请求的数据原封不动地写入虚拟网卡设备，内核从虚拟网卡接收到数据后，将数据进行解封，解封得到最终的 icmp 请求数据，于是内核开始构建用于响应 ping 请求的数据。\n# 2. 概述 目前主流的虚拟网卡方案有tun/tap和veth两种。在时间上 tun/tap 出现得更早，在 Linux Kernel 2.4 版之后发布的内核都会默认编译 tun/tap 的驱动。并且 tun/tap 应用非常广泛，其中云原生虚拟网络中， flannel 的 UDP 模式中的 flannel0 就是一个 tun 设备，OpenVPN 也利用到了 tun/tap 进行数据的转发。\nveth 是另一种主流的虚拟网卡方案，在 Linux Kernel 2.6 版本，Linux 开始支持网络名空间隔离的同时，也提供了专门的虚拟以太网（Virtual Ethernet，习惯简写做 veth）让两个隔离的网络名称空间之间可以互相通信。veth 实际上不是一个设备，而是一对设备，因而也常被称作 Veth-Pair。\nDocker 中的 Bridge 模式就是依靠 veth-pair 连接到 docker0 网桥上与宿主机乃至外界的其他机器通信的。\n# 3. tun/tap tun 和 tap 是两个相对独立的虚拟网络设备，它们作为虚拟网卡，除了不具备物理网卡的硬件功能外，它们和物理网卡的功能是一样的，此外tun/tap负责在内核网络协议栈和用户空间之间传输数据。\ntun 设备是一个三层网络层设备，从 /dev/net/tun 字符设备上读取的是 IP 数据包，写入的也只能是 IP 数据包，因此常用于一些点对点IP隧道，例如OpenVPN，IPSec等； tap 设备是二层链路层设备，等同于一个以太网设备，从 /dev/tap0 字符设备上读取 MAC 层数据帧，写入的也只能是 MAC 层数据帧，因此常用来作为虚拟机模拟网卡使用； 从上面图中，我们可以看出物理网卡和 tun/tap 设备模拟的虚拟网卡的区别，虽然它们的一端都是连着网络协议栈，但是物理网卡另一端连接的是物理网络，而 tun/tap 设备另一端连接的是一个文件作为传输通道。\n根据前面的介绍，我们大约知道虚拟网卡主要有两个功能，一个是连接其它设备（虚拟网卡或物理网卡）和 Bridge 这是 tap 设备的作用；另一个是提供用户空间程序去收发虚拟网卡上的数据，这是 tun 设备的作用。\n主要区别是因为它们作用在不同的网络协议层，换句话说 tap设备是一个二层设备所以通常接入到 Bridge上作为局域网的一个节点，tun设备是一个三层设备通常用来实现 vpn。\n# 4. OpenVPN 使用 tun 设备收发数据 OpenVPN 是使用 tun 设备的常见例子，它可以方便的在不同网络访问场所之间搭建类似于局域网的专用网络通道。其核心机制就是在 OpenVPN 服务器和客户端所在的计算机上都安装一个 tun 设备，通过其虚拟 IP 实现相互访问。\n例如公网上的两个主机节点A、B，物理网卡上配置的IP分别是 ipA_eth0 和 ipB_eth0。然后在A、B两个节点上分别运行 openvpn 的客户端和服务端，它们会在自己的节点上创建 tun 设备，且都会读取或写入这个 tun 设备。\n假设这两个设备对应的虚拟 IP 是 ipA_tun0 和 ipB_tun0，那么节点 B 上面的应用程序想要通过虚拟 IP 对节点 A 通信，那么数据包流向就是：\n用户进程对 ipA_tun0 发起请求，经过路由决策后内核将数据从网络协议栈写入 tun0 设备；然后 OpenVPN 从字符设备文件中读取 tun0 设备数据，将数据请求发出去；内核网络协议栈根据路由决策将数据从本机的 eth0 接口流出发往 ipA_eth0 。\n同样我们来看看节点 A 是如何接受数据：\n当节点A 通过物理网卡 eth0 接受到数据后会将写入内核网络协议栈，因为目标端口号是OpenVPN程序所监听的，所以网络协议栈会将数据交给 OpenVPN ；\nOpenVPN 程序得到数据之后，发现目标IP是tun0设备的，于是将数据从用户空间写入到字符设备文件中，然后 tun0 设备会将数据写入到协议栈中，网络协议栈最终将数据转发给应用进程。\n从上面我们知道使用 tun/tap 设备传输数据需要经过两次协议栈，不可避免地会有一定的性能损耗，如果条件允许，容器对容器的直接通信并不会把 tun/tap 作为首选方案，一般是基于稍后介绍的 veth 来实现的。但是 tun/tap 没有 veth 那样要求设备成对出现、数据要原样传输的限制，数据包到用户态程序后，程序员就有完全掌控的权力，要进行哪些修改，要发送到什么地方，都可以编写代码去实现，因此 tun/tap 方案比起 veth 方案有更广泛的适用范围。\n# 5. flannel UDP 模式使用 tun 设备收发数据 早期 flannel 利用 tun 设备实现了 UDP 模式下的跨主网络相互访问，实际上原理和上面的 OpenVPN 是差不多的。\n在 flannel 中 flannel0 是一个三层的 tun 设备，用作在操作系统内核和用户应用程序之间传递 IP 包。当操作系统将一个 IP 包发送给 flannel0 设备之后，flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 flanneld 进程，flanneld 进程是一个 UDP 进程，负责处理 flannel0 发送过来的数据包：\nflanneld 进程会根据目的 IP 的地址匹配到对应的子网，从 Etcd 中找到这个子网对应的宿主机 Node2 的 IP 地址，然后将这个数据包直接封装在 UDP 包里面，然后发送给 Node 2。由于每台宿主机上的 flanneld 都监听着一个 8285 端口，所以 Node2 机器上 flanneld 进程会从 8285 端口获取到传过来的数据，解析出封装在里面的发给 ContainerA 的 IP 地址。\nflanneld 会直接把这个 IP 包发送给它所管理的 TUN 设备，即 flannel0 设备。然后网络栈会将这个数据包根据路由发送到 docker0 网桥，docker0 网桥会扮演二层交换机的角色，将数据包发送给正确的端口，进而通过 veth pair 设备进入到 containerA 的 Network Namespace 里。\n上面所讲的 Flannel UDP 模式现在已经废弃，原因就是因为它经过三次用户态与内核态之间的数据拷贝。容器发送数据包经过 docker0 网桥进入内核态一次；数据包由 flannel0 设备进入到 flanneld 进程又一次；第三次是 flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去。\n# 6. tap 设备作为虚拟机网卡 上面我们也说了，tap 设备是一个二层链路层设备，通常用作实现虚拟网卡。以 qemu-kvm 为例，它利用 tap 设备和 Bridge 配合使用拥有极大的灵活性，可以实现各种各样的网络拓扑。\n在 qume-kvm 开启 tap 模式之后，在启动的时候会向内核注册了一个tap类型虚拟网卡 tapx，x 代表依次递增的数字； 这个虚拟网卡 tapx 是绑定在 Bridge 上面的，是它上面的一个接口，最终数据会通过 Bridge 来进行转发。\nqume-kvm 会通过其网卡 eth0 向外发送数据，从 Host 角度看就是用户层程序 qume-kvm 进程将字符设备写入数据；然后 tapx 设备会收到数据后由 Bridge 决定数据包如何转发。如果 qume-kvm 要和外界通信，那么数据包会被发送到物理网卡，最终实现与外部通信。\n从上面的图也可以看出 qume-kvm 发出的数据包通过 tap 设备先到达 Bridge ，然后到物理网络中，数据包不需要经过主机的的协议栈，这样效率也比较高。\n# 7. 总结 本篇文章只是讲了两种常见的虚拟网络设备。起因是在看 flannel 的时候，书里面都会讲到 flannel0 是一个 tun 设备，但是什么是 tun 设备却不明白，所以导致 flannel 也看的不明白。\n经过研究，发现 tun/tap 设备是一个虚拟网络设备，负责数据转发，但是它需要通过文件作为传输通道，这样不可避免的引申出 tun/tap 设备为什么要转发两次，这也是为什么 flannel 设备 UDP 模式下性能不好的原因，导致了后面这种模式被废弃掉。\n因为 tun/tap 设备作为虚拟网络设备性能不好，容器对容器的直接通信并不会把 tun/tap 作为首选方案，一般是基于后面介绍的 veth 来实现的。veth 作为一个二层设备，可以让两个隔离的网络名称空间之间可以互相通信，不需要反复多次经过网络协议栈， veth pair 是一端连着协议栈，另一端彼此相连的，数据之间的传输变得十分简单，这也让 veth 比起 tap/tun 具有更好的性能。\n了解更多:\nIntroduction to Linux interfaces for virtual networking | Red Hat Developer Linux Virtual Networking ","date":"2023-09-11T21:57:37Z","permalink":"https://blog.yorforger.cc/p/%E8%99%9A%E6%8B%9F%E7%BD%91%E5%8D%A1-tun/tap-%E9%98%85%E8%AF%BB%E8%BD%AC%E8%BD%BD/","title":"虚拟网卡 tun/tap - 阅读转载"},{"content":" # 0. Make it eassy This is all this post is going to talk, if you know this, you don\u0026rsquo;t need to go through all this post.\na string literal is a slice of bytes:\n1 2 3 4 5 6 7 8 func main() { str := \u0026#34;汉\u0026#34; for i:=0; i\u0026lt;len(str); i++{ fmt.Printf(\u0026#34;%x \u0026#34;, str[i]) } } --------------------------- e6 b1 89 a string literal consists of one or more rune values:\n1 2 3 4 5 6 7 8 9 10 11 12 13 func main() { str := \u0026#34;abc汉\u0026#34; for _, s := range str { fmt.Printf(\u0026#34;%x \u0026#34;, s) } fmt.Println() for _, s := range str { fmt.Printf(\u0026#34;%c \u0026#34;, s) } } -------------------------- 61 62 63 6c49 a b c 汉 Note that the ways of iterating the string literal in the codes above are different.\n# 1. string literals are encoded in UTF-8 1 2 str := \u0026#34;汉\u0026#34; fmt.Printf(\u0026#34;length of str: %d\u0026#34;, len(str)) Output:\n1 length of str: 3 Why the output is 3? It\u0026rsquo;s all about the encoding.\nCharacter 汉 is a chinese character which occupies 3 bytes in utf-8 encoding format, and its code point is 6C49. Code point is a concept defined in Unicode. If you are not familiar with these you can check my another post: https://davidzhu.xyz/post/cs-basics/001-encoding/\nIn Go, string literals always contain UTF-8 text as long as they have no byte-level escapes. Therefore, the length of str is 3 bytes. This is the string in Go runtime sourcode which will help you understand string in go:\n1 2 3 4 5 // src/runtime/string.go type stringStruct struct { str unsafe.Pointer len int } Note that: 'x' represents a single character (called a rune), whereas \u0026quot;x\u0026quot; represents a string literal containing the character x. This is same as in c++ and c.\nSize is not length, the size of a stirng value is always 2 words, learn more: Value Variable and Types - Go - David\u0026rsquo;s Blog\n# 2. string is read-only slice of bytes In Go, a string is in effect a read-only slice of bytes. We have disscussed slice in previous post.\n1 2 3 4 5 6 func main() { str := \u0026#34;汉\u0026#34; for i := 0; i \u0026lt; len(str); i++ { fmt.Printf(\u0026#34;%x \u0026#34;, str[i]) } } Output:\n1 e6 b1 89 As we saw, indexing a string yields its bytes, not its characters: a string is just a bunch of bytes.\nHere is another example that I want to talk:\n1 2 3 4 5 6 7 8 func main() { str := \u0026#34;汉\u0026#34; fmt.Printf(\u0026#34;%x \u0026#34;, str) fmt.Println() for i := 0; i \u0026lt; len(str); i++ { fmt.Printf(\u0026#34;%x \u0026#34;, str[i]) } } Output:\n1 2 6c49 e6 b1 89 That character 汉 has Unicode value U+6C49 , this is just its Unicode code point, if we want wrtie this string into a file, we have to use a encoding scheme, gbk, utf-8 and other encoding scheme. After 汉 is encoded by utf-8, the value represents it is e6 b1 89 Note that the value that written in the disk is e6 b1 89 which stands for 汉 in utf-8. Learn more: https://davidzhu.xyz/post/cs-basics/001-encoding/\n# 3. string is immutable 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func EchoCommandArgs() { var s string // The variable os.Args is a slice of strings. // range produces a pair of values: the index and the value of the element at that index. // we ignore index with _ marker here for _, arg := range os.Args[1:]{ // s += ... will make a new string everytime, because string is immutable // The += statement makes a new string by concatenating the old string, // a space character, and the next argument, then assigns the new string to s. // The old contents of s are no longer in use, so they will be garbage-collected in due course. // If the amount of data involved is large, this could be costly. // A simpler and more efficient solution would be to use the Join function from the strings package s += arg s += \u0026#34; \u0026#34; } fmt.Println(s) fmt.Println(os.Args[1:]) fmt.Println(strings.Join(os.Args[1:], \u0026#34; \u0026#34;)) } --------------------------------------- $ go run main.go hello hi hi hello hi hi [hello hi hi] hello hi hi # 4. byte \u0026amp; rune A byte in Go is an unsigned 8-bit integer whose type is actually uint8, which can represent an ASCII character.\nGo uses rune, which has type int32, to deal with multibyte characters. rune occupies 32bit (4 bytes) and is meant to represent a Unicode CodePoint which is 4 bytes. Source: https://stackoverflow.com/a/19325804/16317008\n汉 is a multibyte character. Here should note that we say \u0026ldquo;rune is used to deal with multibyte characters\u0026rdquo; doesn\u0026rsquo;t mean it cannot represent singlebyte characters (ASCII), it\u0026rsquo;s just a int32 type, not something magic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func main() { str := \u0026#34;abc汉\u0026#34; for _, c := range str { fmt.Printf(\u0026#34;%T \u0026#34;, c) } fmt.Println() for _, c := range str { fmt.Printf(\u0026#34;%x \u0026#34;, c) } fmt.Println() for i := 0; i \u0026lt; len(str); i++ { fmt.Printf(\u0026#34;%x \u0026#34;, str[i]) } } Output:\n1 2 3 int32 int32 int32 int32 61 62 63 6c49 61 62 63 e6 b1 89 As we see here from the output line-3, a string literal is just a slice of bytes, but the first two line prove that we can think a string literal is just a slice of rune which is int32.\nHere is another example may help you to understand the string literal and rune:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func isValid(s string) bool { for _, c := range s { // error: Invalid operation: c == \u0026#34;{\u0026#34; (mismatched types int32 and string) if c == \u0026#34;{\u0026#34;: ... // no error // `\u0026#39;x\u0026#39;` represents a single character (called a rune) // c is a rune here if c == \u0026#39;{\u0026#39; { } } return false } # 5. Go source code is UTF-8 Source code in Go is defined to be UTF-8 text; no other representation is allowed. That implies that when, in the source code, we write the text\n1 `⌘` the text editor used to create the program places the UTF-8 encoding of the symbol ⌘ into the source text. When we print out the hexadecimal bytes, we’re just dumping the data the editor placed in the file.\nRecall the 汉 example we talked above, the real thing written in the disk is not its Unicode code point but the data encoded by uft-8 which is 3 bytes.\nIn short, Go source code is UTF-8, so the source code for the string literal is UTF-8 text. If that string literal contains no escape sequences, which a raw string cannot, the constructed string will hold exactly the source text between the quotes. Thus by definition and by construction the raw string will always contain a valid UTF-8 representation of its contents. Similarly, unless it contains UTF-8-breaking escapes like those from the previous section, a regular string literal will also always contain valid UTF-8.\nSome people think Go strings are always UTF-8, but they are not: only string literals are UTF-8. As we showed in the previous section, string values can contain arbitrary bytes; as we showed in this one, string literals always contain UTF-8 text as long as they have no byte-level escapes.\nTo summarize, strings can contain arbitrary bytes, but when constructed from string literals, those bytes are (almost always) UTF-8.\n# 6. []byte vs []rune A slice of byte just a slice of uint8, just a list of small numbers (0~127). We usually use a []byte slice when we want store ASCII characters which are singlebye characters, if you want store multibyte characters, you should use []rune or the string literals.\n# 7. []byte, []rune and string conversion will makes copy 1 2 3 4 5 6 7 8 9 10 11 12 13 func main() { // this is a slice not an array // you can replace []byte with []rune data := []byte{102, 97, 108, 99, 111, 110} // convert byte slice to string fmt.Println(string(data)) // string to byte slice fmt.Println([]byte(\u0026#34;falcon\u0026#34;)) } ---------------------------------------- [102 97 108 99 111 110] falcon [102 97 108 99 111 110] The compiler will allocate a new piece of memory and copy the data over when you convet string to bytes or bytes to string. The reason is that []byte is not immutable. You can change the bytes within a slice. You can grow the slice by appending bytes to the end. But a string is immutable. You cannot change it and you cannot grow it without creating a whole new string. If a string and a []byte were backed by the same memory, then the string would change when the []byte was changed, and that might not be what you expect.\nSo, each time you do []byte(“fred”) or string([]byte{0x40, 0x040}), there’s an allocation and a copy. This is done by https://golang.org/pkg/runtime/?m=all#stringtoslicebyte and https://golang.org/pkg/runtime/?m=all#slicebytetostring. And this is normally the right thing to do and normally has no significant consequences.\nReferences:\nGo byte - working with bytes in Golang []byte vs string in Go Strings, bytes, runes and characters in Go - The Go Programming Language go - Cannot assign string with single quote in golang - Stack Overflow ","date":"2023-09-11T13:50:59Z","permalink":"https://blog.yorforger.cc/p/string-bytes-runes-and-characters-go/","title":"string, bytes, runes and characters - Go"},{"content":"A properly formed git commit subject line should always be able to complete the following sentence:\nIf applied, this commit will # 1. Things to avoid when creating commits Sending large new features in a single giant commit. Mixing two unrelated functional changes. Again the reviewer will find it harder to identify flaws if two unrelated changes are mixed together. If it becomes necessary to later revert a broken commit, the two unrelated changes will need to be untangled, with further risk of bug creation. Mixing whitespace changes with functional code changes. 1 2 If a code change can be split into a sequence of patches/commits, then it should be split. Less is not more. More is more. # 2. Commit message format Each commit message consists of a header, a body, and a footer.\n1 2 3 4 5 \u0026lt;header\u0026gt; \u0026lt;BLANK LINE\u0026gt; \u0026lt;body\u0026gt; \u0026lt;BLANK LINE\u0026gt; \u0026lt;footer\u0026gt; # 2.1. Commit message header 1 2 3 4 5 6 7 8 9 10 11 \u0026lt;type\u0026gt;(\u0026lt;scope\u0026gt;): \u0026lt;short summary\u0026gt; │ │ │ │ │ └─⫸ Summary in present tense. Not capitalized. No period at the end. │ │ │ └─⫸ Commit Scope: animations|bazel|benchpress|common|compiler|compiler-cli|core| │ elements|forms|http|language-service|localize|platform-browser| │ platform-browser-dynamic|platform-server|router|service-worker| │ upgrade|zone.js|packaging|changelog|docs-infra|migrations|ngcc|ve| │ devtools │ └─⫸ Commit Type: build|ci|docs|feat|fix|perf|refactor|test The \u0026lt;type\u0026gt; and \u0026lt;summary\u0026gt; fields are mandatory, the (\u0026lt;scope\u0026gt;) field is optional.\n# Type Must be one of the following:\nbuild: Changes that affect the build system or external dependencies (example scopes: gulp, broccoli, npm) ci: Changes to our CI configuration files and scripts (examples: CircleCi, SauceLabs) docs: Documentation only changes feat: A new feature fix: A bug fix perf: A code change that improves performance refactor: A code change that neither fixes a bug nor adds a feature test: Adding missing tests or correcting existing tests # Summary Use the summary field to provide a succinct description of the change:\nuse the imperative, present tense: \u0026ldquo;change\u0026rdquo; not \u0026ldquo;changed\u0026rdquo; nor \u0026ldquo;changes\u0026rdquo; This convention matches up with commit messages generated by commands like git merge and git revert. don\u0026rsquo;t capitalize the first letter no dot (.) at the end # 3. Information in commit messages 1 2 3 4 5 6 7 commit 468e64d019f51d364afb30b0eed2ad09483e0b98 Author: [removed] Date: Mon Jun 18 16:07:37 2012 -0400 Fix missing import in compute/utils.py Fixes bug 1014829 Problem: this does not mention what imports where missing and why they were needed. This info was actually in the bug tracker, and should have been copied into the commit message, so that it would provide a self-contained description. e.g.:\n1 2 3 4 Add missing import of \u0026#39;exception\u0026#39; in compute/utils.py nova/compute/utils.py makes a reference to exception.NotFound, however exception has not been imported. # 4. Common wrods 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 feat: add support for configuring libvirt VM clock and timers feat: add libvirt version check, min 0.9.7 feat: update default policies for KVM guest PIT \u0026amp; RTC timers perf: refine serialize refactor: reformat code refactor: rename type and namespace refactor: add a comment to rpc.queue_get_for() Add, Fix, Remove, Refactor, Reformat, Optimise and Update Add = Create a capability e.g. feature, test, dependency. Remove = Remove a capability e.g. feature, test, dependency. Fix = Fix an issue e.g. bug, typo, accident, misstatement. Refactor = A code change that MUST be just a refactoring. Reformat = Refactor of formatting, e.g. omit whitespace. Optimise = Refactor of performance, e.g. speed up code. # 5. Check commit history 1 $ git config --global alias.lgg \u0026#34;log --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit --date=relative\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ git lgg * d7c4459 - (HEAD, M, fromA) M \u0026lt;VonC\u0026gt; * 82b011d - (L) Merge commit \u0026#39;J\u0026#39; into fromA \u0026lt;VonC\u0026gt; |\\ | * 190265b - (J, master) J \u0026lt;VonC\u0026gt; | * ef8e325 - (I) Merge commit \u0026#39;F\u0026#39; \u0026lt;VonC\u0026gt; | |\\ | | * 4b6d976 - (F, fromB) F \u0026lt;VonC\u0026gt; | * | 45a5d4d - (H) H \u0026lt;VonC\u0026gt; | * | 834b239 - (G) Merge commit \u0026#39;E\u0026#39; \u0026lt;VonC\u0026gt; | |\\ \\ | | |/ | | * f8e9272 - (E) E \u0026lt;VonC\u0026gt; | | * 96b5538 - (D) D \u0026lt;VonC\u0026gt; | * | 49eff7f - (C) C \u0026lt;VonC\u0026gt; | |/ | * 02c3ef4 - (B) B \u0026lt;VonC\u0026gt; * | c0d9e1e - (K) K \u0026lt;VonC\u0026gt; |/ * 6530d79 - (A) A \u0026lt;VonC\u0026gt; Source: https://stackoverflow.com/a/6744268/16317008\nReferences:\nCommit message guidelines Angular Commit Format Reference Sheet knowbl/git-commit-message: Git commit message verbs used by Knowbl. Example list of verbs / first word to use in git commit title #git #commit #title ","date":"2023-09-11T07:47:23Z","permalink":"https://blog.yorforger.cc/p/commit-message-guidelines/","title":"Commit Message Guidelines"},{"content":"HTTPS is used for encryption, VPN is too. What\u0026rsquo;s difference?\nIn previou post we talked about Is HTTPS Secure Enough?, and we concluded that https is secure itself which means no one can decrypt the data without the session key. However there still are some security issues in HTTPS, the man-in-the-middle attack, for example.\n# Why do I need a VPN if https connections are secure? A VPN will secure all of the traffic between point A and point B in a tunnel. This helps ensure that you are not having your traffic intercepted by anyone at, say, the coffee shop.\nHTTPS is a secure protocol between your browser and a particular website.\nNote that the tunnel in VPN is not a physical entity but rather a logical concept used to describe the secure pathway created for data transmission. Because tunneling involves repackaging the traffic data into a different form, perhaps with encryption as standard, it can hide the nature of the traffic that is run through a tunnel. Tunneling protocol\nThis means the format of the packets transmitted between VPN clients and server are different from the format of OSI model, they are encapulated with tunneling protocol. Acts like your original packets transmitted in a magic container other people cannot know what\u0026rsquo;s in it. Acs like a tunnel.\nHowever, HTTPS will only help with traffic over port 443, which mean it only provides encryption for data exchanged between a client and a specific server, it does not encrypt all your internet traffic.\nA VPN stops that because everything is connected securely through that pipe. The primary thing that a VPN helps with is what is know as a man in the middle attack: Man-in-the-middle attack - Wikipedia\n# VPN vs Proxy In computer networks, a proxy server is a server (a computer system or an application) that acts as an intermediary for requests from clients seeking resources from other servers. Proxy server\nA virtual private network (VPN) is an encrypted connection between two or more computers. VPN connections take place over public networks, but the data exchanged over the VPN is still private because it is encrypted. IPsec | Cloudflare\nWhat a VPN does logically is turn your internet connection into a big Ethernet cable. When you are logged on to a company\u0026rsquo;s VPN, the effect is similar as though you took your computer to the company\u0026rsquo;s building and directly connected it. VPNs (usually) use encryption so that intermediate systems between you and the company (such as your ISP or a malicious wireless network sniffer) cannot eavesdrop your traffic.\nProxies, on the other hand, do not typically provide encryption for all traffic and may only encrypt specific types of traffic (such as HTTPS).\nProxies generally work on specific types of application traffic. For example, there are HTTP proxies, DNS proxies, etc. Although there are SOCKS proxies that proxy everything\u0026hellip;\nHow do sites like Netflix know I\u0026rsquo;m using a VPN?\nThis is often caused by many netflix uers use a same von to access thier streaming service.\nWe have known that: when you are logged on to a company\u0026rsquo;s VPN, the effect is similar as though you took your computer to the company\u0026rsquo;s building and directly connected it.\nIf eveybody drives the same car to Walmart, Walmart will sooner or later know it\u0026rsquo;s a rental car, but they cannot track this car back to you because everybody drives the car with same the plate.\nReferences:\nWhy do I need VPN if https connections are secure? - Quora security - What is the difference between a proxy and a VPN? - Super User How do sites like Netflix know I\u0026rsquo;m using a VPN? : VPN ","date":"2023-09-10T10:54:30Z","permalink":"https://blog.yorforger.cc/p/https-vs-vpn-vs-proxy/","title":"HTTPS vs VPN vs Proxy"},{"content":" # VPN VPN（Virtual Private Network）的核心特性之一就是使用隧道协议（Tunneling Protocol）。通过这些隧道协议，VPN 能够保证数据在不安全的网络中的安全传输，使得VPN在保护在线隐私和绕过网络限制方面非常有效。\n常见的隧道协议包括：\nIPsec (Internet Protocol Security)：用于在IP通信过程中确保数据的完整性、认证和机密性。 OpenVPN：一个基于SSL/TLS的开源VPN协议，提供高度的安全性和灵活性。 WireGuard：一个较新的协议，旨在提供更高的速度和更先进的加密技术。 SSL/TLS (Secure Sockets Layer/Transport Layer Security)：SSL VPN usually connects using a Web browser, whereas an IPSec VPN generally requires client software on the remote system. SSL VPN is a component of virtually every Web browser. Any OS that runs a browser is supported. OpenVPN 是一个独立的 VPN 协议，它不使用像 PPTP、L2TP 或 IPsec 这样的标准隧道协议。相反，OpenVPN 使用自己的协议，基于 SSL/TLS 来提供加密和安全连接。它是一个开源的软件应用程序，允许创建安全的点对点或站点对站点连接 OpenVPN 的关键特性包括：\n自定义加密协议：虽然基于 SSL/TLS，但 OpenVPN 有其独特的实现方式，允许高度的定制和灵活性。 身份验证：支持各种认证机制，包括证书、预共享密钥和用户认证。 跨平台兼容性：OpenVPN 可以在多种操作系统上运行，包括 Windows、macOS、Linux 和移动平台。 # 1. Tunneling In previous post, we know that TUN and TAP are two different kernel virtual network devices, which are used for tunneling purposes. In this post, we\u0026rsquo;ll discuss what is tunneling and coomon tunneling protocols.\nIn the physical world, tunneling is a way to cross terrain or boundaries that could not normally be crossed. Similarly, in networking, tunnels are a method for transporting data across a network using protocols that are not supported by that network. Tunneling works by encapsulating packets: wrapping packets inside of other packets.\nEncapsulating packets within other packets is called \u0026ldquo;tunneling.\u0026rdquo;\nBecause tunneling involves repackaging the traffic data into a different form, perhaps with encryption as standard, it can hide the nature of the traffic that is run through a tunnel.\nEncapsulation:\nData traveling over a network is divided into packets. A typical packet has two parts: the header, which indicates the packet\u0026rsquo;s destination and which protocol it uses, and the payload, which is the packet\u0026rsquo;s actual contents.\nAn encapsulated packet is essentially a packet inside another packet. In an encapsulated packet, the header and payload of the first packet goes inside the payload section of the surrounding packet. The original packet itself becomes the payload.\nA VPN just uses one of the tunneling protocols, therefore, when you know how each tunneling protocol works, you will know the essence of VPN of that type.\n# 2. Types of Tunneling protocol In addition to GRE, IPsec, IP-in-IP, and SSH, other tunneling protocols include:\nPoint-to-Point Tunneling Protocol (PPTP) Secure Socket Tunneling Protocol (SSTP) Layer 2 Tunneling Protocol (L2TP) Virtual Extensible Local Area Network (VXLAN) # 3. IPsec - Network layer The IPsec protocol suite operates at the network layer of the OSI model. Within the term \u0026ldquo;IPsec,\u0026rdquo; \u0026ldquo;IP\u0026rdquo; stands for \u0026ldquo;Internet Protocol\u0026rdquo; and \u0026ldquo;sec\u0026rdquo; for \u0026ldquo;secure.\u0026rdquo; IPsec usually uses port 500.\nThe Internet Protocol is the main routing protocol used on the Internet; it designates where data will go using IP addresses. IPsec is secure because it adds encryption and authentication to this process.\n# 3.1. Why IPSec is important? Networking protocol suites like TCP/IP are only concerned with connection and delivery, and messages sent are not concealed. Anyone in the middle can read them. IPsec, and other protocols that encrypt data, essentially put an envelope around data as it traverses networks, keeping it secure.\n# 3.2. How does IPsec work? IPsec connections include the following steps:\nKey exchange: Keys are necessary for encryption;\nPacket headers and trailers: IP packets contain both a payload and a header. IPsec adds several headers to data packets containing authentication and encryption information. IPsec also adds trailers, which go after each packet\u0026rsquo;s payload instead of before.\nLearn more: What is IPsec? | How IPsec VPNs work | Cloudflare\n# 3.3. IPsec tunnel and IPsec transport mode Tunnel Mode：\n在隧道模式下，IPsec对整个IP数据包（包括头部信息）进行加密。 这意味着原始的IP数据包被封装在一个新的IP数据包中。新的数据包有一个新的IP头部。 隧道模式常用于VPN（Virtual Private Network，虚拟私人网络），允许不同网络之间安全地传输数据。 Transport Mode：\n在传输模式下，IPsec只加密IP数据包的有效载荷（Payload），而不加密头部信息。 # 3.4. What protocols are used in IPsec? In networking, a protocol is a specified way of formatting data so that any networked computer can interpret the data. IPsec is not one protocol, but a suite of protocols. The following protocols make up the IPsec suite:\nAuthentication Header (AH): The AH protocol ensures that data packets are from a trusted source and that the data has not been tampered with, like a tamper-proof seal on a consumer product. These headers do not provide any encryption; they do not help conceal the data from attackers.\nEncapsulating Security Protocol (ESP): ESP encrypts the IP header and the payload for each packet — unless transport mode is used, in which case it only encrypts the payload. ESP adds its own header and a trailer to each data packet.\nSecurity Association (SA): SA refers to a number of protocols used for negotiating encryption keys and algorithms. One of the most common SA protocols is Internet Key Exchange (IKE).\nFinally, while the Internet Protocol (IP) is not part of the IPsec suite, IPsec runs directly on top of IP.\n# 3.5. How does IPsec impact MSS and MTU? IPsec protocols add several headers and trailers to packets, all of which take up several bytes. For networks that use IPsec, either the MSS and MTU have to be adjusted accordingly, or packets will be fragmented and slightly delayed. Usually, the MTU for a network is 1,500 bytes. A normal IP header is 20 bytes long, and a TCP header is also 20 bytes long, meaning each packet can contain 1,460 bytes of payload. However, IPsec adds an Authentication Header, an ESP header, and associated trailers. These add 50-60 bytes to a packet, or more.\n# 4. GRE - Network Layer Encapsulating packets within other packets is called \u0026ldquo;tunneling.\u0026rdquo; GRE tunnels are usually configured between two routers, with each router acting like one end of the tunnel. The routers are set up to send and receive GRE packets directly to each other. Any routers in between those two routers will not open the encapsulated packets; they only reference the headers surrounding the encapsulated packets in order to forward them.\nGRE enables the usage of protocols that are not normally supported by a network, because the packets are wrapped within other packets that do use supported protocols. To understand how this works, think about the difference between a car and a ferry. A car travels over roads on land, while a ferry travels over water. A car cannot normally travel on water — however, a car can be loaded onto a ferry in order to do so.\nFor instance, suppose a company needs to set up a connection between the local area networks (LANs) in their two different offices. Both LANs use the latest version of the Internet Protocol, IPv6. But in order to get from one office network to another, traffic must pass through a network managed by a third party — which is somewhat outdated and only supports the older IPv4 protocol.\nWith GRE, the company could send traffic through this network by encapsulating IPv6 packets within IPv4 packets. Referring back to the analogy, the IPv6 packets are the car, the IPv4 packets are the ferry, and the third-party network is the water.\n# 4.1. What goes in a GRE header? GRE adds two headers to each packet: the GRE header, which is 4 bytes long, and an IP header, which is 20 bytes long. The GRE header indicates the protocol type used by the encapsulated packet. The IP header encapsulates the original packet\u0026rsquo;s header and payload. This means that a GRE packet usually has two IP headers: one for the original packet, and one added by the GRE protocol. Only the routers at each end of the GRE tunnel will reference the original, non-GRE IP header.\n# 4.2. How does the use of GRE impact MTU and MSS requirements? Like any protocol, using GRE adds a few bytes to the size of data packets. This must be factored into the MSS and MTU settings for packets. If the MTU is 1,500 bytes and the MSS is 1,460 bytes (to account for the size of the necessary IP and TCP headers), the addition of GRE 24-byte headers will cause the packets to exceed the MTU:\n1,460 bytes [payload] + 20 bytes [TCP header] + 20 bytes [IP header] + 24 bytes [GRE header + IP header] = 1,524 bytes\nAs a result, the packets will be fragmented. Fragmentation slows down packet delivery times and increases how much compute power is used, because packets that exceed the MTU must be broken down and then reassembled.\nThis can be avoided by reducing the MSS to accommodate the GRE headers. If the MSS is set to 1,436 instead of 1,460, the GRE headers will be accounted for and the packets will not exceed the MTU of 1,500:\n1,436 bytes [payload] + 20 bytes [TCP header] + 20 bytes [IP header] + 24 bytes [GRE header + IP header] = 1,500 bytes\nWhile fragmentation is avoided, the result is that payloads are slightly smaller, meaning it will take extra packets to deliver data. For instance, if the goal is to deliver 150,000 bytes of content (or about 150 kB), and if the MTU is set to 1,500 and no other layer 3 protocols are used, compare how many packets are necessary when GRE is used versus when it is not used:\nWithout GRE, MSS 1,460: 103 packets With GRE, MSS 1,436: 105 packets The extra two packets add milliseconds of delay to the data transfer. However, the usage of GRE may allow these packets to take faster network paths than they could otherwise take, which can make up for the lost time.\n# 5. SSH Tunneling - Application layer The Secure Shell (SSH) protocol sets up encrypted connections between client and server, and can also be used to set up a secure tunnel. SSH operates at layer 7 of the OSI model, the application layer. By contrast, IPsec, IP-in-IP, and GRE operate at the network layer.\nA Secure Shell (SSH) tunnel consists of an encrypted tunnel created through an SSH protocol connection. Users may set up SSH tunnels to transfer unencrypted traffic over a network through an encrypted channel. It is a software-based approach to network security and the result is transparent encryption.\nReferences:\nWhat is IPsec? | How IPsec VPNs work | Cloudflare What is tunneling? | Tunneling in networking | Cloudflare Tunneling protocol What is GRE tunneling? | How GRE protocol works | Cloudflare ","date":"2023-09-10T08:27:59Z","permalink":"https://blog.yorforger.cc/p/tunneling-protocols/","title":"Tunneling Protocols"},{"content":" # 1. Login without using keypairs Copy your ssh public key on your computer into the ~/.ssh/authorized_keys file on you EC2 instance.\nOn you local machine:\n1 2 $ cat ~/.ssh/id_rsa.pub ... Copy the content into your EC2 instance:\n1 $ sudo vi .ssh/authorized_keys Then you can login directly.\n# 2. oh-my-zsh shell 1 2 3 4 5 sudo apt install zsh -y chsh -s /bin/zsh sh -c \u0026#34;$(wget https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)\u0026#34; # wget 有时候会出现 timeout, 可以用 curl 代替: # sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # 3. Set up ufw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ➜ ~ ufw status Status: active To Action From -- ------ ---- 22/tcp ALLOW Anywhere 22/tcp (v6) ALLOW Anywhere (v6) # if you need 443, do this: ➜ ~ ufw allow 443 Rule added Rule added (v6) # or just disable ➜ ~ ufw disable Learn more: ufw vs AWS Security Group - David\u0026rsquo;s Blog\n# 4. Other info After basic settings, on Alpine Linux: 1 vCPUs, 512 MB RAM, 10G SSD, Vultr VPS:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ➜ ~ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 10M 0 10M 0% /dev shm 231M 0 231M 0% /dev/shm /dev/vda2 9.2G 579M 8.2G 7% / tmpfs 93M 336K 92M 1% /run /dev/vda1 256M 266K 256M 1% /boot/efi tmpfs 231M 0 231M 0% /tmp ➜ ~ speedtest-cli Testing download speed................................................................................ Download: 1648.48 Mbit/s Testing upload speed...................................................................................................... Upload: 1013.83 Mbit/s ❯ iperf -c 149.22.12.30 -R [ ID] Interval Transfer Bandwidth [ *1] 0.00-3.00 sec 77.2 MBytes 216 Mbits/sec [ *1] 3.00-6.00 sec 105 MBytes 295 Mbits/sec [ *1] 6.00-9.00 sec 101 MBytes 282 Mbits/sec ❯ iperf -c 149.22.12.30 [ ID] Interval Transfer Bandwidth [ 1] 0.00-3.00 sec 102 MBytes 286 Mbits/sec [ 1] 3.00-6.00 sec 104 MBytes 290 Mbits/sec [ 1] 6.00-9.00 sec 100 MBytes 281 Mbits/sec 1 2 # vpn x-ui bash \u0026lt;(curl -Ls https://raw.githubusercontent.com/FranzKafkaYu/x-ui/956bf85bbac978d56c0e319c5fac2d6db7df9564/install.sh) 0.3.4.4 ","date":"2023-09-09T15:53:57Z","permalink":"https://blog.yorforger.cc/p/basic-configuration-of-vps/","title":"Basic Configuration of VPS"},{"content":" # 1. DDoS attack A distributed denial-of-service (DDoS) attack is a malicious attempt to disrupt the normal traffic of a targeted server, service or network by overwhelming the target or its surrounding infrastructure with a flood of Internet traffic.\nWhen a DDoS attack happens, a large volume of traffic is sent to a website. The site under attack typically crashes because the increased traffic exhausts the bandwidth limit or overloads the website’s servers.\nvideo: https://youtu.be/7kB9-nQJR44?si=R8qrLRhRwlqofAu5\n# 2. How does a DDoS attack work? DDoS attacks are carried out with networks of Internet-connected machines.\nThese networks consist of computers and other devices (such as IoT devices)which have been infected with malware, allowing them to be controlled remotely by an attacker. These individual devices are referred to as bots (or zombies), and a group of bots is called a botnet.\nOnce a botnet has been established, the attacker is able to direct an attack by sending remote instructions to each bot.\nWhen a victim’s server or network is targeted by the botnet, each bot sends requests to the target’s IP address, potentially causing the server or network to become overwhelmed, resulting in a denial-of-service to normal traffic.\nBecause each bot is a legitimate Internet device, separating the attack traffic from normal traffic can be difficult.\n# 3. How to identify a DDoS attack The most obvious symptom of a DDoS attack is a site or service suddenly becoming slow or unavailable. But since a number of causes — such a legitimate spike in traffic — can create similar performance issues, further investigation is usually required. Traffic analytics tools can help you spot some of these telltale signs of a DDoS attack:\nSuspicious amounts of traffic originating from a single IP address or IP range A flood of traffic from users who share a single behavioral profile, such as device type, geolocation, or web browser version An unexplained surge in requests to a single page or endpoint Odd traffic patterns such as spikes at odd hours of the day or patterns that appear to be unnatural (e.g. a spike every 10 minutes) # 4. Challenge Collapsar (CC) attack - DDoS A Challenge Collapsar (CC) attack is an attack where standard HTTP requests are sent to a targeted web server frequently.\nIn 2004, a Chinese hacker nicknamed KiKi invented a hacking tool to send these kinds of requests to attack a NSFOCUS firewall named Collapsar, and thus the hacking tool was known as Challenge Collapsar, or CC for short. Consequently, this type of attack got the name CC attack.\nDDoS is not a specific attack, but a general term for a large types of attacks. There are dozens of types, and new attack methods are constantly being invented.\n# 4.1. Intercept cc attack - intercept http request (1) Hardware firewall\nSet a physical firewall before your server machine which used to filter request, this is best way but most expensive too.\n**(2) Software firewall **\nAlmost all OS has firewall installed，Linux server usually use iptables, intercept request from IP address 1.2.3.4, for example:\n1 $ iptables -A INPUT -s 1.2.3.4 -j DROP (3) Web server\nWeb can also used to intercept IP address 1.2.3.4, on nginx:\n1 2 3 location / { deny 1.2.3.4; } On Apache, modify the .htaccess file:\n1 2 3 4 \u0026lt;RequireAll\u0026gt; Require all granted Require not ip 1.2.3.4 \u0026lt;/RequireAll\u0026gt; Web server have a impact impact on the performance when used in firewall and cannot protect when there are huge DDoS attack.\nLearn more: https://www.ruanyifeng.com/blog/2018/06/ddos.html\nReferences:\nWhat is a distributed denial-of-service (DDoS) attack? | Cloudflare What is a distributed denial-of-service (DDoS) attack? | Cloudflare Denial-of-service attack ","date":"2023-09-09T09:29:57Z","permalink":"https://blog.yorforger.cc/p/ddos-attack/","title":"DDoS Attack"},{"content":" # 1. TUN/TAP Devices In computer networking, TUN and TAP are two different kernel virtual network devices. Though both are for tunneling purposes, TUN and TAP can\u0026rsquo;t be used together because they transmit and receive packets at different layers of the network stack. TUN (network TUNnel) devices are used for IP packet-level tunneling, while TAP (network TAP) devices are used for Ethernet frame-level tunneling.\nA network interface can be a physical device, called network interface controller (NIC), such as an ethernet card or wireless adapter, or a virtual device, such as a TUN or TAP interface.\nSource: https://www.baeldung.com/linux/create-check-network-interfaces\nTUN/TAP provides packet reception and transmission for user space programs. It can be seen as a simple Point-to-Point or Ethernet device, which, instead of receiving packets from physical media, receives them from user space program and instead of sending packets via physical media writes them to the user space program. It interacts with user space program not the kernel.\nSource: https://docs.kernel.org/networking/tuntap.html\n# 2. TUN/TAP \u0026amp; Network Stack TUN/TAP is an operating-system interface for creating network interfaces managed by userspace. This is usually used to implement userspace Virtual Private Networks[1] (VPNs), for example with OpenVPN, OpenSSH (Tunnel configuration or -w argument), l2tpns, etc.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 +--------------------------------------------+ | Processes | +--------------------------------------------+ ↕ Socket interface +---------------------------------------------+ | Network Stack (kernel) |\u0026lt;--+ +---------------------------------------------+ | ↕ Eth. frame ↕ Eth. frame ↕ IP packet | +--------------+ +-------------+ +------------+ | | enp2s0 | | tap0 | | tun0 | | +--------------+ +-------------+ +------------+ | ↑ Eth. frame ↕ Eth. frame¹ ↕ IP packet¹ | +--------------+ +-------------+ +------------+ | | Driver | | Process | | Process | | +--------------+ +-------------+ +------------+ | ↕ Eth. frame² ↑ ↑ | +--------------+ +---------------+--------------+ | Eth. Adapter | (encapsulated packets) +--------------+ ↕ Eth. frame +--------------+ | Eth. Network | +--------------+ Physical netdev Ethernet VPN IP VPN ¹: via /dev/net/tun ²: over PCI Express for example There are many applications running on our OS (reside in user space), they send network packets with socket interface (on linux). All the packets through socket interface will go to one place: the Network Stack (resides in kernel). And the packets from the Network Stack only have two direction to go, one is the physical network interfce (NIC), another direction is the virtual network devices tap/tun, and we can get the packets from tap/tun which usually used in VPN application (resides in user space). Of course, VPN applicatipn (user space) can write packet into tap/tun devices and after written into tap/tun the packet will goes to Network Stack, and there are two direction again\u0026hellip;\nYou probably notice that if we use TUN/TAP devices to handle the packets from other normal applications, the packet seem to go through the Network Stack twice: the normal applications write data into socket interface and these packets will go through Netwok Stack and intercepted by TUN/TAP devices, then after vpn handles these packets, these packet will be sent to Network Stack again, which probably not efficient. (Notice that in qemu-kvm, tap device works with bridge, and the packets goes to bridge directly without going through Network Stack which is more efficient)\nApplications send or receive packet to/from socket interface:\n1 int socket = socket(AF_PACKET,SOCK_RAW,IPPROTO_IP) VPN application send or receive packet to/from tun devices:\n1 2 3 4 5 6 7 8 9 // Request a TUN device int fd = open(\u0026#34;/dev/net/tun\u0026#34;, O_RDWR); // receives a (single) packet or frame from the virtual network interface; // Read an IP packet (because TUN works on IP layer): ssize_t count = read(fd, buffer, BUFFLEN); // sends a (single) packet or frame to the virtual network interface; write(...) Source: TUN/TAP interface (on Linux) - /dev/posts/\n# 3. TUN vs. TAP There are two types of virtual network interfaces managed by /dev/net/tun:\nTUN interfaces transport IP packets (layer 3); TAP interfaces transport Ethernet frames (layer 2). # 3.1. TUN interfaces (L3) TUN interfaces (IFF_TUN) transport layer 3 (L3) Protocol Data Units (PDUs):\nin practice, it transports IPv4 and/or IPv6 packets; read() gets a L3 PDU (an IP packet); you must write() L3 PDUs (an IP packet); there is no layer 2 (Ethernet, etc.) involved in the interface; # 3.2. TAP interfaces (L2) TAP interfaces (IFF_TUN) transport layer 2 (L2) PDUs:\nin practice, it transports Ethernet frames (i.e. this is a virtual Ethernet adapter); read() gets a L2 PDU; you must write() L2 PDUs. Source: TUN/TAP interface (on Linux) - /dev/posts/\n# 4. Set up TUN/TAP devices Because TUN is a layer 3 connection, it acts as a point-to-point link. We’ll assign these parameters:\nlocal address (for your machine): 192.0.2.1 remote address (for Scapy): 192.0.2.2 On Linux, you would use:\n1 2 3 sudo ip link set tun0 up sudo ip addr add 192.0.2.1 peer 192.0.2.2 dev tun0 # sudo ip addr add 192.0.2.1 peer 192.0.2.2 dev tun0 up On BSD and macOS, use:\n1 2 3 sudo ifconfig tun0 up sudo ifconfig tun0 192.0.2.1 192.0.2.2 # sudo ifconfig tun0 192.0.2.1 192.0.2.2 up Source: https://scapy.readthedocs.io/en/latest/layers/tuntap.html#tuntapinterface-reference\n","date":"2023-09-08T17:54:59Z","permalink":"https://blog.yorforger.cc/p/tun/tap-devices/","title":"TUN/TAP Devices"},{"content":" # 1.error interface The error type is an interface type. An error variable represents any value that can describe itself as a string.\n1 2 3 type error interface { Error() string } Interface error is a built-in type, as with all other built in types, is predeclared in the universe block. The most commonly-used error implementation is the errors package’s unexported errorString type.\n1 2 3 4 5 6 7 8 // errorString is a trivial implementation of error. type errorString struct { s string } func (e *errorString) Error() string { return e.s } errorString is an unexported type which means we cannot use it directly outside of errors package, but we can use New function declared in the same package to create a value of errorString.\n1 2 3 4 // New returns an error that formats as the given text. func New(text string) error { return \u0026amp;errorString{text} } The type of function returns is an error but it actually returns a pointer, a little weird probably for newb from c++. In Go everything can implement a interface an int, string even a pointer. It\u0026rsquo;s all about if the method set of that type contians all the methods declared in a interface, learn more: Methods Receivers \u0026amp; Concurrency - Go - David\u0026rsquo;s Blog\n# 2. Summarize the context # 2.1. Ways to create an error value You can create an error with these functions:\nerrors.New(), fmt.Errorf(), often used to provide conetxt. Use a custom error type, typically used for provide error details. # 2.2. Summarize the context when create an error value It is the error implementation’s responsibility to summarize the context. The error returned by os.Open formats as “open /etc/passwd: permission denied,” not just “permission denied.”\n1 2 3 4 5 6 func Sqrt(f float64) (float64, error) { if f \u0026lt; 0 { return 0, fmt.Errorf(\u0026#34;math: square root of negative number %g\u0026#34;, f) } // implementation } 1 2 3 if err != nil { return nil, fmt.Errorf(\u0026#34;math: failed to calculate sqrt: %v\u0026#34;, err) } # 3. Some common ways for error handling We have talked that there are three ways to create an error, now let\u0026rsquo;s discuss how to use them in practice.\n# 3.1. Create error value with a custom error type - provide details In many cases fmt.Errorf is good enough, but since error is an interface, you can use arbitrary data structures as error values, to allow callers to inspect the details of the error.\nThe json package specifies a SyntaxError type that the json.Decode function returns when it encounters a syntax error parsing a JSON blob.\n1 2 3 4 5 6 type SyntaxError struct { msg string // description of error Offset int64 // error occurred after reading Offset bytes } func (e *SyntaxError) Error() string { return e.msg } The Offset field isn’t even shown in the default formatting of the error, but callers can use it to add file and line information to their error messages:\n1 2 3 4 5 6 7 8 if err := dec.Decode(\u0026amp;val); err != nil { if serr, ok := err.(*json.SyntaxError); ok { // serr.Offset provide detials about error line, col := findLine(f, serr.Offset) return fmt.Errorf(\u0026#34;%s:%d:%d: %v\u0026#34;, f.Name(), line, col, err) } return err } # 3.2. Don\u0026rsquo;t return error dirctly - avoid repetitive error handling # 3.2.1. Return a bool value instead to indicate an abnormal state Here’s a simple example from the bufio package’s Scanner type. Its Scan method performs the underlying I/O, which can of course lead to an error. Yet the Scan method does not expose an error at all. Instead, it returns a boolean, and a separate method, to be run at the end of the scan, reports whether an error occurred. Client code looks like this:\n1 2 3 4 5 6 7 8 scanner := bufio.NewScanner(input) for scanner.Scan() { token := scanner.Text() // process token } if err := scanner.Err(); err != nil { // process the error } Sure, there is a nil check for an error, but it appears and executes only once. The Scan method could instead have been defined as\n1 func (s *Scanner) Scan() (token []byte, error) and then the example user code might be (depending on how the token is retrieved),\n1 2 3 4 5 6 7 8 scanner := bufio.NewScanner(input) for { token, err := scanner.Scan() if err != nil { return err // or maybe break } // process token } This isn’t very different, but there is one important distinction. In this code, the client must check for an error on every iteration, but in the real Scanner API, the error handling is abstracted away from the key API element, which is iterating over tokens. With the real API, the client’s code therefore feels more natural: loop until done, then worry about errors. Error handling does not obscure the flow of control.\n# 3.2.1. Return nothing 1 2 3 4 5 6 7 8 9 10 11 12 13 _, err = fd.Write(p0[a:b]) if err != nil { return err } _, err = fd.Write(p1[c:d]) if err != nil { return err } _, err = fd.Write(p2[e:f]) if err != nil { return err } // and so on The code above is very repetitive. A function literal closing over the error variable would help:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 var err error write := func(buf []byte) { if err != nil { return } _, err = w.Write(buf) } write(p0[a:b]) write(p1[c:d]) write(p2[e:f]) // and so on if err != nil { return err } This pattern works well, but requires a closure in each function doing the writes; a separate helper function is clumsier to use because the err variable needs to be maintained across calls (try it).\nWe can make this cleaner, more general, and reusable by borrowing the idea from the Scan method above.\nI defined an object called an errWriter, something like this:\n1 2 3 4 type errWriter struct { w io.Writer err error } and gave it one method, write. It doesn’t need to have the standard Write signature, and it’s lower-cased in part to highlight the distinction. The write method calls the Write method of the underlying Writer and records the first error for future reference:\n1 2 3 4 5 6 func (ew *errWriter) write(buf []byte) { if ew.err != nil { return } _, ew.err = ew.w.Write(buf) } As soon as an error occurs, the write method becomes a no-op but the error value is saved.\nGiven the errWriter type and its write method, the code above can be refactored:\n1 2 3 4 5 6 7 8 ew := \u0026amp;errWriter{w: fd} ew.write(p0[a:b]) ew.write(p1[c:d]) ew.write(p2[e:f]) // and so on if ew.err != nil { return ew.err } This is cleaner, even compared to the use of a closure, and also makes the actual sequence of writes being done easier to see on the page. There is no clutter anymore. Programming with error values (and interfaces) has made the code nicer.\nIn fact, this pattern appears often in the standard library. The archive/zip and net/http packages use it. More salient to this discussion, the bufio package’s Writer is actually an implementation of the errWriter idea. Although bufio.Writer.Write returns an error, that is mostly about honoring the io.Writer interface. The Write method of bufio.Writer behaves just like our errWriter.write method above, with Flush reporting the error, so our example could be written like this:\n1 2 3 4 5 6 7 8 b := bufio.NewWriter(fd) b.Write(p0[a:b]) b.Write(p1[c:d]) b.Write(p2[e:f]) // and so on if b.Flush() != nil { return b.Flush() } There is one significant drawback to this approach, at least for some applications: there is no way to know how much of the processing completed before the error occurred. If that information is important, a more fine-grained approach is necessary. Often, though, an all-or-nothing check at the end is sufficient.\nWe’ve looked at just one technique for avoiding repetitive error handling code. Keep in mind that the use of errWriter or bufio.Writer isn’t the only way to simplify error handling, and this approach is not suitable for all situations. The key lesson, however, is that errors are values and the full power of the Go programming language is available for processing them.\nReferences:\nError handling and Go - The Go Programming Language Errors are values - The Go Programming Language ","date":"2023-09-08T09:13:06Z","permalink":"https://blog.yorforger.cc/p/error-handling-go/","title":"Error handling - Go"},{"content":" # 1. Four types of parameter Default argument Keyword arguments (named arguments) Positional arguments Arbitrary arguments (variable-length arguments *args and **kwargs) # 2. Parameters vs arguments 1 2 3 4 5 6 # a and b here are pamameter def my_sum(a, b): # function definition returen a + b # a and 99 are arguments my_sum(1, 99) # function call # 3. Positional arguments vs keyword argumentss 1 2 def add(lhs, rhs): ... For this function we have many ways to call it, add(10, 20), add(lhs=10, rhs=20) and add(10, rhs=20),\nIn add(10, 20), 10, 20 are positional arguments,\nIn add(lhs=10, rhs=20), lhs=10 and rhs=20 are keyword arguments,\nIn add(10, rhs=20), 10 is positional arguments, rhs=20 is keyword arguments,\n# 4. Arbitrary positional arguments (*args) We can pass multiple arguments to the function. Internally all these values are represented in the form of a tuple.\n1 2 3 4 5 def percentage(sub1, sub2, sub3): avg = (sub1 + sub2 + sub3) / 3 print(\u0026#39;Average\u0026#39;, avg) percentage(56, 61, 73) This function works, but it’s limited to only three arguments.\n1 2 3 4 5 6 7 8 9 10 11 # function with variable-length arguments def percentage(*args): sum = 0 for i in args: # get total sum = sum + i # calculate average avg = sum / len(args) print(\u0026#39;Average =\u0026#39;, avg) percentage(56, 61, 73) # 5. Arbitrary keyword arguments (**kwargs) arbitrary keyword arguments use dictionary to store keyword arguments,\n1 2 3 4 5 6 7 8 9 10 11 12 # function with variable-length keyword arguments def percentage(**kwargs): sum = 0 for sub in kwargs: # get argument name sub_name = sub # get argument value sub_marks = kwargs[sub] print(sub_name, \u0026#34;=\u0026#34;, sub_marks) # pass multiple keyword arguments percentage(math=56, english=61, science=73) Output:\n1 2 3 math = 56 english = 61 science = 73 We can learn a lot from the source code if some python program:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def __prepare_create_request( cls, api_key=None, api_base=None, api_type=None, api_version=None, organization=None, **params, ): deployment_id = params.pop(\u0026#34;deployment_id\u0026#34;, None) engine = params.pop(\u0026#34;engine\u0026#34;, deployment_id) model = params.get(\u0026#34;model\u0026#34;, None) timeout = params.pop(\u0026#34;timeout\u0026#34;, None) stream = params.get(\u0026#34;stream\u0026#34;, False) headers = params.pop(\u0026#34;headers\u0026#34;, None) model = params.get(\u0026quot;model\u0026quot;, None) : It tries to retrieve the \u0026ldquo;model\u0026rdquo; key from the params dictionary, If that key exists, it uses that value, If that key does not exist, it uses the default value None.\ndeployment_id = params.pop(\u0026quot;deployment_id\u0026quot;, None): Same as params.get() but it will removes that key.\nReferences:\nPython Function Arguments [4 Types] – PYnative Dictionary - Python ","date":"2023-09-07T22:51:26Z","permalink":"https://blog.yorforger.cc/p/variable-parameter-python/","title":"Variable Parameter Python"},{"content":" # 1. Useful commands 1 $ go test -run \u0026#34;ExampleSession|TestSession\u0026#34; -v Note that the -run flag is used for specifying test functions, not benchmarks, not fuzz test. Because example test is similar to test function, you can specify both example and test function with -run flag. If you want to specify benchmarks, use -bench flag:\n1 2 3 4 5 6 7 8 9 10 -bench regexp Run benchmarks matching the regular expression. -run regexp Run only tests and examples matching the regular expression. # Flags below can be used with flags above. -race enable data race detection. -cover enable code coverage instrumentation. Note that -bench does not disable the normal test selection. If you want run the benchmark test only you should pass -run=xxx to disable the normal test selection.\n1 2 3 4 go test -run=xxx -bench \u0026#39;BenchmarkDirectRead|BenchmarkLimitedRead\u0026#39; # just one function, no single quote is fine: $ go test -run=xxx -bench BenchmarkStore Learn more: go command - cmd/go - Go Packages\n# 2. -bench \u0026amp; -benchmem The benchmark tool only reports heap allocations. Stack allocations via escape analysis are less costly, possibly free, so are not reported.\n1 2 3 ❯ go test -run=xxx -bench \u0026#39;BenchmarkDirectRead|BenchmarkLimitedRead\u0026#39; -benchmem BenchmarkDirectRead-8 38036484 31.47 ns/op 48 B/op 1 allocs/op BenchmarkLimitedRead-8 12315 97480 ns/op 72 B/op 2 allocs/op The -8 indicates that these benchmarks were run with a GOMAXPROCS value of 8, meaning up to 8 OS threads were used concurrently. This is typically set to the number of CPU cores available.\n38036484`` for BenchmarkDirectRead and 12315`` for BenchmarkLimitedRead are the number of iterations the benchmark function was executed.\n48 B/op: This means that on average, 48 bytes are being allocated per operation (op) in the benchmarked function.\nSource:\nhttps://stackoverflow.com/questions/56832207/golang-benchmark-why-does-allocs-op-show-0-b-op https://stackoverflow.com/a/35588683/16317008 ","date":"2023-09-05T15:10:18Z","permalink":"https://blog.yorforger.cc/p/unit-test-basic-commands-go/","title":"Unit Test Basic Commands - Go"},{"content":" # 1. Different behaviors - pointer and value receiver There are two reasons to use a pointer receiver:\nThe first is so that the method can modify the value that its receiver points to.\nThe second is to avoid copying the value on each method call. This can be more efficient if the receiver is a large struct, for example.\n\u0026ldquo;the value\u0026rdquo; above refers to an object of the struct, but there is no object in golang, we call all of them value.\nI\u0026rsquo;ll explain it to you the different behaviors between pointer receiver and value receiver first, see the code below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 type Person struct { name string } func (p Person) foo() *Person { // As in all languages in the C family, everything in Go is passed by value. // \u0026#39;p\u0026#39; here, is a copy of \u0026#39;coco\u0026#39; return \u0026amp;p } func (p Person) setName(name string) { // \u0026#39;p\u0026#39; here, is a copy of \u0026#39;coco\u0026#39; p.name = name } func (p Person) getName() string { return p.name } func main() { coco := Person{name: \u0026#34;Coco\u0026#34;} coco.setName(\u0026#34;Bella\u0026#34;) fmt.Println(coco.getName()) // print: Coco } As you can see here, value receiver method will make a copy of that \u0026ldquo;object\u0026rdquo;. There is no object in golang, using term \u0026ldquo;object\u0026rdquo; here is for easy understanding. This is that if you want to avoid copying the value of the struct on each method call, try to use a pointer receiver. And if you want modify the value\u0026rsquo;s fields, try to use a pointer receiver.\nI find a snippet in go source code, e.g.,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type Handler interface { ServeHTTP(ResponseWriter, *Request) } type HandlerFunc func(ResponseWriter, *Request) // 1. We don\u0026#39;t need to modify any fileds of the value of HandlerFunc, just a function // 2. There is no a lot copies, when we call this method, just a copy of function type, // small as a pointer so choosing a value receiver probably more appropriate here // or thinking in this way probably better // I know what I do, this is a function type, it has no filed // no concurrency issues we mentioned above func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) } NOTE: Generally, in practice, we seldom use pointer types whose base types are slice types, map types, channel types, function types, string types and interface types. The costs of copying values of these assumed base types are very small.\nSource: https://go101.org/article/value-copy-cost.html\n# 2. Method receivers in concurrency I came across a satement about when to use value receiver but forget where I found:\nYou should notice that value receivers are concurrency safe, while pointer receivers are not concurrency safe. So if there is no a lot copy, and you don\u0026rsquo;t need modify any field of the value, try to use value receiver.\nIs this correct, yes it\u0026rsquo;s correct to some extend, but things probably are more complicated when come across concurrent programming.\nI find a good blog talks about this written by Dave Cheney, and I\u0026rsquo;ll share some parts of the blog here:\nObviously if your method mutates its receiver, it should be declared on *T. However, if the method does not mutate its receiver, is it safe to declare it on T instead *T?\nIt turns out that the cases where it is safe to do so are very limited. For example, it is well known that you should not copy a sync.Mutex value as that breaks the invariants of the mutex. As mutexes control access to other things, they are frequently wrapped up in a struct with the value they control:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package counter type Val struct { mu sync.Mutex val int } func (v *Val) Get() int { v.mu.Lock() defer v.mu.Unlock() return v.val } func (v *Val) Add(n int) { v.mu.Lock() defer v.mu.Unlock() v.val += n } Most Go programmers know that it is a mistake to forget to declare the Get or Add methods on the pointer receiver *Val. However any type that embeds a Val to utilise its zero value, must also only declare methods on its pointer receiver otherwise it may inadvertently copy the contents of its embedded type’s values.\n1 2 3 4 5 6 7 type Stats struct { a, b, c counter.Val } func (s Stats) Sum() int { return s.a.Get() + s.b.Get() + s.c.Get() // whoops } A similar pitfall can occur with types that maintain slices of values, and of course there is the possibility for an unintended data race.\nIn short, I think that you should prefer declaring methods on *T unless you have a strong reason to do otherwise.\n","date":"2023-09-02T22:18:20Z","permalink":"https://blog.yorforger.cc/p/methods-receivers-concurrency-go/","title":"Methods Receivers \u0026 Concurrency - Go"},{"content":" # Problem On github shwezhu.github.io repository I have two branch already, one is master for static html files that is generate web page, another is backup for backup of my hexo themes files and blogs.\nI want backup the hugo theme files with my blog files.\n# Solution I\u0026rsquo;m using hugo now but I don\u0026rsquo;t want to override or delete the theme files for hexo, therefore, I cannot just fetch remote branch backup and merge it with my local branch and then push. This will override the files stored on backup before.\n1 2 3 4 # should not do this git fetch origin backup git branch -a git merge origin/backup --allow-unrelated-histories So I create a new branch called backup-hugo, and push all files to this branch.\n1 2 3 4 5 6 7 cd blogs git init # you can add .gitignore here git add . git commit... git branch -M backup-hugo # rename current branch git push -u origin backup-hugo # Other Findings During this process, when I do commit I got wrong message that there is another git repository under themes/even/, however, I have do git add ., what should I do now is to go to themes/even/ and delete .git file, then undo git add which is called unstage file from index area,\n1 2 3 4 5 6 cd themes/even. rm -rf .git cd .. \u0026amp;\u0026amp; cd .. git rm --cached themes/even/.git git add . git commit... git rm --cached is a very useful command, it can make file from tracked to untracked status.\n","date":"2023-09-01T14:12:50Z","permalink":"https://blog.yorforger.cc/p/backup-blogs-with-git-sovled/","title":"Backup Blogs with Git - Sovled"},{"content":" # 1. Run a script A hello world script:\n1 2 #!/usr/bin/bash echo \u0026#34;hello world\u0026#34; The first here specifies which shell should execute this script, if you don\u0026rsquo;t write, the default shell on your computer will run it, check all the shells on your computer:\n1 2 3 4 5 6 7 8 9 10 11 $ cat /etc/shells # List of acceptable shells for chpass(1). # Ftpd will not allow users to connect who are not using # one of these shells. /bin/bash /bin/csh /bin/dash /bin/ksh /bin/sh /bin/tcsh /bin/zsh Don\u0026rsquo;t write a wrong location of bash interpreter, /bin/nmsd below doesn\u0026rsquo;t exist, execute this script will go wrong:\n1 2 3 4 5 6 7 $ cat script.sh #!/bin/nmsd echo \u0026#34;hello world\u0026#34; $ chmod u+x script.sh $ ./script.sh zsh: ./a.sh: bad interpreter: /bin/ash: no such file or directory chmod u+x will made the file executable for your user (it will only add it for your user, though it may be already executable by the group owner, or \u0026ldquo;other\u0026rdquo;). chmod a+x (\u0026lsquo;all plus executable bit\u0026rsquo;) makes the file executable by everyone. The u is standing for \u0026ldquo;user\u0026rdquo;. That means that the executable flag only applies to the current user of the file.\nu for user (the user who is owner of the file g for group (other users in the file group o for others (users not in the file group a for all (all users # 2. Bash variables are untyped Bash does not type variables like C and related languages, defining them as integers, floating points, or string types. In Bash, all variables are strings. A string that is an integer can be used in integer arithmetic, which is the only type of math that Bash is capable of doing.\nThe assignment VAR=10 sets the value of the variable VAR to 10. To print the value of the variable, you can use the statement echo $VAR.\nNote: The syntax of variable assignment is very strict. There must be no spaces on either side of the equal = sign in the assignment statement.\nIt is not suitable for scientific computing or anything that requires decimals, such as financial calculations.\n# 3. for loop and array 1 2 3 4 5 6 for item in (list) do command_one command_two ... done e.g.,\n1 2 3 4 5 6 7 #!/usr/bin/bash archs=(amd64 arm64 ppc64le ppc64 s390x) for arch in ${archs} do env GOOS=linux GOARCH=${arch} go build -o prepnode_${arch} done 1 2 3 4 5 # rename all file under current folder form lowercase to capital for file in *; do mv \u0026#34;$file\u0026#34; `echo $file | tr \u0026#39;a-z\u0026#39; \u0026#39;A-Z\u0026#39;` done But this isn\u0026rsquo;t the best choice, the command below will rename all files recursively which the code above cannot do,\n1 2 $ brew install rename $ find . -depth -execdir rename -f \u0026#39;y/A-Z/a-z/\u0026#39; {} \\; You probably wonder what is the \u0026quot;\u0026quot;, * and ; above, I\u0026rsquo;ll explain it to you next.\n# 4. Control operators ;: Will run one command after another has finished, irrespective of the outcome of the first. This means even there is wrong in fiest command, the second can execute. \u0026amp;\u0026amp;: if the precceeding command goes wrong, the commands left won\u0026rsquo;t be executed. ||: It executes the command on the right only if the command on the left returned an error. 1 2 3 4 5 6 7 8 9 10 11 # even command `cd fd.sh` goes wrong, command `ls` can be executed $ cd fd.sh; ls cd: not a directory: fd.sh economic fd.sh test.sh $ cd fd.sh \u0026amp;\u0026amp; ls cd: not a directory: fd.sh $ cd fd.sh || ls cd: not a directory: fd.sh economic fd.sh test.sh # 5. Quotes # 5.1. Why need qoutes Quoting is used to remove the special meaning of metacharacters or words to the shell. Quoting can be used to disable special treatment for special characters, to prevent reserved words from being recognized as such, and to prevent parameter expansion.\nMetacharacter: a character that, when unquoted, separates words. A metacharacter is a space, tab, newline, or one of the following characters: *, ?, |, \u0026amp;, ;, (, ), \u0026lt;, or \u0026gt;, etc\u0026hellip;\nParameter Expansion: The $ character introduces parameter expansion, command substitution, or arithmetic expansion. source\nFor example, you want to print \u0026lt;-$1500.**\u0026gt;; (update?) [y|n], but \u0026lt;\u0026gt;*;$ has a special meaning in bash, and printing it directly won\u0026rsquo;t work.\nYou can use escape character \\ to solve this problem:\n1 2 bash-3.2$ echo \\\u0026lt;-\\$1500.\\*\\*\\\u0026gt;\\; \\(update\\?\\) \\[y\\|n\\] \u0026lt;-$1500.**\u0026gt;; (update?) [y|n] But this is not elegant, you can use single quote to solve this problem:\n1 2 bash-3.2$ echo \u0026#39;\u0026lt;-$1500.**\u0026gt;; (update?) [y|n]\u0026#39; \u0026lt;-$1500.**\u0026gt;; (update?) [y|n] Why does this work? Because all special characters between single quotes lose their special meaning, I find a table will help:\nSr.No. Quoting \u0026amp; Description 1 **Single quote ** All special characters between these quotes lose their special meaning. 2 Backslash \\ Any character immediately following the backslash loses its special meaning. 3 Backquote Anything in between back quotes would be treated as a command and would be executed. 4 The characters $ and \u0026lsquo;`\u0026rsquo; retain their special meaning within double quotes. # 5.2. Backqoute You may wonder what does back quote mean in the table, I\u0026rsquo;ll give you an example:\n1 2 3 4 $ cat a.txt hello world $ A=`cat a.txt`; echo \u0026#34;$A\u0026#34; hello world The code between back quotes will be executed, and if there is output, you can save it into a variable,\n# 5.3. Double quotes Sometimes * may acts like it remains its spcecial meaning in double quotes:\nWithout any special options:\nInside double quotes (\u0026quot;), the asterisk * retains its literal meaning and does not perform globbing (expansion to match filenames).\nFor example, \u0026quot;*\u0026quot; will be treated as a literal asterisk character.\nWith some spcecial options:\nfind . -name \u0026quot;*.sh\u0026quot; -type f , in the command the -name \u0026quot;*.sh\u0026quot; the \u0026quot;*.sh\u0026quot; acts as a pattern, the double quotes prevent the shell from interpreting the asterisk as a wildcard character and expanding it to match filenames in the current directory. Therefore, in the given command, \u0026quot;*.c\u0026quot; is treated as a literal string by the shell. The find command receives the pattern \u0026quot;*.c\u0026quot; as an argument, and it performs the matching internally during its execution. Which acts like the * doesn\u0026rsquo;t lose its special meaning. # 6. $ \u0026amp; * in bash # 6.1. $ $ has many use cases, I\u0026rsquo;ll introduce one here, in bash scripting, variables are used to store values, the $ symbol is used to retrieve the value of a variable Value.\n1 name=\u0026#34;Mark\u0026#34;; echo \u0026#34;My name is $name\u0026#34; ⚠ Don\u0026rsquo;t forget $ and \u0026lsquo;back quote\u0026rsquo; sign retain their special meaning between double quote.\n# 6.2. * ⚠ * will lose its special meaning between double quote.\nThe Shell (bash) considers an asterisk * to be a wildcard character that can match one or more occurrences of any character, including no character.\nThe “*” character is a shortcut for “everything”. Thus, if you enter ls *, you will see all of the contents of a given directory.\nls *fq lists every file that ends with a fq. ls /usr/bin/*.sh This lists every file in /usr/bin directory that ends in the characters .sh. * can be placed anywhere in your pattern: ls Mov10*fq Now when you check the command we did before, you may understand:\n1 2 3 4 5 # rename all file under current folder form lower-case to upper-case for file in *; do mv \u0026#34;$file\u0026#34; `echo $file | tr \u0026#39;a-z\u0026#39; \u0026#39;A-Z\u0026#39;` done # 5. Arithmetic expansion Parameter Expansion: The $ character introduces parameter expansion, command substitution, or arithmetic expansion. source\nThe behaviour of bash interpreter is different from other programming interpreter so if you write code like below:\n1 2 3 #!/usr/bin/bash $ a=3; b=5; c=a+b; echo $c # Print: a+b You may not get the result you want, the correct way to do this:\n1 2 3 $ a=3; b=5; c=$((a+b)); echo $c $ a=3; b=5; echo $((a+b)) $ a=3; b=5; let c=\u0026#34;a+b\u0026#34;; echo $c $((...)) is called arithmetic expansion, which is typical of the bash and ksh shells. This allows doing simple integer arithmetic, no floating point stuff though. The result of the expression replaces the expression, as in echo $((1+1)) would become echo 2 ((...)) is referred to as arithmetic evaluation and can be used as part of if ((...)); then or while ((...)) ; do statements. Arithmetic expansion $((..)) substitutes the output of the operation and can be used to assign variables as in i=$((i+1)) but cannot be used in conditional statements. let is a bash and ksh keyword which allows for variable creation with simple arithmetic evaluation. If you try to assign a string there like let a=\u0026quot;hello world\u0026quot; you\u0026rsquo;ll get a syntax error. # 6. Conclusion There must be no spaces on either side of the equal = sign in the assignment statement. Three control operators: ;, ||, \u0026amp;\u0026amp; set defalul shell: chsh -s /bin/zsh, switch shell temporarily: shell-name + enter check version of shell: echo \u0026quot;$ZSH_VERSION\u0026quot;, echo \u0026quot;$BASH_VERSION\u0026quot; check all shells: cat /etc/shells References:\nbash - \u0026lsquo;chmod u+x\u0026rsquo; versus \u0026lsquo;chmod +x\u0026rsquo; - Ask Ubuntu Linux: Difference between \u0026ldquo;chmod +x\u0026rdquo; and \u0026ldquo;chmod u+x\u0026rdquo; Quoting (Bash Reference Manual) Unix / Linux - Shell Quoting Mechanisms bash - Understanding backtick (`) - Unix \u0026amp; Linux Stack Exchange Shell Expansions (Bash Reference Manual) The Shell | Introduction to the command line interface (Shell) ","date":"2023-09-01T12:23:59Z","permalink":"https://blog.yorforger.cc/p/shell-script-basic-syntax/","title":"Shell Script Basic Syntax"},{"content":" # 1. Recover 1 $ hugo new site blogs \u0026amp;\u0026amp; cd blogs # 2. Deploy to Github Pages # 2.1. Local Repository 1 2 3 4 5 6 $ cd blogs/public/ $ git init $ git remote add origin git@github.com:shwezhu/shwezhu.github.io.git ... $ git branch -M master $ git push origin master # 2.2. Remote Repository Go to Settings -\u0026gt; Pages -\u0026gt; Build and deployment\nSource: depoly from a branch\nBranch: choose master\nThen go to Actions options, you can see your page is building in progess, when it finished, visit: https://your-username.github.io\nYou probably need wait about 10 mintes even your the processof building bolgs has been finished, try to clear cache of your browser and restart your browser if you constantly get 404 page for yor website.\n# 3. Custom Domain (optional) Update DNS Records of your domain according to the provided ip address by github:\nGo to Settings -\u0026gt; Pages -\u0026gt; Custom domain: input your domain. And wait about 15 mintes to take effect (try to clear your browser cache).\nIf https cannot enable, try to remove your custom domain and wait minutes add again. The CNAME DNS Records has to be set towww, when I set blog the https cannot be enabled.\nLearn more: https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site\n# 4. Issues - 404 page Hugo will make the url into lowercase automatically, therefore, make sure all you blog files and folders under /content/post/ are lowercase. Otherwise, you will get 404 for that blog.\nAnd if you didn\u0026rsquo;t make it lowercase before generate html pages with hugo command, then you need delete all the generated html files under /public/post/ then run hugo command again, this should work.\n# 5. Insert Images Put images under /static/file-name/ and insert image with![](/file-name/xxx.png),\nOn Typora, go to settings -\u0026gt; images -\u0026gt; When instert, choose custome folder and write someting like this:\n1 /Users/David/blogs/static/${filename} then, write typora-root-url: ../../../static at front matter of blog, if you don\u0026rsquo;t know how to wtite, you can choose option:\nFromat -\u0026gt; Image -\u0026gt; Use Image Root Path, then typora will add someting like typora-root-url: ../../../static in yur md file at the front matter part. Then both typora and website will disply image correctly.\n# 6. Backup \u0026amp; Recover # 6.1. Backup Please check: Backup Blogs with Git - David\u0026rsquo;s Blog\n# 6.2. Recover 1 hugo new site blogs Copy the files stored in github into folder blogs/, then\n1 hugo server and visit localhost to check\u0026hellip;\nReferences: https://olowolo.com/post/hugo-quick-start/\n","date":"2023-08-31T23:51:57Z","permalink":"https://blog.yorforger.cc/p/write-blog-with-hugo-deploy-to-github-pages/","title":"Write Blog with Hugo \u0026 Deploy to Github Pages"},{"content":" # 1. Why do we need encoding To transmit a data structure across a network or to store it in a file, it must be encoded and then decoded again. Cause computer just know binary.\nThere are many encodings available, of course: JSON, XML, Google’s protocol buffers, and more. And now there’s another, provided by Go’s gob package.\n# 2. encoding/gob # 2.1. Why gob Why define a new encoding? It’s a lot of work and redundant at that. Why not just use one of the existing formats? Well, for one thing, we do! Go has packages supporting all the encodings just mentioned (the protocol buffer package is in a separate repository but it’s one of the most frequently downloaded). And for many purposes, including communicating with tools and systems written in other languages, they’re the right choice.\nBut for a Go-specific environment, such as communicating between two servers written in Go, there’s an opportunity to build something much easier to use and possibly more efficient.\nGob is much more preferred when communicating between Go programs. However, gob is currently supported only in Go and, well, C, so only ever use that when you\u0026rsquo;re sure no program written in any other programming language will try to decode the values. source\n# 2.2. Google’s Protocol Buffers misfeatures Gobs implements thress important features compared with Google\u0026rsquo;s Protocol Buffers:\nThe type being encoded does\u0026rsquo;t need to be a struct, it can be a map, slice, array etc\u0026hellip; Don\u0026rsquo;t need all fields of a type exist when decoding and encoding. If the varibale being transmitted has \u0026ldquo;zero value\u0026rdquo; for its type, it doesn\u0026rsquo;t need to be transmitted. Decoder know its type, it will set its default value automatically. # 2.3. How does gob work - value of encoded gob data is just integer The encoded gob data isn’t about types like int8 and uint16. Instead, somewhat analogous to constants in Go, its integer values are abstract, sizeless numbers, either signed or unsigned. When you encode an int8, its value is transmitted as an unsized, variable-length integer. When you encode an string, its value is also transmitted as an unsized, variable-length integer.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func main() { // Initialize the encoder and decoder. Normally enc and dec would be // bound to network connections and the encoder and decoder would // run in different processes. var network bytes.Buffer // Stand-in for a network connection enc := gob.NewEncoder(\u0026amp;network) // Will write to network. dec := gob.NewDecoder(\u0026amp;network) // Will read from network. message := \u0026#34;hello, there\u0026#34; // Encode (send) the value. _ = enc.Encode(message) fmt.Println(network.Bytes()) // Decode (receive) the value. var ms string _ = dec.Decode(\u0026amp;ms) fmt.Println(network.Bytes()) fmt.Println(ms) } ------------------------------------------ [15 12 0 12 104 101 108 108 111 44 32 116 104 101 114 101] [] hello, there As you can see, all encoded data is a variable-length integer, bytes.Buffer is just a struct, network is an object of it, network.Bytes() returns a slice holding the unread portion of the buffer so it print [] on second line,\n1 2 3 4 5 type Buffer struct { buf []byte // contents are the bytes buf[off : len(buf)] off int // read at \u0026amp;buf[off], write at \u0026amp;buf[len(buf)] lastRead readOp // last read operation, so that Unread* can work correctly. } Besides, enc.Encode(message) does two things: encode message and transmit it, similar todec.Decode(\u0026amp;ms).\n# 2.4. Values are flattened A stream of gobs is self-describing. Each data item in the stream is preceded by a specification of its type, expressed in terms of a small set of predefined types. Pointers are not transmitted, but the things they point to are transmitted; that is, the values are flattened. Nil pointers are not permitted, as they have no value. Recursive types work fine, but recursive values (data with cycles) are problematic. source\nI find a blog, which implement a function that can deep copy a map with gobs, even the map has a map inside. Golang: deepcopy map[string]interface{}. Could be used for any other Go type with minor modifications.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // Package deepcopy provides a function for deep copying map[string]interface{} // values. Inspired by the StackOverflow answer at: // http://stackoverflow.com/a/28579297/1366283 // // Uses the golang.org/pkg/encoding/gob package to do this and therefore has the // same caveats. // See: https://blog.golang.org/gobs-of-data // See: https://golang.org/pkg/encoding/gob/ package deepcopy import ( \u0026#34;bytes\u0026#34; \u0026#34;encoding/gob\u0026#34; ) func init() { gob.Register(map[string]interface{}{}) } // Map performs a deep copy of the given map m. func Map(m map[string]interface{}) (map[string]interface{}, error) { var buf bytes.Buffer enc := gob.NewEncoder(\u0026amp;buf) dec := gob.NewDecoder(\u0026amp;buf) err := enc.Encode(m) if err != nil { return nil, err } var copy map[string]interface{} err = dec.Decode(\u0026amp;copy) if err != nil { return nil, err } return copy, nil } # 2.5. Types on the wire The first time you send a given type, the gob package includes in the data stream a description of that type. In fact, what happens is that the encoder is used to encode, in the standard gob encoding format, an internal struct that describes the type and gives it a unique number. (Basic types, plus the layout of the type description structure, are predefined by the software for bootstrapping.) After the type is described, it can be referenced by its type number.\nThus when we send our first type T, the gob encoder sends a description of T and tags it with a type number, say 127. All values, including the first, are then prefixed by that number, so a stream of T values looks like:\n1 (\u0026#34;define type id\u0026#34; 127, definition of type T)(127, T value)(127, T value), ... CommonType holds elements of all types. It is a historical artifact, kept for binary compatibility and exported only for the benefit of the package\u0026rsquo;s encoding of type descriptors. It is not intended for direct use by clients.\n1 2 3 4 type CommonType struct { Name string Id typeId } # 2.6. Functions and channels Functions and channels will not be sent in a gob. Attempting to encode such a value at the top level will fail. A struct field of chan or func type is treated exactly like an unexported field and is ignored.\n# 2.7. gob.Register method Register records a type, identified by a value for that type, under its internal type name. That name will identify the concrete type of a value sent or received as an interface variable. Only types that will be transferred as implementations of interface values need to be registered. Expecting to be used only during initialization, it panics if the mapping between types and names is not a bijection.\n1 func Register(value any) If you\u0026rsquo;re dealing with concrete types (structs) only, you don\u0026rsquo;t really need it. Once you\u0026rsquo;re dealing with interfaces you must register your concrete type first.\nFor example, let\u0026rsquo;s assume we have these struct and interface (the struct implements the interface):\n1 2 3 4 5 6 7 8 9 10 11 type Getter interface { Get() string } type Foo struct { Bar string } func (f Foo) Get() string { return f.Bar } To send a Foo over gob as a Getter and decode it back, we must first call\n1 gob.Register(Foo{}) So the flow would be:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // init and register buf := bytes.NewBuffer(nil) gob.Register(Foo{}) // create a getter of Foo g := Getter(Foo{\u0026#34;wazzup\u0026#34;}) // encode enc := gob.NewEncoder(buf) enc.Encode(\u0026amp;g) // decode dec := gob.NewDecoder(buf) var gg Getter if err := dec.Decode(\u0026amp;gg); err != nil { panic(err) } Now try removing the Register and this won\u0026rsquo;t work because gob wouldn\u0026rsquo;t know how to map things back to their appropriate type.\n# 2.8. gob.Register When there is an interface, be careful, you should figure out all the possiable concrete types (implementations) of the interface would be, and if these concrete type is not primitive type, you need register for them. You don\u0026rsquo;t need to register for interface itself.\n1 2 3 4 5 6 7 8 gob.Register(map[string]int{}) expectedCopy := map[string]interface{}{ \u0026#34;id\u0026#34;: \u0026#34;0007\u0026#34;, \u0026#34;cats\u0026#34;: map[string]int{ \u0026#34;kitten\u0026#34;: 3, \u0026#34;milo\u0026#34;: 1, }, } If you don\u0026rsquo;t register for map[string]int you will get an error\n1 error: gob: type not registered for interface: map[string]int All of this because we have map[string]interface{}, there is an interface, according to gob package, Only types that will be transferred as implementations of interface values need to be registered.\nYou need to register for nothing if expectedCopy type is map[string]map[string]int (because there is no interface):\n1 2 3 4 5 6 7 8 // don\u0026#39;t need this: gob.Register(map[string]map[string]int{}) // don\u0026#39;t need this: gob.Register(map[string]int{}) expectedCopy := map[string]map[string]int{ \u0026#34;cats\u0026#34;: map[string]int{ \u0026#34;kitten\u0026#34;: 3, \u0026#34;milo\u0026#34;: 1, }, } Note that you don\u0026rsquo;t need to register for a slice of primitive type or primitive type itself when they are the implementations of an interface, because Go has done that for you:\n1 2 3 4 5 6 7 8 9 10 func registerBasics() { Register(int(0)) ... Register(float32(0)) Register(complex64(0i)) Register([]uint(nil)) ... Register([]bool(nil)) Register([]string(nil)) } Therefore, if you want encode expectedOriginal below, you need register for nothing:\n1 2 3 4 // Go has done this for use: gob.Register([]string{}) expectedOriginal: map[string]interface{}{ \u0026#34;cats\u0026#34;: []string{\u0026#34;Coco\u0026#34;, \u0026#34;Bella\u0026#34;}, }, If the implementation\u0026rsquo;s type of the interface is a custom type, you have to register for that type:\n1 2 3 4 5 6 7 8 type Cat struct { Name string } // you have to register for Cat gob.Register(Cat{}) expectedOriginal := map[string]interface{}{ \u0026#34;cats\u0026#34;: Cat{Name: \u0026#34;jack\u0026#34;}, } Similarly to we have talked above:\n1 2 3 4 5 // don\u0026#39;t need this: gob.Register(Cat{}) // there is no interface expectedOriginal := map[string]Cat { \u0026#34;cats\u0026#34;: Cat{Name: \u0026#34;jack\u0026#34;}, } And don\u0026rsquo;t forget, the first letter of the field of Cat must be Capital, namely, expored fields, otherwise, it (the field) won\u0026rsquo;t encode by gob.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type Cat struct { Name string } func main() { m := map[interface{}][]Cat{ \u0026#34;cats\u0026#34;: []Cat{{Name: \u0026#34;jack\u0026#34;}}, } buf := new(bytes.Buffer) enc := gob.NewEncoder(buf) dec := gob.NewDecoder(buf) if err := enc.Encode(m); err != nil { fmt.Sprintf(\u0026#34;failed to copy map: %v\u0026#34;, err) } result := make(map[interface{}][]Cat) if err := dec.Decode(\u0026amp;result); err != nil { fmt.Sprintf(\u0026#34;failed to copy map: %v\u0026#34;, err) } fmt.Println(result) } # 3. encoding/json Marshal() → to encode GO values to JSON in string format Unmarshal() → to decode JSON data to GO values 1 2 func Marshal(v interface{}) ([]byte, error) func Unmarshal(data []byte, v interface{}) error # 3.1. json.Marshal() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 type Cat struct { // lowercase field cannot be exported // `json:\u0026#34;name\u0026#34;` makes \u0026#34;Name\u0026#34; to \u0026#34;name\u0026#34; in json string after apply json.Marshal() // check in output Name string `json:\u0026#34;name\u0026#34;` Age int IsAdult bool } func main() { data, _ := json.Marshal(Cat{ Name: \u0026#34;Kitten\u0026#34;, Age: 2, IsAdult: true, }) println(data) println(string(data)) data, _ = json.Marshal(\u0026#34;Hello\u0026#34;) println(data) println(string(data)) } ---------------------------------- [40/48]0x140000161b0 {\u0026#34;name\u0026#34;:\u0026#34;Kitten\u0026#34;,\u0026#34;Age\u0026#34;:2,\u0026#34;IsAdult\u0026#34;:true} [7/8]0x1400001c1a8 \u0026#34;Hello\u0026#34; ⚠️Note: Channel, complex, and function values cannot be encoded in JSON. Attempting to encode such a value causes Marshal to return an UnsupportedTypeError. json package - encoding/json - Go Packages\nIn the past, if you use json.Marshal() to encode map, you need to ensure that the type of the key is strin, otherwise, it will fail, It\u0026rsquo;s not because of Go, but because of Json: Json does not support anything else than strings for keys.\nLearn more: https://stackoverflow.com/questions/24284612/failed-to-json-marshal-map-with-non-string-keys\nBut now you can use json.Marshal() to encode the map whose key\u0026rsquo;s type is int, but not float:\n1 2 3 4 5 6 7 m := make(map[float32]string) m[3] = \u0026#34;helllo\u0026#34; b, err := json.Marshal(m) if err != nil { panic(err) } // panic: json: unsupported type: map[float32]string Therefore, you may want do something like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // JSONSerializer encode the session map to JSON. type JSONSerializer struct{} // Serialize to JSON. Will err if there are unmarshalable key values func (s JSONSerializer) Serialize(ss *sessions.Session) ([]byte, error) { m := make(map[string]interface{}, len(ss.Values)) for k, v := range ss.Values { ks, ok := k.(string) if !ok { err := fmt.Errorf(\u0026#34;Non-string key value, cannot serialize session to JSON: %v\u0026#34;, k) fmt.Printf(\u0026#34;redistore.JSONSerializer.serialize() Error: %v\u0026#34;, err) return nil, err } m[ks] = v } return json.Marshal(m) } Code from: https://github.com/boj/redistore\nNow support for non string key types for maps for json Marshal/UnMarshal has been added through the use of TextMarshaler and TextUnmarshaler interfaces here. You could just implement these interfaces for your key types and then json.Marshal would work as expected. Learn more: https://stackoverflow.com/a/55879732/16317008\n# 3.2. json.Unmarshal() 1 2 3 4 5 6 7 8 9 10 11 12 13 func main() { data, _ := json.Marshal(Cat{ Name: \u0026#34;Kitten\u0026#34;, Age: 2, IsAdult: true, }) cat := Cat{} _ = json.Unmarshal(data, \u0026amp;cat) fmt.Println(cat) } ---------------------------------- {Kitten 2 true} # 3.3. json.NewDecoder() \u0026amp; json.Unmarshal() Use json.Decoder if your data is coming from an io.Reader stream, or you need to decode multiple values from a stream of data. Use json.Unmarshal if you already have the JSON data in memory. If you check the signature, you should understand:\n1 2 3 func NewDecoder(r io.Reader) *Decoder func Unmarshal(data []byte, v any) error Source: https://stackoverflow.com/a/21198571/16317008\nReferences:\nGobs of data - The Go Programming Language gob package - encoding/gob - Go Packages difference between encoding/gob and encoding/json go - What\u0026rsquo;s the purpose of gob.Register method? - Stack Overflow go - gob.Register() by type or for each variable? - Stack Overflow Learn more:\nGolang Benchmark: gob vs json gob - The Go Programming Language ","date":"2023-08-31T11:38:20Z","permalink":"https://blog.yorforger.cc/p/encoding/gob-encoding/json-in-go/","title":"encoding/gob \u0026 encoding/json in Go"},{"content":" # Tricks # remove an element from a slice 1 2 3 // remove the element at index i from a a = append(a[:i], a[i+1:]...) # handle close() 1 2 3 4 5 6 7 8 9 10 11 12 13 // credit to: https://gist.github.com/benbjohnson/9eebd201ec096ab6430e1f33411e6427 func doSomething() error { f, err := os.Create(\u0026#34;foo\u0026#34;) if err != nil { return err } // ensure that it\u0026#39;s definitely closed by the end defer f.Close() if _, err := f.Write([]byte(\u0026#34;bar\u0026#34;); err != nil { return err } return f.Close() } Why we bother to care about getting error from f.Close()? We have got error from f.Write(). Well, there are a lot to say, learn more: Don\u0026rsquo;t Defer Close() on Writable Files - Go Notes\n# Common mistakes # 1. Encoding non-exported fields struct value with gobs gobs can encode the exported fields of a struct value, if a sturct without exported field, when you try encode its value, you will get a nil.\nFunctions and channels will not be sent in a gob. Attempting to encode such a value at the top level will fail. A struct field of chan or func type is treated exactly like an unexported field and is ignored.\n# 2. Use var to declare channel Variables declared without an explicit initial value are given their zero value. Zero value for a channel is nil, read and write a nil channel will block forever. The code below is a common mistake:\n1 2 3 4 5 6 7 8 9 go func() { var expiredSessions chan []*Session // expiredSessions == nil var errSession := chan error // errSession == nil store.GetExpiredSessions(expiredSessions, errSession) select { case sessions, _ := \u0026lt;-expiredSessions: ... } }() Don\u0026rsquo;t use var to declare channel, map or slice values, use make(), keep this convension you will won\u0026rsquo;t make mistakes.\nLearn more: https://davidzhu.xyz/post/golang/basics/003-collections/#5-var-vs-make\n# 3. Using goroutines on a loop iterator variable In Go, the loop iterator variable is a single variable that takes different values in each loop iteration.\n1 2 3 4 5 6 7 8 9 10 11 func main() { var out []*int for i := 0; i \u0026lt; 3; i++ { out = append(out, \u0026amp;i) } fmt.Println(\u0026#34;Values:\u0026#34;, *out[0], *out[1], *out[2]) fmt.Println(\u0026#34;Addresses:\u0026#34;, out[0], out[1], out[2]) } Values: 3 3 3 Addresses: 0x40e020 0x40e020 0x40e020 For example, you might write something like this, using a closure:\n1 2 3 4 5 6 // this is bad for _, val := range values { go func() { fmt.Println(val) }() } Because the closures are all only bound to that one variable, there is a very good chance that when you run this code you will see the last element printed for every iteration instead of each value in sequence, because the goroutines will probably not begin executing until after the loop.\nThe proper way to write that closure loop is:\n1 2 3 4 5 for _, val := range values { go func(val interface{}) { fmt.Println(val) }(val) } Avoid mixing anonymous functions and goroutines. Concurrency Made Easy\nLearn more:\nhttps://go.dev/doc/effective_go#channels https://github.com/golang/go/wiki/CommonMistakes ","date":"2023-08-27T17:12:55Z","permalink":"https://blog.yorforger.cc/p/tricks-and-common-mistakes-go/","title":"Tricks and Common Mistakes - Go"},{"content":"Original: https://qr.ae/pyztor\nWhy is a multithreaded web server better than a single thread server? It isn’t.\nThere are four basic ways how a web server can handle concurrency:\nforking an OS process per request (like old versions of Apache) spawning an OS thread per request (like a new versions of Apache) using a single-threaded event loop (like nginx) using green threads or lightweight processes scheduled by a VM runtime instead of the OS (like in Erlang) Currently the most common approaches are number 2 and 3.\nThere are pros and cons of both of them. For I/O-bound operations (a characteristic of a typical web server) you get better performance and higher number of concurrent requests when you use a single-threaded event loop. But the drawback is that you need to use exclusively asynchronous non-blocking I/O for all operations or otherwise you’ll block the event loop and lose performance. For that reason it’s easier to implement a multi-threaded server but you pay in performance.\nFor CPU-bound operations (less common for a usual web server, maybe more common for a computationally intensive API) it’s best to have one OS thread or process per core. It’s easy to do with single-threaded event loops because you can run a cluster of a number of processes one per core. It’s hard to do with multi-threaded servers because if spawning threads is your only way to handle concurrent requests then you cannot really control how many threads you will have - as you don’t control the number of requests. Once you have more threads than the number of CPU cores then you loose performance for context switches and you also use a lot of RAM.\nThat is why a single-threaded nginx server performs better than a multi-threaded Apache web server (and that is why nginx was created in the first place). Also Redis, a database known for exceptionally high performance is single-threaded.\nA real example I can give you is this: My first web server was Apache running on a Linux machine with 500MB of RAM. It forked a new process for every request (it actually had a pool so there was not much forking involved but it had to keep those processes alive to reuse them and kill them once in a while to avoid resource leakage).\nMy OS used around 100MB of RAM. Every Apache process used 20MB of RAM. It meant that my server could only handle 20 concurrent requests and there was no way around it because I had no more RAM. The processes were mostly blocked on I/O so the CPU utilization was very low, every request above those 20 had to wait and if those 20 was e.g. long running downloads then my server was completely unresponsive.\nWhen nginx web server was introduced it used a single-threaded event loop and didn’t block for any request. It could handle much more concurrent requests, having no problem with the mythical c10k problem - nginx was basically created to solve the c10k problem (10,000 concurrent requests).\nImagine how much RAM is wasted for 10,000 threads if you could even spawn that many and how much time is used for context switches.\nMemory usage of multi-threaded Apache vs single-threaded nginx:\nIncidentally, this is the reason why Ryan Dahl used a non-blocking I/O and a single-threaded event loop in Node.js and he still uses the same idea in Deno, because that is the way to write high performance network servers (contrary to what you might read in other answers here).\nNote that nginx “core functionality” doc mentions that on most servers nginx defaults to multiple workers (which will be ran as threads) so it’s not always “single threaded.” from a comment of this blog\n","date":"2023-08-26T19:03:59Z","permalink":"https://blog.yorforger.cc/p/is-multithreaded-server-better-than-a-single-thread-server/","title":"Is Multithreaded Server Better than a Single Thread Server?"},{"content":" # 1. Shell expansion When the shell receives a command, either from the user typing at the keyboard, or from a shell script, it breaks it up into words. After this happens, the shell performs seven operations on the words, which can change how they are interpreted. There are seven operations are collectively known as \u0026lsquo;shell expansion\u0026rsquo;.\nThe seven operations that the shell performs are:\nBrace Expansion - expanding values between braces, such as file{1..3} into file1 file2 file3 Tilde Expansion - expanding the ~ tilde symbol for the home directory into the path to the home directory, such as ~/effective-shell into /home/dwmkerr/effective-shell Parameter Expansion - expanding terms that start with a $ symbol into parameter values, such as $HOME into the value of the variable named HOME Command Substitution - evaluation of the contents of $(command) sequences, which are used to run commands and return the results to the shell command line Arithmetic Expansion - evaluation of the contents of $((expression)) sequences, which are used to perform basic mathematical operations Word Splitting - once all of the previous operations are run, the shell splits the command up into \u0026lsquo;words\u0026rsquo;, which are the units of text that you can run loops over Pathname Expansion - the shell expands wildcards and special characters in pathnames, such as *.txt into the set of files that are matched by the sequence Brace Expansion\n1 2 3 4 5 6 7 8 9 10 11 mkdir /tmp/{one,two,three} # The line above is expanded to: mdkir /tmp/one /tmp/two /tmp/three touch file{1..5}.txt # The line above is expanded to: touch file1.txt file2.txt file3.txt file4.txt file5.txt for x in {0..10..2}; do print $x; done # The line above is expanded to: for x in 0 2 4 6 8 10; do print $x; done Parameter Expansion\n1 2 3 4 fruit=apples echo \u0026#34;I like $fruit\u0026#34; # The line above is expanded to: echo \u0026#34;I like apples\u0026#34; When using parameter expansion it is generally preferable to surround the name of the parameter with braces - this allows you to tell the shell unambiguously what the name of the parameter is. For example:\n1 2 3 echo \u0026#34;My backup folder is: ${HOME}backup\u0026#34; # The line above is expanded to: echo \u0026#34;My backup folder is: /home/dwmkerrbackup\u0026#34; You should use\u0026quot;My backup folder is: ${HOME}backup\u0026quot;, not \u0026quot;I like $fruit\u0026quot; style.\nTilde Expansion\n1 2 3 cd ~/effective-shell # The line above is expanded to: cd $HOME/effective-shell Note that if you want check what the command exactly is after expansion, check it with echo:\n1 2 3 4 5 6 $ ls one.txt two.txt three.dat $ echo rm *.txt rm one.txt two.txt # 2. Wildcard pattern # 2.1. Wildcard pattern in shell A string is a wildcard pattern if it contains one of the characters ?, *, or [, Globbing is the operation that expands a wildcard pattern into the list of pathnames matching the pattern. Matching is defined by:\nA ? (not between brackets) matches any single character. A * (not between brackets) matches any string, including the empty string. Don\u0026rsquo;t forget * may lose its special meaning in double qoutes, but it depends, at some conditions, it won\u0026rsquo;t lost its special meaning for globing wich is filepath expansion. Learn more: Shell Script Basic Syntax - David\u0026rsquo;s Blog\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Expands to: tar xvf file1.tar file2.tar file42.tar ... tar xvf *.tar # Delete all files ends with .txt under current folder rm *.txt # with double, it won\u0026#39;t be expanded $ rm \u0026#34;*.txt\u0026#34; rm: *.txt: No such file or directory # search for a string in your current directory and all other subdirectories $ grep -r ‘hello’ * a.txt:hello world sub/c.txt:hello, this is... sub/b.txt:hello, this is... # check what the command is after expansion $ echo grep -r ‘hello’ * # 2.2. Metacharacter in regular expressions In regular expression the * is a metacharacter that represents zero or more occurrences of the preceding element.\n1 2 3 4 $ printf \u0026#34;colour\\ncolor\\ncolouur\\n\u0026#34; | egrep \u0026#39;colou*r\u0026#39; colour color colouur References:\nglob(7) - Linux manual page\nUnderstanding Shell Expansion | Effective Shell\n","date":"2023-08-26T11:30:20Z","permalink":"https://blog.yorforger.cc/p/shell-expansion/","title":"Shell Expansion"},{"content":" # 1. Concurrency Safe In a program with two or more goroutines, the steps within each goroutine happen in the familiar order, but in general we don’t know whether an event x in one goroutine happens before an event y in another goroutine, or happens after it, or is simultaneous with it. When we cannot confidently say that one event happens before the other, then the events x and y are concurrent.\nThat function is concurrency-safe if it continues to work correctly even when called concurrently, that is, from two or more goroutines with no additional synchronization. We can generalize this notion to a set of collaborating functions, such as the methods and operations of a particular type. A type is concurrency-safe if all its accessible methods and operations are concurrency-safe.\nConcurrency-safe types are the exception rather than the rule, so you should access a variable concurrently only if the documentation for its type says that this is safe. We avoid concurrent access to most variables either by confining them to a single goroutine or by maintaining a higher-level invariant of mutual exclusion.\nIn contrast, exported package-level functions are generally expected to be concurrency-safe. Since package-level variables cannot be confined to a single goroutine, functions that modify them must enforce mutual exclusion.\n# 2. Data Race A data race occurs whenever two goroutines access the same variable concurrently and at least one of the accesses is a write. Notice that \u0026ldquo;access the same variable concurrently\u0026rdquo; means: there are two or more goroutines (threads) to access a same variable x, and we cannot assure the order of accessing variable x.\nAussume we have these code:\n1 2 3 4 5 var balance int func Deposit(amount int) { balance = balance + amount } func Balance() int { return balance } And there are two goroutines access balance concurrently,\n1 2 3 4 5 6 7 8 // Alice: go func() { bank.Deposit(200) // A1 fmt.Println(\u0026#34;=\u0026#34;, bank.Balance()) // A2 }() // Bob: go bank.Deposit(100) // B Alice deposits $200, then checks her balance, while Bob deposits $100. Since the steps A1 and A2 occur concurrently with B, we cannot predict the order in which they happen. Intuitively, it might seem that there are only three possible orderings, which we’ll call “Alice first,” “Bob first,” and “Alice/Bob/Alice.” The following table shows the value of the balance variable after each step. The quoted strings represent the printed balance slips.\nIn all cases the final balance is $300. The only variation is whether Alice’s balance slip includes Bob’s transaction or not, but the customers are satisfied either way.\nBut this intuition is wrong. There is a fourth possible outcome, in which Bob’s deposit occurs in the middle of Alice’s deposit, after the balance has been read (balance + amount) but before it has been updated (balance = ...), causing Bob’s transaction to disappear. This is because Alice’s deposit operation A1 is really a sequence of two operations, a read and a write; call them A1r and A1w. Here’s the problematic interleaving:\n1 2 3 4 5 6 7 Data race 0 A1r 0 ... = balance + amount # happend in the middle, balance = 100 but A1 will make balance = balance + amount which equals to 200 above B 100 A1w 200 balance = ... A2 \u0026#34;= 200\u0026#34; After A1r, the expression balance + amount evaluates to 200, so this is the value written during A1w, despite the intervening deposit. The final balance is only $200.\nWe’ll repeat the definition, since it is so important: A data race occurs whenever two goroutines access the same variable concurrently and at least one of the accesses is a write. It follows from this definition that there are three ways to avoid a data race:\nfirst way: not to write the variable try to use read only variable second way: avoid accessing the variable from multiple goroutines (confined to a single goroutine) access the variable need to write in only one goroutine which called monitor goroutine other goroutine want to modify this variable can notify the monitor goroutine of that variable Since other goroutines cannot access the variable directly, they must use a channel to send the confining goroutine a request to query or update the variable. This is what is meant by the Go mantra “Do not communicate by sharing memory; instead, share memory by communicating.” Here’s the bank example rewritten with the balance variable confined to a monitor goroutine called teller:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Package bank provides a concurrency-safe bank with one account. package bank var deposits = make(chan int) // send amount to deposit var balances = make(chan int) // receive balance func Deposit(amount int) { deposits \u0026lt;- amount } func Balance() int { return \u0026lt;-balances } func teller() { var balance int // balance is confined to teller goroutine for { select { case amount := \u0026lt;-deposits: balance += amount case balances \u0026lt;- balance: } } } func init() { go teller() // start the monitor goroutine } You can see, when there are some variables in a package, we better tell user if it is concurrency-safe.\nThe third way to avoid a data race is to allow many goroutines to access the variable, but only one at a time. This approach is known as mutual exclusion.\n# 3. Mutual Exclusion # 3.1. Mutex vs Semaphore A semaphore is a non-negative integer variable that is shared between various threads. Mutex is a specific kind of binary semaphore that is used to provide a locking mechanism.\n1 2 3 P(mutex); execute CS; V(mutex); Mutex Semaphore A mutex is an object. A semaphore is an integer. Mutex works upon the locking mechanism. Semaphore uses signaling mechanism Operations on mutex:LockUnlock Operation on semaphore:WaitSignal Mutex doesn’t have any subtypes. Semaphore is of two types:Counting SemaphoreBinary Semaphore A mutex can only be modified by the process that is requesting or releasing a resource. Semaphore work with two atomic operations (Wait, signal) which can modify it. If the mutex is locked then the process needs to wait in the process queue, and mutex can only be accessed once the lock is released. If the process needs a resource, and no resource is free. So, the process needs to perform a wait operation until the semaphore value is greater than zero. # 3.2. Mutual Exclusion: sync.Mutex In Section 8.6, we used a buffered channel as a counting semaphore to ensure that no more than 20 goroutines made simultaneous HTTP requests. With the same idea, we can use a channel of capacity 1 to ensure that at most one goroutine accesses a shared variable at a time. A semaphore that counts only to 1 is called a binary semaphore.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 var ( sema = make(chan struct{}, 1) // a binary semaphore guarding balance balance int ) func Deposit(amount int) { sema \u0026lt;- struct{}{} // acquire token balance = balance + amount \u0026lt;-sema // release token } func Balance() int { sema \u0026lt;- struct{}{} // acquire token b := balance \u0026lt;-sema // release token return b } This pattern of mutual exclusion is so useful that it is supported directly by the Mutex type from the sync package. Its Lock method acquires the token (called a lock) and its Unlock method releases it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import \u0026#34;sync\u0026#34; var ( mu sync.Mutex // guards balance balance int ) func Deposit(amount int) { mu.Lock() balance = balance + amount mu.Unlock() } func Balance() int { mu.Lock() b := balance mu.Unlock() return b } # 3.3. Read/Write Mutexes: sync.RWMutex Since the Balance function only needs to read the state of the variable, it would in fact be safe for multiple Balance calls to run concurrently, so long as no Deposit or Withdraw call is running. In this scenario we need a special kind of lock that allows read-only operations to proceed in parallel with each other, but write operations to have fully exclusive access. This lock is called a multiple readers, single writer lock, and in Go it’s provided by sync.RWMutex:\n1 2 3 4 5 6 7 8 var mu sync.RWMutex var balance int func Balance() int { mu.RLock() // readers lock defer mu.RUnlock() return balance } # 4. Memory Synchronization You may wonder why the Balance method needs mutual exclusion, either channel-based or mutex-based. After all, unlike Deposit, it consists only of a single operation, so there is no danger of another goroutine executing “in the middle” of it.\nThe reason is that synchronization is about more than just the order of execution of multiple goroutines; synchronization also affects memory.\nIn a modern computer there may be dozens of processors, each with its own local cache of the main memory. For efficiency, writes to memory are buffered within each processor and flushed out to main memory only when necessary. Synchronization primitives like channel communications and mutex operations cause the processor to flush out and commit all its accumulated writes so that the effects of goroutine execution up to that point are guaranteed to be visible to goroutines running on other processors.\nConsider the possible outputs of the following snippet of code:\n1 2 3 4 5 6 7 8 9 var x, y int go func() { x = 1 // A1 fmt.Print(\u0026#34;y:\u0026#34;, y, \u0026#34; \u0026#34;) // A2 }() go func() { y = 1 // B1 fmt.Print(\u0026#34;x:\u0026#34;, x, \u0026#34; \u0026#34;) // B2 }() Since these two goroutines are concurrent and access shared variables without mutual exclusion, there is a data race, so we should not be surprised that the program is not deterministic.\n1 2 3 4 y:0 x:1 x:0 y:1 x:1 y:1 y:1 x:1 The fourth line could be explained by the sequence A1,B1,A2,B2 or by B1,A1,A2,B2, for example. However, these two outcomes might come as a surprise:\n1 2 x:0 y:0 y:0 x:0 but depending on the compiler, CPU, and many other factors, they can happen too. What possible interleaving of the four statements could explain them?\nWithin a single goroutine, the effects of each statement are guaranteed to occur in the order of execution; goroutines are sequentially consistent. But in the absence of explicit synchronization using a channel or mutex, there is no guarantee that events are seen in the same order by all goroutines. Although goroutine A must observe the effect of the write x = 1 before it reads the value of y, it does not necessarily observe the write to y done by goroutine B, so A may print a stale value of y.\nIt is tempting to try to understand concurrency as if it corresponds to some interleaving of the statements of each goroutine, but as the example above shows, this is not how a modern compiler or CPU works. Because the assignment and the Print refer to different variables, a compiler may conclude that the order of the two statements cannot affect the result, and swap them. If the two goroutines execute on different CPUs, each with its own cache, writes by one goroutine are not visible to the other goroutine’s Print until the caches are synchronized with main memory.\nAll these concurrency problems can be avoided by the consistent use of simple, established patterns. Where possible, confine variables to a single goroutine; for all other variables, use mutual exclusion.\n# 5. Race Conditions vs Data Race A race condition is a flaw that occurs when the timing or ordering of events affects a program’s correctness. Generally speaking, some kind of external timing or ordering non-determinism is needed to produce a race condition; typical examples are context switches, OS signals, memory operations on a multiprocessor, and hardware interrupts.\nA data race occurs whenever two goroutines access the same variable concurrently and at least one of the accesses is a write.\nRace conditions are pernicious because they may remain latent in a program and appear infrequently, perhaps only under heavy load or when using certain compilers, platforms, or architectures. This makes them hard to reproduce and diagnose.\n1 2 3 4 5 6 transfer1 (amount, account_from, account_to) { if (account_from.balance \u0026lt; amount) return NOPE; account_to.balance += amount; account_from.balance -= amount; return YEP; } Of course this is not how banks really move money, but the example is useful anyway because we understand intuitively that account balances should be non-negative and that a transfer must not create or lose money. When called from multiple threads without external synchronization, this function admits both data races (multiple threads can concurrently try to update an account balance) and race conditions (in a parallel context it will create or lose money). We can try to fix it like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 transfer2 (amount, account_from, account_to) { atomic { bal = account_from.balance; } if (bal \u0026lt; amount) return NOPE; atomic { account_to.balance += amount; } atomic { account_from.balance -= amount; } return YEP; } Here “atomic” is implemented by the language runtime, perhaps simply by acquiring a thread mutex at the start of the atomic block and releasing it at the end, perhaps using some sort of transaction, or perhaps by disabling interrupts — for purposes of the example it doesn’t matter as long as the code inside the block executes atomically.\ntransfer2 has no data races when called by multiple threads, but obviously it is an extremely silly function containing race conditions that will cause it to create or lose money almost as badly as the unsynchronized function. From a technical point of view, the problem with transfer2 is that it permits other threads to see memory states where a key invariant — conservation of money — is broken.\nTo preserve the invariant, we have to use a better locking strategy. As long as atomic’s semanatics are to end the atomic section on any exit from the block, the solution can be blunt:\n1 2 3 4 5 6 7 8 transfer3 (amount, account_from, account_to) { atomic { if (account_from.balance \u0026lt; amount) return NOPE; account_to.balance += amount; account_from.balance -= amount; return YEP; } } This function is free of data races and also of race conditions.\nEverything in this post is pretty obvious, but I’ve observed real confusion about the distinction between data race and race condition by people who should know better (for example because they are doing research on concurrency correctness). Muddying the waters further, even when people are perfectly clear on the underlying concepts, they sometimes say “race condition” when the really mean “data race.” Certainly I’ve caught myself doing this.\n# 6. Conclusion concurrent envolves two or more threads, and it\u0026rsquo;s all about the unpredictable order of the execution.\nconcurrency-safe type if that all its accessible methods and operations are concurrency-safe.\nA data race occurs whenever two goroutines access the same variable concurrently and at least one of the accesses is a write.\ndata race can cause data lost, the bank example when there is data race, you\u0026rsquo;d better use Mutual Exclusion: sync.RWMutex, sync.Mutex Mutex vs Semaphore\nA race condition is a flaw that occurs when the timing or ordering of events affects a program’s correctness.\nReferences:\n9.1 Race Conditions | The Go Programming Language Mutex vs Semaphore - GeeksforGeeks Race Condition vs. Data Race – Embedded in Academia ","date":"2023-08-24T19:16:59Z","permalink":"https://blog.yorforger.cc/p/concurrency-with-shared-variables/","title":"Concurrency with Shared Variables"},{"content":" # 1. Two Styles of Concurrent Programming Go enables two styles of concurrent programming.\ncommunicating sequential processes or CSP goroutines and channels shared memory multithreading traditional model multiple threads # 2. Goroutines vs OS Threads Creation and Destruction Creating and destroying OS threads requires interacting with the operating system, involving system calls. Go routines managed by Go scheduler, in user space, no system calls. Context Switching needs a full context switch in os thread Level of Operation User Space Level vs Kernel Space Level Goroutines are multiplexed onto a smaller number of OS threads. Stack Size growable stack vs fixed-size stack # 2.1. Growable Stacks Each OS thread has a fixed-size block of memory (often as large as 2MB) for its stack. This fixed-size stack is simultaneously too much and too little. Yet despite their size, fixed-size stacks are not always big enough for the most complex and deeply recursive of functions. Changing the fixed size can improve space efficiency and allow more threads to be created, or it can enable more deeply recursive functions, but it cannot do both.\nIn contrast, a goroutine starts life with a small stack, typically 2KB. A goroutine’s stack, like the stack of an OS thread, holds the local variables of active and suspended function calls, but unlike an OS thread, a goroutine’s stack is not fixed; it grows and shrinks as needed. The size limit for a goroutine stack may be as much as 1GB.\n# 2.2. Goroutine Scheduling OS threads are scheduled by the OS kernel. Every few milliseconds, a hardware timer interrupts the processor, which causes a kernel function called the scheduler to be invoked. This function suspends the currently executing thread and saves its registers in memory, looks over the list of threads and decides which one should run next, restores that thread’s registers from memory, then resumes the execution of that thread. Because OS threads are scheduled by the kernel, passing control from one thread to another requires a full context switch, that is, saving the state of one user thread to memory, restoring the state of another, and updating the scheduler’s data structures. This operation is slow, due to its poor locality and the number of memory accesses required\nThe Go runtime contains its own scheduler that uses a technique known as m:n scheduling, because it multiplexes (or schedules) m goroutines on n OS threads. The job of the Go scheduler is analogous to that of the kernel scheduler, but it is concerned only with the goroutines of a single Go program.\nUnlike the operating system’s thread scheduler, the Go scheduler is not invoked periodically by a hardware timer, but implicitly by certain Go language constructs. For example, when a goroutine calls time.Sleep or blocks in a channel or mutex operation, the scheduler puts it to sleep and runs another goroutine until it is time to wake the first one up. Because it doesn’t need a switch to kernel context, rescheduling a goroutine is much cheaper than rescheduling a thread.\n# 3. Goroutine Model go-routines are user-space threads not kernel threads, kernel threads created and managed by OS (sleep, wait, running), OS doesn\u0026rsquo;t know user-space threads exist, Go shcduler multiplexes (or schedules) m goroutines on n OS threads, which is known as m:n scheduling.\nLearn how goroutine works:\n{% youtube YHRO5WQGh0k %}\n# 3.1. Physical Thread vs Kernel Thread A \u0026ldquo;hardware thread\u0026rdquo; is just a confusing name for a logical core, aka execution context. It has nothing to do with software threads or kernel threads. One physical core may support more than 1 logical core, e.g. via hyperthreading or other SMT.\nA hardware thread is a set of registers that are able to hold the context of a software thread. Having two \u0026ldquo;hardware threads\u0026rdquo; enables a single CPU to concurrently execute the instructions of two software threads without help from the OS. P.S.: If a software thread is like an automobile, then a hardware thread is like a lane on a highway in which the automobile can drive. OK, that\u0026rsquo;s kind of a weak anaology, but what I\u0026rsquo;m trying to say is that a \u0026ldquo;hardware thread\u0026rdquo; and a thread in your program don\u0026rsquo;t have much more in common with each other than the lane on the highway has in common with your car.\n\u0026ndash;multithreading - Kernel threads VS CPU threads - Stack Overflow\nLearn more Hyper-Threading \u0026amp; Physical Threads 无论语言层面何种并发模型, 到了操作系统一定是运行在 kernel thread 上的, 上面我们说到Go的做法是把多个 user-space threads 映射到一个 kernel thread, 以减少kernel thread切换时带来的消耗, 那其他语言怎么做的呢? 在C++里, 是通过syscall直接调用OS的kernel thread, 线程所有的行为如创建, 终止, 切换等操作都由内核来完成, 一个用户态的线程对应一个系统线程, 这时候C++在频繁创建删除thread的时候就要考虑上下文切换的开销了, 因为操作的直接是kernel thread, 比如来一个tcp连接就创建一个thread, 开销太大了, 所以这时候就出现了线程池, 说到底我们就是想要减少kernel thread创建切换的次数, 以减少开销, 你看无论C++还是Go都有自己的解决办法, 前者是通过thread pool来对kernel thread重复利用, 而后者因为通过map 多个goroutine到较少个kernel thread, 实现对kernel thread的重复利用, 减少上下文切换的次数, 减少开销,\n# 4. Frequently Asked Questions (FAQ) # 4.1. why goroutines instead of threads? Goroutines are part of making concurrency easy to use. The idea, which has been around for a while, is to multiplex independently executing functions—coroutines—onto a set of threads. When a coroutine blocks, such as by calling a blocking system call, the run-time automatically moves other coroutines on the same operating system thread to a different, runnable thread so they won\u0026rsquo;t be blocked. The programmer sees none of this, which is the point. The result, which we call goroutines, can be very cheap: they have little overhead beyond the memory for the stack, which is just a few kilobytes.\nTo make the stacks small, Go\u0026rsquo;s run-time uses resizable, bounded stacks. A newly minted goroutine is given a few kilobytes, which is almost always enough. When it isn\u0026rsquo;t, the run-time grows (and shrinks) the memory for storing the stack automatically, allowing many goroutines to live in a modest amount of memory. The CPU overhead averages about three cheap instructions per function call. It is practical to create hundreds of thousands of goroutines in the same address space. If goroutines were just threads, system resources would run out at a much smaller number.\n# 4.2. why is there no goroutine ID? Goroutines do not have names; they are just anonymous workers. They expose no unique identifier, name, or data structure to the programmer. Some people are surprised by this, expecting the go statement to return some item that can be used to access and control the goroutine later.\nThe fundamental reason goroutines are anonymous is so that the full Go language is available when programming concurrent code. By contrast, the usage patterns that develop when threads and goroutines are named can restrict what a library using them can do.\nHere is an illustration of the difficulties. Once one names a goroutine and constructs a model around it, it becomes special, and one is tempted to associate all computation with that goroutine, ignoring the possibility of using multiple, possibly shared goroutines for the processing. If the net/http package associated per-request state with a goroutine, clients would be unable to use more goroutines when serving a request.\nMoreover, experience with libraries such as those for graphics systems that require all processing to occur on the \u0026ldquo;main thread\u0026rdquo; has shown how awkward and limiting the approach can be when deployed in a concurrent language. The very existence of a special thread or goroutine forces the programmer to distort the program to avoid crashes and other problems caused by inadvertently operating on the wrong thread.\nFor those cases where a particular goroutine is truly special, the language provides features such as channels that can be used in flexible ways to interact with it.\n# 5. Conclusion OS thread has a fixed-size block of memory (as large as 2MB) for its stack. A goroutine starts life with a small stack, typically 2KB but goroutine’s stack is not fixed. Thread stack holds the local variables of active and suspended function calls. OS threads are scheduled by the OS kernel, Go runtime contains its own scheduler that uses a technique known as m:n scheduling, goroutine is much cheaper than rescheduling a thread. A great bolg: Share memory by communicating · The Ethically-Trained Programmer\n参考:\n9.8 Goroutines and Threads | The Go Programming Language Frequently Asked Questions (FAQ) - The Go Programming Language GopherCon 2018: Kavya Joshi - The Scheduler Saga multithreading - Kernel threads VS CPU threads - Stack Overflow 书籍《Go并发编程实战》 Effective Go - The Go Programming Language https://zhuanlan.zhihu.com/p/60613088 ","date":"2023-08-24T14:58:18Z","permalink":"https://blog.yorforger.cc/p/goroutines-model/","title":"Goroutines Model"},{"content":" # 1. Goroutines When a program starts, its only goroutine is the one that calls the main function, so we call it the main goroutine. The go statement itself completes immediately:\n1 2 f() // call f(); wait for it to return go f() // create a new goroutine that calls f(); don\u0026#39;t wait The code below will just print helllo, this because when main function returns, all goroutines are abruptly terminated and the program exits.\n1 2 3 4 5 6 7 func main() { go func() { time.Sleep(time.Millisecond * 100) fmt.Println(\u0026#34;hello form another goroutine\u0026#34;) }() fmt.Println(\u0026#34;hello\u0026#34;) } Other than (除了) by returning from main or exiting the program, there is no programmatic way for one goroutine to stop another, but there are ways to communicate with a goroutine to request that it stop itself.\n# 2. Channels A Go channel is a means of communication that enables data sharing between goroutines. Each channel has a type associated with it.\n1 2 data := \u0026lt;- ch // read from a channel ch ch \u0026lt;- data // write to a channel ch # 2.1. Unbuffered channel This creates an unbuffered channel of type int. An unbuffered channel is one that can only hold one value at a time.\n1 ch := make(chan int) Sends and receives on an unbuffered channel will block until the corresponding operation is ready to proceed. Run this will cause a dealock error,\n1 2 3 4 5 6 7 8 9 10 func main() { ch := make(chan int) // This send operation will block until there is a receive operation // ready to proceed. ch \u0026lt;- 42 // This receive operation will block until there is a send operation // ready to proceed. val := \u0026lt;-ch fmt.Println(val) } With the nature of unbuffered channel, we can do something like this:\n1 2 3 4 5 6 7 8 9 10 func main() { ch := make(chan bool) go func() { // Do some work ch \u0026lt;- true }() // Wait for the goroutine to finish on another goroutine, // main() function for example \u0026lt;-ch } # 2.2. Buffered channel You can also specify the buffer size of a channel when creating it. A buffered channel allows for multiple values to be stored in the channel at once before they shall be read.\n1 ch := make(chan int, 3) # 2.3. Buffered channel vs unbuffered channel ch := make(chan int, 1) is totally different from ch := make(chan int), the code below won\u0026rsquo;t get deadlock error:\n1 2 3 4 5 6 7 func main() { ch := make(chan int, 1) ch \u0026lt;- 42 // if you put another \u0026#34;ch \u0026lt;- 42\u0026#34; below, there will be a deadlock error val := \u0026lt;-ch fmt.Println(val) } # 2.4. Read \u0026amp; send on closed channel # 2.4.1 Read on a closed channel A closed channel returns the default value of that type:\n1 2 3 4 5 6 7 8 9 10 11 12 func main() { c := make(chan bool, 1) c \u0026lt;- true fmt.Println(\u0026lt;-c) close(c) fmt.Println(\u0026lt;-c) fmt.Println(\u0026lt;-c) } ------------------------------ true false false We can also check whether a channel is open or closed with the help of the given syntax:\n1 ele, ok:= \u0026lt;- channel_name If the value of ok is true, this indicates that the channel is open and read operations can be done.\n# 2.4.2 Send and close a closed channel Send or close a closed channel will cause panic.\n1 2 3 4 5 ch = make(chan bool) ch \u0026lt;- true close(ch) // this will get a panic ch \u0026lt;- true Don\u0026rsquo;t close a channel from the receiver side and don\u0026rsquo;t close a channel if the channel has multiple concurrent senders. -\u0026gt; Don\u0026rsquo;t close (or send values to) closed channels. Close channel elegently: https://qcrao91.gitbook.io/go/channel/ru-he-you-ya-di-guan-bi-channel\n# 2.5. Read \u0026amp; send on a nil channel An attempt to send/read value on a nil channel will block that goroutine forerver.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func main() { s := make(chan bool, 1) s\u0026lt;- true \u0026lt;-s go func() { s = nil s\u0026lt;- true fmt.Println(\u0026#34;hello from goroutine 2\u0026#34;) }() fmt.Println(\u0026#34;hello from goroutine 1\u0026#34;) time.Sleep(time.Second * 3) fmt.Println(\u0026#34;hi from goroutine 1 after seconds\u0026#34;) } --------------------------------------- hello from goroutine 1 hi from goroutine 1 after seconds If you don\u0026rsquo;t know whether a channel is closed or not and blindly write to it, then you have a badly designed program. Redesign it so that there is no way to write into it after it is closed. A comment from Stack Overflow\nHow to know if a channel is closed only by sending value to it? Answer: You can\u0026rsquo;t.\nlearn more: https://stackoverflow.com/a/61101887/16317008\n# 3. Should a channel has to be closed It\u0026rsquo;s OK to leave a Go channel open forever and never close it. When the channel is no longer used, it will be garbage collected.\nNote that it is only necessary to close a channel if the receiver is looking for a close. Closing the channel is a control signal on the channel indicating that no more data follows.\nDesign Question: Channel Closing\nSource: https://stackoverflow.com/a/8593986/16317008\n# 4. Use cases of channels # 4.1. Synchronization between goroutines Traditional threading models require the programmer to communicate between threads using shared memory. Typically, shared data structures are protected by locks, and threads will contend over those locks to access the data. Instead of explicitly using locks to mediate access to shared data, Go encourages the use of channels to pass references to data between goroutines.\nChannels can be used to ensure that one goroutine doesn\u0026rsquo;t proceed until another goroutine has completed its work. This is particularly useful in scenarios where data needs to be shared between two or more goroutines, and it\u0026rsquo;s important to ensure that the data isn\u0026rsquo;t modified by multiple goroutines at the same time.\nNOTE: Channels can be used to ensure one goroutine doesn\u0026rsquo;t proceed until another goroutine has completed its work. Doesn\u0026rsquo;t mean channels ensure that itself. You need to implement this by utilizing channel. This is the channel use case for notification.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // main goroutine won\u0026#39;t exit until the worker goroutine has completed its work. // this is channel\u0026#39;s use case for notification // you can use this to ensue that data isn\u0026#39;t modified by multiple goroutines at a same time. func main() { done := make(chan struct{}) go func() { fmt.Println(\u0026#34;working...\u0026#34;) time.Sleep(time.Second) fmt.Println(\u0026#34;done\u0026#34;) done\u0026lt;- struct{}{} }() \u0026lt;-done } Actually, in essence, synchronization between goroutines implemented by channel is just a channel\u0026rsquo;s use case for notification.\nThe code above only can limit one groutine access the data at a time, if you want to prevent concurrent modifications to a variable while retaining the ability to read, which means enables more than one groutine to access the data, you\u0026rsquo;d typically embed a sync.RWMutex. This is no exemption.\nWhen you pass a data let\u0026rsquo;s say cat to an unbuffered channel in a goroutine g1, this operation let\u0026rsquo;s say ch \u0026lt;- cat will block until another goroutine let\u0026rsquo;s sayg2 takes cat out from the channel. After the g2 got data, this is done, ch \u0026lt;- cat won\u0026rsquo;t block. Then g2 can do anything to your cat and g1 can also access cat, which means you have to consider data race even you send your data with channel. For example. when you pass a cat to a channel, what if cat has a slice or a pointer in its field? Whenver use channel pass value, you should remember that everything passed by value in go, and the direct value not the underlying value. You pass a pointer to a channel, there is a copy for an address, not the underlying value that pinter point to.\nBut there is an exception, you pass a simple value to channel not struct value, not slice, not a map or pointer value, just a int, string value because these simple value has no underlying values, and everything passed by value, when a int passed to a channel, there is a copy for that int value, therefore you don\u0026rsquo;t need to consider data race.\nFor large objects like arrays or large structs, passing a pointer is usually the logical thing to do to avoid expensive copies. But you should consider and promise that when one gouroutine write the data, there is no other goroutine access that data. You can use sync.RWMutex, you can add another for notification after the writing operation is done by using the blocking nature of a channel.\nLearn more: go - does passing pointer through channel break the csp design?\n# 4.2. Use channels for notifications Notifications can be viewed as special requests/responses in which the responded values are not important. Generally, we use the blank struct type struct{} as the element types of the notification channels, for the size of type struct{} is zero, hence values of struct{} doesn\u0026rsquo;t consume memory.\n# 4.3. Use Channels as counting semaphores This is the buffered channel\u0026rsquo;s use case, this blog gives an excellent example: Share memory by communicating · The Ethically-Trained Programmer\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 // Share memory by communicating // https://blog.carlmjohnson.net/post/share-memory-by-communicating/ package semaphores type Semaphore struct { acquire chan bool release chan struct{} stop chan chan struct{} } func New(n int) *Semaphore { s := Semaphore{ acquire: make(chan bool), release: make(chan struct{}), stop: make(chan chan struct{}), } go s.start(n) return \u0026amp;s } func (s *Semaphore) start(max int) { count := 0 for { var acquire = s.acquire // nil always blocks sends and read operation if count \u0026gt;= max { acquire = nil } select { case acquire \u0026lt;- true: count++ case s.release \u0026lt;- struct{}{}: count-- case wait := \u0026lt;-s.stop: close(s.acquire) // Drain remaining calls to Release for count \u0026gt; 0 { s.release \u0026lt;- struct{}{} count-- } close(wait) return } } } // Acquire a closed channel returns its default value as many times as it is called. // if s.acquire is closed, the Acquire() get called in other goroutine will return false immediately // if s.acquire is not closed, and no data written into it, Acquire() will block func (s *Semaphore) Acquire() bool { return \u0026lt;-s.acquire } func (s *Semaphore) Release() { \u0026lt;-s.release } func (s *Semaphore) Stop() { blocker := make(chan struct{}) s.stop \u0026lt;- blocker \u0026lt;-blocker } References:\nShare Memory By Communicating - The Go Programming Language Understanding Go Channels: An Overview for Beginners Closing the Channel in Golang - Scaler Topics Share memory by communicating · The Ethically-Trained Programmer Learn more: Channel Use Cases -Go 101\n","date":"2023-08-24T10:06:10Z","permalink":"https://blog.yorforger.cc/p/goroutine-channel-go-basics/","title":"Goroutine \u0026 Channel - Go Basics"},{"content":" # 问题描述 写完博客把内容 push 到了 GitHub, 发现有个文件 a.txt 不应该被追踪, a.txt 已经被追踪了,\n# 原因分析 首先你可能会想到 直接在本地删除 a.txt commit, 然后 push 就行了, 可是即使这样 GitHub依然会有 a.txt , 况且 a.txt 不能删除, 就是留在本地有用但是因为隐私不想放到GitHub, 而且这么做岂不是太粗鲁了,\n# 解决办法 解决方法不唯一, 这里介绍两个,\n其中一个可行的是在 GitHub 手动删除 a.txt , 然后在本地进行 fetch, 再merge, ummm, 貌似比前面还粗鲁,\n比较优雅的做法是在本地仓库使用 git rm --cached \u0026lt;filename\u0026gt; 让 a.txt 从 tracked -\u0026gt; untracked,\n然后把 a.txt 加入到 .gitignore, 再进行 commit 和 push, 此时 github 上的 a.txt 会消失\n","date":"2023-08-20T07:40:57Z","permalink":"https://blog.yorforger.cc/p/pushed-a-wrong-file-to-github-solved/","title":"Pushed a Wrong File to Github - Solved"},{"content":" # 1. Introduction Source Code: gptbot\n# 2. Database - gorm Don\u0026rsquo;t Open db connection for each request, initialize a global DB and use it for all requests.\nClose is one of the most misused methods, most application don\u0026rsquo;t need it, be sure you really need it.\nit is not necessary to close when crash, and you can still Close DB connection in V2.\njinzhu - creator of gorm\n1 2 3 4 5 6 func main() { db, _ := gorm.Open(sqlite.Open(\u0026#34;gpt_bot.db\u0026#34;), \u0026amp;gorm.Config{}) store, _ := redistore.NewRediStore(10, \u0026#34;tcp\u0026#34;, \u0026#34;:6379\u0026#34;, \u0026#34;\u0026#34;, []byte(os.Getenv(\u0026#34;SESSION_KEY\u0026#34;))) ... http.Handle(\u0026#34;/login\u0026#34;, LoginHandler(db, store)) } As you can see, we use a global database connection here accross all handler, and it\u0026rsquo;s worth noticing that every HTTP request that comes in to the server gets its own goroutine. Therefore, you have to consider data race in the handers.\nBecause we share the db across all handlers, we have to consider that if db is thread-safe. I find some answers on stack overflow which explains well:\nDoes gorm.Open() create a new connection pool every time it\u0026rsquo;s called?\nyes, try to reuse the returned DB object.\ngorm.Open does the following: (more or less):\nlookup the driver for the given dialect call sql.Open to return a DB object call DB.Ping() to force it to talk to the database This means that the recommendations for sql.Open apply to gorm.Open:\nThe returned DB is safe for concurrent use by multiple goroutines and maintains its own pool of idle connections. Thus, the Open function should be called just once. It is rarely necessary to close a DB.\nAlso note that the connection pool can be configured as such, in both GORM v1 and v2:\n1 2 3 4 5 6 7 8 // SetMaxIdleConns sets the maximum number of connections in the idle connection pool. db.DB().SetMaxIdleConns(10) // SetMaxOpenConns sets the maximum number of open connections to the database. db.DB().SetMaxOpenConns(100) // SetConnMaxLifetime sets the maximum amount of time a connection may be reused. db.DB().SetConnMaxLifetime(time.Hour) // d, err := db.DB() // d.SetMaxIdleConns(10) # 3. Session - gorilla/session # 3.1. encode before save When try to save data in session, we usually use the Values field of session, for example,\n1 session.Values[\u0026#34;username\u0026#34;] = \u0026#34;Jack\u0026#34; The first problem I came across is that, you can save int, bool, string and other basic value into session.Values[] directly, but when you try to assign other type value, custom object, array to session.Values[\u0026quot;xxx\u0026quot;] for example, you have to encode the value before assignment, so that gorilla/session can save data to Redis and read correctly for later request . Otherwise, you cannot get the value corrctly, I mean, after you restart your server, you cannot get the session stored in the Redis, the IsNew will be always true, this is because the encoing issues,\nI want to save a slice into session, which is chat history, so I have to encode it before assign it to session.Values[\u0026quot;messages\u0026quot;], I\u0026rsquo;ll choose encoding/json here, you can choose other encoding tech, encoding/gob for example,\n1 2 3 // chatHistory == []openai.ChatCompletionMessage data, _ := json.Marshal(chatHistory) session.Values[\u0026#34;messages\u0026#34;] = data Bacause json.Marshal(chatHistory) encodes the object I passed in and return as a []byte, so the type of the variable data above is []byte, so I set session.Values[\u0026quot;messages\u0026quot;] as a emtpy byte slice at begining, when login,\n1 2 3 4 5 6 func initSession(session *sessions.Session) error { session.Options.MaxAge = 20 session.Values[\u0026#34;authenticated\u0026#34;] = true session.Values[\u0026#34;messages\u0026#34;] = []byte{} return nil } When you want get message that stored in session, you just need to get the encoded value which is []byte, and pass it to json.Unmarshal(), this function will convert the encoded value back to golang object,\ngo - gorilla/sessions persistent between server restarts? - Stack Overflow\n# 3.2. reset MaxAge whenever call session.Save I don\u0026rsquo;t know if I did it wrong, or it\u0026rsquo;s a bug in gorilla/sessions, for example, I set the MaxAge of the session to 60 seconds when user first logged in I create session, and for later request, for example, user will send request for talking with chatGPT, and during this process, the server will update the history message which stored in session.Values[\u0026quot;messages\u0026quot;], after update the value of session.Values[\u0026quot;messages\u0026quot;], we should call session.Save() so that this update can be saved for later request,\nBut this will make session.Options.MaxAge back to its defalult value, which is 30 days, so you should set it again when you call session.Save(),\n1 2 3 4 5 ... session.Values[\u0026#34;messages\u0026#34;] = data // set MaxAge whenever you call session.Save(r, w) session.Options.MaxAge = 24 * 3600 _ = session.Save(r, w) Otherwise, the session stored in Redis won\u0026rsquo;t be deleted until 30 days later, and your user will keep logged in satus for 30 days,\ngo - Sessions variables in golang not saved while using gorilla sessions - Stack Overflow\nReturn and wrap error in low layer function (try to provide more context info), only handle errors and log info on the top of the function call stack.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func parseUsernamePassword(r *http.Request) (*map[string]string, error) { userInfo := make(map[string]string) if err := r.ParseForm(); err != nil { // wrap error which can provide more context info // usually, say: \u0026#34;failed to\u0026#34; + \u0026#34;function name\u0026#34; + \u0026#34;error\u0026#34; // but not always return nil, fmt.Errorf(\u0026#34;falied to parse username and password: %v\u0026#34;, err) } ... } func updateMessage(w http.ResponseWriter, r *http.Request, store *redistore.RediStore) error { session, err := store.Get(r, \u0026#34;session_id\u0026#34;) if err != nil { return fmt.Errorf(\u0026#34;cannot update message: %v\u0026#34;, err) } ... if err = session.Save(r, w); err != nil { return fmt.Errorf(\u0026#34;cannot update message: %v\u0026#34;, err) } ... } Learn more: Error handling - Go - David\u0026rsquo;s Blog\n# 4. Session or redis cache I did some experiments , found that a query with gorm for sqlite3 takes about 380us, it\u0026rsquo;s not that much, and there will no that many users can overwhelm my server, so I finally decide query the tokens value from database everytime when user make a request to chat with ChatGPT,\n1 2 3 4 5 6 7 8 9 10 11 12 13 // https://stackoverflow.com/a/45791377/16317008 start := time.Now() err = db.Limit(1).Find(\u0026amp;user, \u0026#34;username = ?\u0026#34;, (*userInfo)[\u0026#34;username\u0026#34;]).Error t := time.Now() fmt.Println(t.Sub(start)) -------------------------------- 1.514166ms // first time 542.125µs 387.625µs 484.958µs 477.417µs 377.959µs .. # 5. Session store sessions need a place to store, in-memory, file or Redis, package gorilla/sessions provides two way to save sessions, one is file, another is in-memory,\n1 2 3 var store = sessions.NewCookieStore([]byte(os.Getenv(\u0026#34;SESSION_KEY\u0026#34;))) // or var store = NewFilesytemStore(\u0026#34;path/to/file\u0026#34;, []byte(os.Getenv(\u0026#34;SESSION_KEY\u0026#34;))) I choose redis to save sessions with redistore library, which implements a redis store based on gorilla/sessions,\n1 var store, _ = redistore.NewRediStore(10, \u0026#34;tcp\u0026#34;, \u0026#34;:6379\u0026#34;, \u0026#34;\u0026#34;, []byte(os.Getenv(\u0026#34;SESSION_KEY\u0026#34;))) You can use gorilla/sessions which help you save session in-memory, if you want save sessions on a redis server too, you have to install Redis on your computer, learn more about session and Redis:\nIntroduce Redis | 橘猫小八的鱼 Cookie \u0026amp; Session | 橘猫小八的鱼 Go 每日一库之 gorilla/sessions - 大俊的博客 GoAuthentication/readme.md at master · karankumarshreds/GoAuthentication ","date":"2023-08-18T09:20:57Z","permalink":"https://blog.yorforger.cc/p/golang-web-a-simple-gptbot-with-openai-api/","title":"Golang Web - A Simple gptbot with Openai API"},{"content":"众所周知，HTTP 是一个无状态协议，所以客户端每次发出请求时，下一次请求无法得知上一次请求所包含的状态数据，如何能把一个用户的状态数据关联起来呢？\n比如在淘宝的某个页面中，你进行了登陆操作。当你跳转到商品页时，服务端如何知道你是已经登陆的状态？\n# 1. cookie 首先产生了 cookie 这门技术来解决这个问题, cookie 是 http 协议的一部分, 它的处理分为如下几步:\n服务器向客户端发送 cookie 通常使用 HTTP 协议规定的 set-cookie 头操作 规范规定 cookie 的格式为 name=value 格式, 且必须包含这部分 浏览器将 cookie 保存 每次请求浏览器都会将 cookie 发向服务器 浏览器可以自动发送 cookie, 即不用在前端写 js 代码手动发送, 服务器返回 cookie 的时需指定 Expires/MaxAge 参数, 客户端浏览器收到 response 后(也包括 cookie), 若 cookie 的 MaxAge 时间已过, 则浏览器会自动设置此 cookie 为 invalid, 之后发送请求并不会带上该 cookie, 另外 Expires/MaxAge只用指定其一, 但是未来兼容一些老浏览器如 IE, 有时候会同时设置 MaxAge 和 Expires\n了解更多: https://stackoverflow.com/a/35729939/16317008\n其他可选的 cookie 参数会影响将 cookie 发送给服务器端的过程，主要有以下几种：\npath：表示 cookie 影响到的路径，匹配该路径才发送这个 cookie。 expires 和 maxAge：告诉浏览器这个 cookie 什么时候过期，expires 是 UTC 格式时间，maxAge 是 cookie 多久后过期的相对时间。当不设置这两个选项时，会产生 session cookie，session cookie 是 transient 的，当用户关闭浏览器时，就被清除。一般用来保存 session 的 session_id。 secure：当 secure 值为 true 时，cookie 在 HTTP 中是无效，在 HTTPS 中才有效。 httpOnly：浏览器不允许脚本操作 document.cookie 去更改 cookie。一般情况下都应该设置这个为 true，这样可以避免被 xss 攻击拿到 cookie。 上面说到的 expires 格式是 UTC, 意味着服务器把 cookie 传给客户端时, 若设置了 expires, 则需要先处理一下时间即把格式转为 UTC 格式, 规定 expires 格式必须时 UTC 的原因是客户端与服务器可能不在同一个时区, 比如服务器所在位置时区为 UTC-3, 客户端的时区为 UTC, 此时服务器比 UTC 慢了三小时, 此时若服务器直接设置 cookie expires 为 Now() + 30, 指的是半小时后过期, 可是传到客户端立刻过期了, 因为客户端浏览器默认来自服务器的 cookie expires 是 UTC,\n下面就是服务器传回 cookie expires 没有为其指定时区的例子, 可以看到请求是 Jun 20 发出的, cookie 过期时间却是 Jun 19:\n服务器端 Python Flask 处理 http request时, 设置 cookie 方式如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 @app.route(\u0026#34;/\u0026#34;) def index(): session = request.cookies.get(\u0026#39;session_id\u0026#39;) # 为客户端设置 cookie, 以便之后来自它的请求不用验证身份 login if session_id is None: resp = make_response(render_template(\u0026#39;index.html\u0026#39;)) resp.set_cookie(\u0026#39;session\u0026#39;, session, # 这里使用相对时间, 不用设置时区 max_age=datetime.timedelta(minutes=2), httponly=True) # 其它逻辑 ... return resp # 其它逻辑 ... All HTTP date/time stamps MUST be represented in Greenwich Mean Time (GMT), without exception. For the purposes of HTTP, GMT is exactly equal to UTC (Coordinated Universal Time). source\nBoth GMT and UTC display the same time.\n# 2. session Session和Cookie的目的相同, 都是为了弥补HTTP协议无状态的缺陷, Session和Cookie都是用来保存客户端状态信息的手段, 不同之处在于Cookie是存储在客户端浏览器方便客户端请求时使用, Session是存储在服务端用于存储客户端连接状态信息的, 从存储的数据类型来看, Cookie仅支持存储字符串, Session可支持多种数据类型 source\nhttp 是无状态的, 即下一次请求无法得知上一次请求所包含的状态数据, 所以产生了 cookie, 使服务器可以“记住”客户端, 大致逻辑为:\n服务器处理响应客户端时顺带返回一个 cookie object 客户端(浏览器) 发送 http request 时, 顺带发送一个 cookie object (若此时浏览器有相关cookie, 且没过期) cookie 在服务器端产生, 之后在客户端和服务器之间往返发送, 因此 cookie 不能保存比较重要的隐私数据, 可是很多时候服务器需要临时记住客户端的信息, 此时就产生了 session 的概念, 即在服务器内存维护一个数据结构, 用来临时保存每个客户端的数据, 然后为每份数据设置一个 session_id, 处理 http request 时, 先查看其 cookie 里有没有 session_id 字段:\n若存在就根据此 id 确定该请求的身份, 这样服务器就知道你是上次访问过的某某某, 然后从服务器的存储中取出上次记录在你身上的数据,\n否则新建一个 session, 然后把该 session 的 session_id 放到 cookie 里, (在cookie里设置 seesion_id 字段), 这样, 服务器既能把用户的重要信息安全保存, 又能确认每个请求的身份, 即把重要信息存在服务器不外露, 通过 session_id 来匹配 session 和 http request,\n所以 cookie 就是用来传递 session_id 的东西, 而 session_id 则用来唯一标识 session, 即会话, session 就是个数据结构, 用来临时存储一个会话的信息, 如客户端A, B, 与 服务器 S 之间,则有两个 session, A - S, B - S\n# 3. session 存储 Session在服务端是如何存储的呢？\n服务端可采用哈希表来保存Session内容，一般而言可在内存中创建相应的数据结构，不过一旦断电内存中所有的会话数据将会丢失。因此可将会话数据写入到文件或保存到数据库，虽然会增加I/O开销，但可以实现某种程序的持久化，也更有利于共享。\nsession 可以存放在:\n内存 cookie 本身 redis 或 memcached 等缓存中 数据库中 缓存的方案比较常见, 存数据库的话, 查询效率相比前三者都太低, 不推荐\n参考:\nhttps://gist.github.com/HellMagic/6e49af318d45311ee2860ac7d7bf09f6 Go Session - Go语言中文网 - Golang中文社区 了解更多:\nGo 每日一库之 gorilla/securecookie - 大俊的博客 Go 每日一库之 gorilla/sessions - 大俊的博客 ","date":"2023-08-17T07:39:56Z","permalink":"https://blog.yorforger.cc/p/cookie-session/","title":"Cookie \u0026 Session"},{"content":" # Problem GitHub 新建一个 repository, 并选择了自动创建 README.md, 在本地初始化项目后进行push, 报错:\n1 2 3 4 5 6 7 8 9 $ git push -u origin master To github.com:shwezhu/gptbot.git ! [rejected] master -\u0026gt; master (fetch first) error: failed to push some refs to \u0026#39;github.com:shwezhu/gptbot.git\u0026#39; hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: (e.g., \u0026#39;git pull ...\u0026#39;) before pushing again. hint: See the \u0026#39;Note about fast-forwards\u0026#39; in \u0026#39;git push --help\u0026#39; for details. # 原因分析 远程仓库存在本地仓库没有的 commit, 导致 push 被拒绝,\n# 解决办法 fetch 下来查看都是什么 commit, 再决定是否进行 merge,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # git fetch \u0026lt;remote\u0026gt; \u0026lt;branch\u0026gt;, \u0026lt;branch\u0026gt; is optional git fetch origin # list all branches git branch -v -a * master a48d922 Rework handler remotes/origin/master f6c60de Initial commit # check the work on the remote git checkout origin/master Note: switching to \u0026#39;origin/master\u0026#39; # 切换远程分支后, workplace 只剩下一个 `README.md` 文件, # 发现没什么问题, 切换回本地原分支, (假装查看变化) git switch master # 注意 如果查看变化后你不想合并这个远程分支 直接删除该分支即可 # git branch -r -d origin/master, 必须加上 -r 否则报错找不到分支 git merge origin/master fatal: refusing to merge unrelated histories git merge origin/master --allow-unrelated-histories Merge made by the \u0026#39;ort\u0026#39; strategy. README.md | 1 + 1 file changed, 1 insertion(+) create mode 100644 README.md # 查看 commit history git log commit 14daf2d5ae37dea131416629c6df22e1fe8ab456 (HEAD -\u0026gt; master) Merge: a48d922 f6c60de Author: David Zhu \u0026lt;shaowenzhu@dal.ca\u0026gt; Date: Tue Aug 15 23:03:47 2023 -0300 Merge remote-tracking branch \u0026#39;origin/master\u0026#39; commit f6c60de218933dcb40e507a88998318b4644034c (origin/master) Author: David Zhu \u0026lt;shwezhu@qq.com\u0026gt; Date: Wed Aug 16 06:29:10 2023 +0800 Initial commit commit a48d922d19e96b76f71dd09e0048f16d8feea03d Author: David Zhu \u0026lt;shaowenzhu@dal.ca\u0026gt; Date: Tue Aug 15 19:28:27 2023 -0300 Rework handler # 此时再 push git push -u origin master # 总结 通过 git fetch origin 拉取远程仓库的commit\n通过 git branch -v -a 查看所有分支, 选择是否进行merge\n执行 merge 时 git 会自动做一次 commit\ngit merge origin/master --allow-unrelated-histories\n","date":"2023-08-15T22:18:50Z","permalink":"https://blog.yorforger.cc/p/push-on-github-get-rejected-solved/","title":"Push on Github get Rejected - Solved"},{"content":"{% youtube iJ2muJniikY %}\n视频内容位置:\nCross Site Request Forgery: 1:16:00 CSRF Spring CSRF Filter 源码分析: 1:22:00 Trace level log 分析: 1:26:00 1 2 3 logging: level: org.springframework.security: TRACE # 1:30:35 理论: AuthenticationManager, ProviderManager, AuthenticationProvider Authentication Object stands for either the request to login or the result of a successful login request ,\n上图中, 如果账号密码正确, 则 ProviderManager 返回的 UsernamePasswordAuthenticationToken 就是同一个对象, 只不过内容从 password, username 变成了一个 richer object (内容更丰富), 如 UserDetails 对象可能包含 你喜欢的颜色, 生日, 等信息, 而 Authority 对象可能包含你的权限, 如 user 还是 admin,\nProviderManager is the most commonly used implementation of AuthenticationManager.\nAuthenticationProvider is used by ProviderManager to perform a specific type of authentication.\n1 2 3 4 5 6 7 8 9 10 11 12 public interface AuthenticationManager { Authentication authenticate(Authentication authentication) throws AuthenticationException; } // 我们经常自己实现这个接口, 实现自定义 AuthenticationProvider public interface AuthenticationProvider { Authentication authenticate(Authentication authentication) throws AuthenticationException; boolean supports(Class\u0026lt;?\u0026gt; authentication); } Basically AuthenticationProvider is the specialized version of the ProviderManager that says I only deal with username password authentication tokens, I only deal with robot authentications, I only deal with over 2 login authentication tokens.\nAnd this allows you to extend Spring Security in a very specific place for changing the way someone logs in, without having to write a custom filter that takes the request, that does some transformation, maybe you don\u0026rsquo;t need to go that far, maybe you have special rules around, you know the email of the user that comes in, well here an AuthenticationProvider would be good enough.\nA ProviderManager just like this, kind of the same idea of the filters, but applied to transforming authentication requests into authenciated objects, it\u0026rsquo;s a for loop over a list of AuthenticationProvider.\n# 1:35:50 实践: AuthenticationProviders, ProviderManager, AuthenticationProvider Our website is great, it has two beautiful pages I can log in with username and password, or with Google login, and have a robot a robot account, I can hit my endpoints and get stuff with credentials that are not in the database for example,\nThe problem with my website is that I have a an admin called Daniel, he\u0026rsquo;s really fine, he\u0026rsquo;s neat, he\u0026rsquo;s kind he\u0026rsquo;s very handsome, uh but he has really bad memory, and and for for the for the love of anything he cannot remember his password, that\u0026rsquo;s very unfortunate, so we\u0026rsquo;re going to make an escape hatch for Daniel so that he doesn\u0026rsquo;t have to remember his password, and so we\u0026rsquo;re going to do this by implementing a special AuthenticationProvider,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public class DavidAuthenticationProvider implements AuthenticationProvider { @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException { var username = authentication.getName(); if (\u0026#34;david\u0026#34;.equals(username)) { return UsernamePasswordAuthenticationToken.authenticated( \u0026#34;david\u0026#34;, null, AuthorityUtils.createAuthorityList(\u0026#34;ROLE_admin\u0026#34;) ); } // What do we do in case it\u0026#39;s not Daniel // Well we don\u0026#39;t know what to do with this, like the user password this must be handled //by some other AuthenticationProvider, so to Signal this, we\u0026#39;re returning null return null; } @Override public boolean supports(Class\u0026lt;?\u0026gt; authentication) { return UsernamePasswordAuthenticationToken.class.isAssignableFrom(authentication); } } UsernamePasswordAuthenticationToken: 实现了 Authentication 接口, 就像我们上节自定义的 RobotAuthentication, 都是用来代表一个自定义的认证用户 (authentication request), 只不过我们的 RobotAuthentication 用来代表通过 http header 认证的用户 (authentication request), 而前者则代表: An Authentication implementation that is designed for simple presentation of a username and password.\nisAssignableFrom(): Determines if the class or interface represented by this Class object is either the same as, or is a superclass or superinterface of, the class or interface represented by the specified Class parameter.\n别忘了 Class\u0026lt;T\u0026gt; 是个包装器, 参考: https://davidzhu.xyz/2023/08/05/Java/Basics/002-reflection-object-class 因此 supports() 方法代表的意思是: If the authentication argument is an instance of UsernamePasswordAuthenticationToken or any of its subclasses, the method returns true, indicating that this AuthenticationProvider implementation supports the provided authentication object. Otherwise, it returns false. The authenticate() method is responsible for performing the authentication process based on the provided authentication object and returning a fully authenticated Authentication object if the authentication is successful.\nwhen the ProviderManager calls this authenticate() method, it\u0026rsquo;s going to always pass in a UsernamePasswordAuthenticationToken or a subclass. This is the same way as we did with the the filter:\nDo I want to handle this request, is it the right type? DavidAuthenticationProvider 的 supports(Class\u0026lt;?\u0026gt; authentication) 用来判断参数 authentication 是不是属于 UsernamePasswordAuthenticationToken子类或就是该类型, 若是才可进行处理 RobotFilter 的 doFilterInternal() 中我们判断 header 中是否有 \u0026ldquo;x-robot-password\u0026rdquo; 字段, 若无则直接调用 doFilter() 并 return, And is the is the content of the payload the thing I care about? RobotFilter 判断密码是否正确 DavidAuthenticationProvider 判断用户名是否为 david The other thing that the AuthenticationProvider can do is throw in an AuthenticationException, so basically this account has expired and so you\u0026rsquo;re not allowed to log in, or the credentials are wrong,\nAuthenticationProvider can return three things, either an authentication object so the result of something that passed me credentials and they said the credentials are good, and an exception if I want to say stop processing this is invalid this should not go forever, or null if we delegate to the rest.\nIf nothing can authenticate, if every provider returns null, it will throw an exception in the end , and the ProviderManager will say no I don\u0026rsquo;t care about this,\nWe mount it into the filter chain, go to the WebSecurityConfig:\n1 2 3 4 5 6 7 ... .formLogin(withDefaults()) .oauth2Login(withDefaults()) .addFilterBefore(new RobotFilter(), UsernamePasswordAuthenticationFilter.class) // 在这 .authenticationProvider(new DavidAuthenticationProvider()) .build(); # 1:41:20 总结解释 为什么 我们应该自定义AuthenticationProvider 而不是 一个 FIlter Usually when you want to do a custom authentication, you should do a custom AuthenticationProvider rather than a full filter.\nWhen I log in ProviderManager produces an event, when I fail to log in and it also produces an event a spring event, so this means we can listen to those events and do stuff when an event is done.\n# 1:45:00 Change our Custom RobotFilter to Custom AuthenticationProvider 源码: https://github.com/shwezhu/springboot-learning/tree/master/3-spring-security-authenciation-provider\n注意结构, 比起下面这个:\n1 2 3 4 5 6 7 8 9 10 11 12 @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException { var authRequest = (RobotAuthentication) authentication; var password = authRequest.getPassword(); if (this.passwords.contains(password)) { // 可以发现, 实现了Authentication接口的类一般有个 authenticated() 静态方法, 也可称为factory method // 比如我们的自定义RobotAuthentication,再比如UsernamePasswordAuthenticationToken return RobotAuthentication.authenticated(); } else { throw new BadCredentialsException(\u0026#34;You are not Mr Robot 🤖️\u0026#34;); } } 用这样的结构更好:\n1 2 3 4 5 6 7 8 9 @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException { var authRequest = (RobotAuthentication) authentication; var password = authRequest.getPassword(); if (!this.passwords.contains(password)) { throw new BadCredentialsException(\u0026#34;You are not Mr Robot 🤖️\u0026#34;); } return RobotAuthentication.authenticated(); } # 1:56:00 AuthenticationProvider 调用顺序及优点 1 2 3 logging: level: org.springframework.security: TRACE 从图中可以看出 使用如下登录, 则会匹配到我们的 DavidAuthenticationProvider,\n1 $ curl -u \u0026#34;david:asd\u0026#34; http://localhost:8080/private -v 若使用正常用户密码登录, 则会跳过 DavidAuthenticationProvider, 使用 DaoAuthenticationProvider 验证,\n1 curl -u \u0026#34;user-1:asd\u0026#34; http://localhost:8080/private -v 看完上图会发现每次 RobotFilter 都先于 UsernamePasswordAuthenticationFilter 调用, 这是因为我们的 SecurityChain 配置代码:\n1 2 ... .addFilterBefore(new RobotFilter(authManager), UsernamePasswordAuthenticationFilter.class) 从上面的输出我们会发现:\n1 2 Invoking BasicAuthenticationFilter Found username \u0026#39;david\u0026#39; in Basic Authorization header 这是因为我们通过代码\n1 2 3 ... .formLogin(withDefaults()) .httpBasic(withDefaults()) 把 BasicAuthenticationFilter 加入到了 Security, 然后验证的时候正如输出那样, 一层一层的调用Filter, 进行匹配, 我们此时使用的是命令行 curl -u \u0026quot;\u0026quot; 进行验证的, 而 BasicAuthenticationFilter 就是做这个的, 如果我们把 BasicAuthenticationFilter 从我们的SecurityChian中移除也就是删除代码 .httpBasic(withDefaults()), 那我们通过 curl -u 提供的账号密码就不会被发现, 然后导致验证失败, 原因是所有的 filter 都被依次掉用完, 但依然没有发现匹配的,\n通过这个我们也会发现, 如果使用自定义 filter 进行验证, 如使用 DavidFIlter, 而不是DavidAuthenticationProvider, 那我们还需要去验证 post 里面的账号密码, 具体分析参考视频 1:58:15\n# 1:59:25 FilterChain 调用顺序 # 2:00:00 Recap # 2:01:35 Configurers - \u0026ldquo;navigate Spring Security\u0026rdquo; Configurers are an abstraction that allows you to configure the filter chain and do multiple operations on the the HTTP builder(),\nRobotLoginConfigurer.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 public class RobotLoginConfigurer extends AbstractHttpConfigurer\u0026lt;RobotLoginConfigurer, HttpSecurity\u0026gt; { private final List\u0026lt;String\u0026gt; passwords = new ArrayList\u0026lt;\u0026gt;(); @Override public void init(HttpSecurity builder) throws Exception { // step 1 // initializes a bunch of objects // -\u0026gt; AuthenticationProviders // all the AuthenticationProviders are populated at init() they\u0026#39;re put in the builder, builder.authenticationProvider(new RobotAuthenticationProvider(passwords)); } @Override public void configure(HttpSecurity builder) throws Exception { // step 2 // this also initializes objects, but can reuse objects from step 1, even from other configurers // -\u0026gt; Filters // each filter chain may have their own AuthenticationManager var authenticationManager = builder.getSharedObject(AuthenticationManager.class); builder.addFilterBefore(new RobotFilter(authenticationManager), UsernamePasswordAuthenticationFilter.class); } public RobotLoginConfigurer password(String password) { this.passwords.add(password); return this; } } WebSecurityConfig.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @Bean public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception { http .authorizeHttpRequests(authorizeConfig -\u0026gt; { authorizeConfig.requestMatchers(\u0026#34;/\u0026#34;).permitAll(); authorizeConfig.requestMatchers(\u0026#34;/error\u0026#34;).permitAll(); authorizeConfig.requestMatchers(\u0026#34;/favicon.ico\u0026#34;).permitAll(); authorizeConfig.anyRequest().authenticated(); }) .formLogin(withDefaults()) // By calling httpBasic(), an instance of the BasicAuthenticationFilter is added to the filter chain. // https://stackoverflow.com/a/57577530/16317008 // 为了实现通过 curl -u \u0026#34;user:password\u0026#34; http://localhost:8080/private 登录 .httpBasic(withDefaults()) .oauth2Login(withDefaults()) // return HttpSecurity .authenticationProvider(new DavidAuthenticationProvider()) // return HttpSecurity .apply(new RobotLoginConfigurer()) // return Configurer .password(\u0026#34;beep-boop\u0026#34;) // return Configurer .password(\u0026#34;boop-beep\u0026#34;); return http.build(); } # 2:10:40 Lambda DSL ","date":"2023-08-08T20:58:54Z","permalink":"https://blog.yorforger.cc/p/csrf-filter-providermanager-authenticationproviders-spring%E5%AD%A6%E4%B9%A0%E4%B8%83/","title":"CSRF Filter \u0026 ProviderManager \u0026 AuthenticationProviders, Spring学习(七)"},{"content":" # 视频介绍 此文章参考视频的 54:35 之后, 用于做笔记, 可直接观看视频:\n{% youtube iJ2muJniikY %}\n关于 authentication object, 可以参考上一篇或者观看视频 33:00, 关于 filter chain, 可参考视频 39:15, 或参考: FIlter Architecture Spring Security # 功能陈述 添加一个管理员角色, 可以不用登录, 通过 special header 就可访问所有需要认证的页面, 大概下面这样: curl localhost:8080/private -H \u0026quot;x-robot-password: beep-boop\u0026quot; -v # 项目结构 此文章所用代码在上一篇文章基础上编写, 本篇文章代码地址: https://github.com/shwezhu/springboot-learning/tree/master/spring-mr-robot-filter-demo\n1 2 3 4 5 6 7 └── david └── zhu ├── RobotAuthentication.java ├── RobotFilter.java ├── SpringSecurityDemoApplication.java ├── WebController.java └── WebSecurityConfig.java 主要是为了理解 FilterChian 是一个 list 结构, 即有多个 filter, 然后每个 filter 都有 doFilter 方法, 他们是在方法体里被调用的 而不是 for loop 结构被循环调用, 这样的好处是我们可以决定在方法体的哪部分进行 doFilter 调用, doFilter 就是进入下一个 filter, 这里说的不准确, 深入理解参考: FIlter Architecture Spring Security\n# 自定义 Filter - RobotFilter类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class RobotFilter extends OncePerRequestFilter { @Override protected void doFilterInternal( HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException { System.out.println(\u0026#34;🤖️ HELLO FORM Robot Filter~\u0026#34;); // Every Filter has a doFilter() method, // But they are not called by for loop over a list of filters // but actually a chain of responsibility // video: https://youtu.be/iJ2muJniikY, at 00:42:53 filterChain.doFilter(request, response); } } 把该 Filter 添加到 SecurityFilterChain, 在我们上篇文章常见的 WebSecurityConfig 类内,\n1 2 3 4 5 6 7 8 9 10 11 12 13 @Bean public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception { return http .authorizeHttpRequests(authorizeConfig -\u0026gt; { authorizeConfig.requestMatchers(\u0026#34;/\u0026#34;).permitAll(); ... }) .formLogin(withDefaults()) .oauth2Login(withDefaults()) // !!! add this line below: .addFilterBefore(new RobotFilter(), UsernamePasswordAuthenticationFilter.class) .build(); } 然后输入\n1 2 $ curl localhost:8080 Hello David~% 项目终端输出,\n1 🤖️ HELLO FORM Robot Filter~ 解释下为何继承 OncePerRequestFilter ,\n每个 servlet 都可以有不同的 SecurityFilterChain, 也就是说一个项目可以有多个 SecurityFilterChain, 在上一篇文章中提到的一个 WebSecurityConfig 类, 我们在该类中配置了 SecurityFilterChain, 其实 SecurityFilterChain 可以有多个, 即我们现在的项目不够复杂, 只用了一个, 比如我们可以再加个 SecurityFilterChain 让其只处理 /api/** 相关 endpoints, 如下:\n1 2 3 4 5 6 7 8 9 10 11 12 @Bean public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception { return http // https://stackoverflow.com/a/75219552/16317008 .securityMatcher(\u0026#34;/api\u0026#34;) .authorizeHttpRequests(authorizeConfig -\u0026gt; { authorizeConfig.requestMatchers(\u0026#34;/\u0026#34;).permitAll(); ... }) .formLogin(withDefaults()) ... } 写 JSP 的时候一个 servlet 负责的是一个对应的 endpoint, 我们写 spring 代码的时候只是用个简单的注解指定对应endpoint, Spring 框架在底层帮我们实现了对应的 servlet , 然后运行的时候这些 servlet 运行在 tomcat 中, tomcat 会把每个 request 分类然后转发给对应的 servlet, 说这些只是想说, servlet 才是真正处理 request 的东西, 即产生 http response, 而 filter 则是工作在 servlet 之前或者之后帮我们过滤, 认证 request, servlet 很笨, 只负责处理传给他的 request, 像是个没大脑的工厂,\n上面我们提到, 每个 servlet 都可以有不同的 SecurityFilterChain, 于是再看下面这些解释使用 OncePerRequestFilter 的原因, 你就能看懂了,\nThe request could be dispatched to a different (or the same) servlet using the request dispatcher. A common use-case is in Spring Security, where authentication and access control functionality is typically implemented as filters that sit in front of the main application servlets. When a request is dispatched using a request dispatcher, it has to go through the filter chain again (or possibly a different one) before it gets to the servlet that is going to deal with it. The problem is that some of the security filter actions should only be performed once for a request. Hence the need for this filter. What is OncePerRequestFilter? - Stack Overflow\nA Filter can be called either before or after servlet execution. When a request is dispatched to a servlet, the RequestDispatcher may forward it to another servlet. There\u0026rsquo;s a possibility that the other servlet also has the same filter. In such scenarios, the same filter gets invoked multiple times. Spring guarantees that the OncePerRequestFilter is executed only once for a given request. What Is OncePerRequestFilter? | Baeldung\n其实这里还有需要深入探讨的东西, 比如 RequestDispatcher 是做什么的, 现在我们要知道的是, 一个项目可以有多个 FilterChain, Servlet 才是真正处理 http request的东西, FilterChain 只是帮 servlet 过滤 request, 至于 每个 request 怎么被 tomcat 分配到 对应servlet的, 这还需要深入的学习才能理解, 关于 FIlterChain 可参考: Architecture Spring Security\n# 实现思路 创建我们的特殊 RobotFilter, 通过继承 OncePerRequestFilter 实现 因为访问的页面需要 authenticated, 因此创建特殊 RobotFilter 时, 需要新建一个 authentication object RobotAuthentication 来表示 robot 用户 在账号密码登录认证 filter 前添加我们刚自定义的 RobotFilter, 通过修改 WebSecurityConfig::securityFilterChain 方法实现 本文主要参考: Spring Security, demystified by Daniel Garnier Moiroux\n","date":"2023-08-07T20:58:53Z","permalink":"https://blog.yorforger.cc/p/spring-security-filter-chian-%E8%87%AA%E5%AE%9A%E4%B9%89-robotfilter-spring%E5%AD%A6%E4%B9%A0%E5%85%AD/","title":"Spring Security Filter Chian 自定义 RobotFilter, Spring学习(六)"},{"content":" # 1. Optional Class Java 8 Optional 定义如下:\n1 2 3 // A container object which may or may not contain a non-null value. If a value is present, `isPresent()` will return `true` and `get()` will return the value. public final class Optional\u0026lt;T\u0026gt; extends Object 之前讨论的 java.lang.Class, 也是一个类包装器, 若一个函数的返回值为 Optional\u0026lt;?\u0026gt; 则就是返回 Optional\u0026lt;?\u0026gt; 的对象, 这很容易看出来, 不要加个泛型就迷了, 如一个方法的签名为 public Cat foo(); 则该方法返回的一定是 Cat 的对象,\nOptional 就是为了减少我们的 null check 代码, Helps you avoid NullPointerException, 所以不要乱用 optional, 下面这个视频讲的很好:\n{% youtube vKVzRbsMnTQ %}\n看完视频再啰嗦几句, 此文章看似是了解 Java 8 Optional, 其实涉及到了generics和functional interface, 了解后阅读源码也会容易很多, 因为很多都用到了这两个特性, 了解更多:\nFunctional Interfaces \u0026amp; Anonymous Classes \u0026amp; Lambda Expressions - Java | 橘猫小八的鱼 java.lang.Object \u0026amp; java.lang.Class | 橘猫小八的鱼 # 2. Method Reference 学 Spring Security, 看到一段自定义代码, 用来获取通过 Open ID 登录用户的 email:\n1 2 3 4 5 6 7 private static String getName(Authentication authentication) { return Optional.of(authentication.getPrincipal()) .filter(OidcUser.class::isInstance) .map(OidcUser.class::cast) .map(OidcUser::getEmail) .orElseGet(authentication::getName); } # 2.1. filter() 简单分析一下 filter(),\n1 public Optional\u0026lt;T\u0026gt; filter(Predicate\u0026lt;? super T\u0026gt; predicate); 关于 filter() 的介绍: If a value is present, and the value matches the given predicate, return an Optional describing the value, otherwise return an empty Optional. 这是什么意思呢?\n假设我们有个方法 getCat() 可以从数据库获取 Cat 对象, 然后我们调用它:\n1 Optional\u0026lt;Cat\u0026gt; optionalCat = getCat(\u0026#34;Milo\u0026#34;); 此时 optionalCat 就是一个 Cat 对象的包装器, a Optional that describes the value, 然后我们接着做下面的事情:\n1 optionalCat.filter(cat -\u0026gt; cat.getName().equals(\u0026#34;Milo\u0026#34;)); 如果 optionalCat 不为空, 且 lambada 表达式返回值为 true 即上面表述: If a value is present, and the value matches the given predicate, 则 return an Optional describing the value, 即相当于对 cat 对象进行二次筛选, 第一次不为空, 第二次需要 cat 名字为 “milo”, 若这俩条件都满足, 此时 filter() 才会返回一个 Optional\u0026lt;Cat\u0026gt; 对象, 也就是和最初的 optionalCat 相同, 若那两个条件有一个没满足, 则 filter() 返回一个空 Optional 对象,\n另外该方法参数类型是一个 functional interface, 即 Predicate\u0026lt;? super T\u0026gt; , 可以看看其定义,\n1 2 3 4 5 @FunctionalInterface public interface Predicate\u0026lt;T\u0026gt; { boolean test(T t); ... } 如果你了解 functional interface, 不难发现对于这种参数类型我们可以直接传 lambda expression, 但是上面代码传的是 OidcUser.class::isInstance 这是什么鬼? 别慌, 这不是 c++ 的 namespace, 这是 Java 8 的新特性 Method Reference,\nMethod references are a special type of lambda expressions. They\u0026rsquo;re often used to create simple lambda expressions by referencing existing methods. Method References in Java\n不得不说 Java 8 带来了很多的新的特性: Optional, Method Reference, Lambda,\n# 2.2. map() 对于另一个方法 map() , 则是会返回一个新类型的 Optional 对象,\n1 public \u0026lt;U\u0026gt; Optional\u0026lt;U\u0026gt; map(Function\u0026lt;? super T,? extends U\u0026gt; mapper); If a value is present, apply the provided mapping function to it, and if the result is non-null, return an Optional describing the result. Otherwise return an empty Optional.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class Main { public static void main(String[] args) { Optional\u0026lt;Cat\u0026gt; optionalCat = getCat(\u0026#34;Milo\u0026#34;); int age = optionalCat .filter(cat -\u0026gt; cat.getName().equals(\u0026#34;Milo\u0026#34;) ) .map(Cat::getAge) .orElse(0); System.out.println(age); } public static Optional\u0026lt;Cat\u0026gt; getCat(String name) { // 模拟查找数据 Cat cat = new Cat(name, 2); return Optional.ofNullable(cat); } } 若 filter() 返回的不是空 Optional, 且 Cat::getAge 没有返回空值, 则返回一个新的 Optional, 类型为 Optional, 即 return an Optional describing the result,\n","date":"2023-08-06T15:44:52Z","permalink":"https://blog.yorforger.cc/p/java-8-optional-method-reference/","title":"Java 8 Optional \u0026 Method Reference"},{"content":" # 1. Anonymous Classes Anonymous Class Extending a Class Anonymous Class Implementing an Interface 如何理解呢, 如 Cat cat = new Cat() {...}, 若 Cat 是个类, 那对象 cat 就是个类型为匿名的类, 只不过这个类继承了 Cat , 同理若 Cat 是个接口 interface, 那对象 cat 的类型就是 a class that implements the interface Cat , 具体可参考下面这个视频, 讲得很好:\n{% youtube DwtPWZn6T1A %}\n# 2. Functional Interfaces \u0026amp; Lambda Expressions 若想定义一个方法, 且这个方法的参数也是一个方法, 就可以考虑使用 functional interface All a Lambda is, is a shorcut to define an implementation of a functional interface. Functional interface 只可以有一个 abstract method, 但可以有多个 default, static methods, 最好使用注解 @FunctionalInterface , 它告诉编译器相关信息, 若接口有多个 abstract methods, 那编译器就会报错, 常见的 functional interface 有: Runnable, ActionListener, and Comparable, 举个例子, Java 创建线程一般如下:\n1 2 3 4 5 6 7 8 9 10 11 public class Main { public static void main(String[] args) { // Creating and starting a thread using lambda expression Thread myThread = new Thread(() -\u0026gt; { // Code to be executed in the thread System.out.println(\u0026#34;Thread is running\u0026#34;); }); myThread.start(); } } 你看 Thread() 的构造方法可以接受一个 lambda 表达式, 我们去看看其函数原型,\n1 2 3 public Thread(Runnable task) { ... } 此时不用看, Runnable 肯定是个 functional interface, 不然是没办法接受 lambda的,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /** * Represents an operation that does not return a result. * * \u0026lt;p\u0026gt; This is a {@linkplain java.util.function functional interface} * whose functional method is {@link #run()}. * * @author Arthur van Hoff * @see java.util.concurrent.Callable * @since 1.0 */ @FunctionalInterface public interface Runnable { /** * Runs this operation. */ void run(); } 具体参考下面这个视频, 讲的很好:\n{% youtube tj5sLSFjVj4 %}\n细品这句话:\nOne of the most welcome changes in Java 8 was the introduction of lambda expressions, as these allow us to forego anonymous classes, greatly reducing boilerplate code and improving readability. Method References in Java\nBack in ye olden times before Java 8, we had to use the clunky old anonymous class syntax for this sort of thing, leading to code that looked something like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 object.gimmeFunction(new SingleMethodInterface(){ @Override public bool really(int n){ return n==\u0026#34;really\u0026#34;.hashCode(); } }); Collections.sort(emplyeesList, new Comparator() { public int compare(Employee a1, Employee a2){ return a1.getId().compareTo(a2.getId()); }}); File[] hiddenFiles = new File(\u0026#34;directory_name\u0026#34;).listFiles(new FileFilter() { public boolean accept(File file) { return file.getName().endsWith(\u0026#34;.xml\u0026#34;); }}); // With lambda: File[] hiddenFiles = new File(\u0026#34;directory_name\u0026#34;).listFiles( file -\u0026gt; file.getName().endsWith(\u0026#34;.xml\u0026#34;)); 另外 Functional Interface 也多用在返回值, 如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 @FunctionalInterface public interface ApplicationListener\u0026lt;E extends ApplicationEvent\u0026gt; extends EventListener {...}; @Bean // ApplicationListener\u0026lt;AuthenticationSuccessEvent\u0026gt; 是个 functional interface // 所以该方法返回的是个 lambda expression public ApplicationListener\u0026lt;AuthenticationSuccessEvent\u0026gt; successListener() { return event -\u0026gt; System.out.printf( \u0026#34;\\uD83C\\uDF89 Success [%s] %s %n\u0026#34;, event.getAuthentication().getClass().getName(), event.getAuthentication().getName() ); } 了解更多:\nlambda - What did Java programmers do before Java 8 if they wanted to pass a function around? - Stack Overflow\nLambda Expressions Before And After Java 8 - Java Code Geeks - 2023\n","date":"2023-08-06T12:03:53Z","permalink":"https://blog.yorforger.cc/p/functional-interfaces-anonymous-classes-lambda-expressions-java/","title":"Functional Interfaces \u0026 Anonymous Classes \u0026 Lambda Expressions - Java"},{"content":" # 1. java.lang.Object 定义:\n1 public class Object {...} The java.lang.Object class is the root of the class hierarchy. Every class has Object as a superclass. 所有的类(包括自定义类)都自动继承了类 java.lang.Object, 你可以自己创建个类, 然后查看其对象可调用的方法, 如下:\n可以看到一些方法如 equals(), getClass(), 这都是 Cat 继承自类 Object 所得,\n1 public final Class\u0026lt;?\u0026gt; getClass() 了解更多: Object (Java Platform SE 8 ),\n# 2. java.lang.Class Class 定义如下 (注意区分 Class 和 class, 前者是类, 后者是关键字):\n1 2 3 public final class Class\u0026lt;T\u0026gt; extends Object implements Serializable, GenericDeclaration, Type, AnnotatedElement 与 java.lang.Objetc 不同, java.lang.Class 是 final 的, 所以没有类可以继承它, 而且它唯一的构造函数是私有的, 这意味着我们不能通过正常的方式来创建 Class 的对象.\n注意 java.lang.Class 是泛型类, 因此我们经常可以见到类似 Class\u0026lt;?\u0026gt; xxx = cat.getClass() 的声明,\n# 2.1. java.lang.Class 为何存在 其实刚开始一直想不明白类 java.lang.Class 到底是什么, 为什么存在?\n因为在我的印象里, 一个泛型类一般都是作为 collection 存在, 如 ArrayList, List 等, 他们使用泛型是为了代码的 reuse, 比如存 string, Integer, 等, 但是 java.lang.Class 呢? 它又不是 collection, 为什么要用泛型呢?\n我现在的理解是 java.lang.Class 是一个类的包装器, 也是为了代码的 reuse, 查看 java.lang.Class 的方法:\ngetClassLoader(), getDeclaredMethods(), getFields() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Employee extends Person { ... ... } public class Main { public static void main(String[] args) throws NoSuchFieldException { Class\u0026lt;?\u0026gt; myClass = Employee.class; System.out.println(\u0026#34;Name: \u0026#34; + myClass.getName()); System.out.println(\u0026#34;Simple name: \u0026#34; + myClass.getSimpleName()); System.out.println(\u0026#34;Superclass: \u0026#34; + myClass.getSuperclass()); System.out.println(\u0026#34;Interfaces: \u0026#34; + Arrays.toString(myClass.getInterfaces())); System.out.println(\u0026#34;Methods: \u0026#34; + Arrays.toString(myClass.getMethods())); System.out.println(\u0026#34;Fields\u0026#34; + Arrays.toString(myClass.getFields())); } } 不难发现所有的类都可以用到这些方法, 这也是反射的根基, runtime 的时候用来获取某个类的信息, 在介绍 java.lang.Object 的时候, Object 的一个方法如下:\n1 public final Class\u0026lt;?\u0026gt; getClass() 这个返回值 Class\u0026lt;?\u0026gt; 代表什么? 代表方法可以返回 getClass() 任何类型的对象, 如 Class\u0026lt;Integer\u0026gt;, Class\u0026lt;String\u0026gt;, or Class\u0026lt;Object\u0026gt;. 你可能会反驳, 不是任何类型的对象啊, 你看你返回的是 Class\u0026lt;Integer\u0026gt; 而不可以返回 Integer 的对象, ummm, 别忘了前面说的, java.klang.Class 是作为一个类额包装器存在的, 为什么需要这样的一个类包装器? 因为反射机制,\nEvery time JVM creates an object , it also creates a java.lang.Class object that describes the type of the object . All instances of the same class share the same java.lang.Class object and you can obtain the java.lang.Class object by calling the getClass() method of the object. This method is inherited from java.lang.Object class . 这里说的很晕各种 object, 比如一个类 Cat, 当你第一次创建 Cat 的对象时, 如 cat_1, 此时一个 java.lang.Class 的对象就会被创建, 类型是 Class\u0026lt;Cat\u0026gt; 用来描述类 Cat, 之后你想获得刚被创建的这个 java.lang.Class 的对象(类型为Class\u0026lt;Cat\u0026gt; ) , 你可以调用 getClass() 方法, 即 cat_1.getClass(), 此时刚好说明了泛型 wildcard 的重要性, 即 getClass() 的返回值类型只能是 Class\u0026lt;?\u0026gt; , 否则当你再创建一个类的对象如 Dog 的时候, 难道还要再另重载个 getClass() 函数吗? 即: public final Class\u0026lt;Cat\u0026gt; getClass(), public final Class\u0026lt;Dog\u0026gt; getClass(), 这也太离谱了,\nWhen a class (say, Cat) is accessed for the very first time (e.g., say an instance of it is created ~ new Hello()), then class loader (a component of JVM) loads it (the bytecodes) and creates an instance of java.lang.Class that represents the class in context, i.e., Cat. Thereafter, when you are creating an object of that same class (Cat), this java.lang.Class instance would be used by JVM to create the object. This java.lang.Class object is also the entry point to use reflection and it can also be used to get meta-information about the class like the class name, super-class name, etc.\n# 2.2. 为什么需要反射 We need java.lang.Class.forName() and java.lang.Class.newInstance() because many times it happens that we don\u0026rsquo;t know the name of the class to instantiate while writing code , we may get it from config files , database , network or from any Java application . This is the reflective way of creating an object which is one of the most powerful feature of Java and which makes way for many frameworks e.g. Spring , Struts which uses Java reflection .\nCan you create an object without using new operator in Java?\n1 2 Class c = Class.forName(\u0026#34;java.lang.String\u0026#34;); String object = (String) c.newInstance(); # 3. Creating a java.lang.Class object # 3.1. Class.forName() Since class Class doesn’t contain any constructor, there is static factory method present in class Class, which is Class.forName(), used for creating object of class Class. Below is the syntax :\n1 2 3 4 5 6 7 8 public class Main { public static void main(String[] args) throws ClassNotFoundException { String nameOfClass = \u0026#34;Employee\u0026#34;; Class\u0026lt;?\u0026gt; cls = Class.forName(nameOfClass); ClassLoader cLoader = cls.getClassLoader(); System.out.println(cLoader); } } # 3.2. Myclass.class Please note that this method is used with class name, not with class instances.\n# 3.3. obj.getClass() obj.getClass()定义在java.lang.Object.java下:\n1 2 3 4 5 public final native Class\u0026lt;?\u0026gt; getClass(); e.g., A a = new A(); // Any class A Class c = a.getClass(); 参考:\njava lang Class Class - java.lang.Class Class in Java https://qr.ae/pyJqWx ","date":"2023-08-05T19:47:51Z","permalink":"https://blog.yorforger.cc/p/java.lang.object-java.lang.class/","title":"java.lang.Object \u0026 java.lang.Class"},{"content":" # 1. Why Generics? 至于泛型提高类型安全性的话题, 暂不讨论, 这太哲学了, 只举例子, 没有泛型之前, 考虑如何实现一个容器(可以装所有的类型), 提示, 可以用 Object 类,\n具体实现可参考: Java Array | 橘猫小八的鱼,\n可以看出, 在 retrieve 数据的时候, 每次都需要 type cast, 这很麻烦也很容易出错, 也不优雅,\n使用泛型实现一个简单的容器, 如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class MyContainer\u0026lt;T\u0026gt; { private static final int DEFAULT_CAPACITY = 10; private int size; private Object[] elements; public MyContainer() { elements = new Object[DEFAULT_CAPACITY]; size = 0; } public void addItem(T element) { if (size \u0026lt; elements.length) { elements[size] = element; size++; } else { // Handle array resizing or throw an exception } } @SuppressWarnings(\u0026#34;unchecked\u0026#34;) public T get(int index) { if (index \u0026lt; 0 || index \u0026gt;= size) { // handle exception } return (T) elements[index]; } } public class Main { public static void main(String []args){ MyContainer\u0026lt;String\u0026gt; container = new MyContainer\u0026lt;\u0026gt;(); container.addItem(\u0026#34;Hello\u0026#34;); // container.addItem(2); // this will cause error String str_1 = container.get(0); // no need to explicit cast System.out.println(str_1); } } 此次学泛型主要是为了阅读源码, 常见符号:\nE: Element K: Key N: Number T: Type V: Value S,U,V etc. - 2nd, 3rd, 4th types\n# 2. Bounded Types Oftentimes there are cases where we need to specify a generic type, but we want to control which types can be specified, rather than keeping the gate wide open. Bounded types can be used to restrict the bounds of the generic type by specifying the extends or the super keyword in the type parameter section to restrict the type by using an upper bound or lower bound, respectively.\n1 2 3 \u0026lt;T extends UpperBoundType\u0026gt; \u0026lt;T super LowerBoundType\u0026gt; e.g.,\n1 2 3 4 5 6 7 8 public class GenericNumberContainer \u0026lt;T extends Number\u0026gt; { // ... } GenericNumberContainer\u0026lt;Integer\u0026gt; gn = new GenericNumberContainer\u0026lt;Integer\u0026gt;(); gn.setObj(3); // Type argument String is not within the upper bounds of type variable T GenericNumberContainer\u0026lt;String\u0026gt; gn2 = new GenericNumberContainer\u0026lt;String\u0026gt;(); # 3. Wildcards In some cases, it is useful to write code that specifies an unknown type. The question mark (?) wildcard character can be used to represent an unknown type using generic code. Wildcards can be used with parameters, fields, local variables, and return types. However, it is a best practice to not use wildcards in a return type, because it is safer to know exactly what is being returned from a method.\nConsider the case where we would like to write a method to verify whether a specified object exists within a specified List. We would like the method to accept two arguments: a List of unknown type as well as an object of any type.\n1 2 3 4 5 6 7 public static \u0026lt;T\u0026gt; void checkList(List\u0026lt;?\u0026gt; myList, T obj){ if(myList.contains(obj)){ System.out.println(\u0026#34;The list contains the element: \u0026#34; + obj); } else { System.out.println(\u0026#34;The list does not contain the element: \u0026#34; + obj); } } Oftentimes wildcards are restricted using upper bounds or lower bounds. Much like specifying a generic type with bounds, for example, if we wanted to alter the checkList method to accept only Lists that extend the Number type,\n1 2 3 4 5 6 7 8 9 public static \u0026lt;T\u0026gt; void checkNumber(List\u0026lt;? extends Number\u0026gt; myList, T obj){ if(myList.contains(obj)){ System.out.println(\u0026#34;The list \u0026#34; + myList + \u0026#34; contains the element: \u0026#34; + obj); } else { System.out.println(\u0026#34;The list \u0026#34; + myList + \u0026#34; does not contain the element: \u0026#34; + obj); } } # 3.1. Wildcards vs Object Now, since Object is the inherent super-type of all types in Java, we would be tempted to think that it can also represent an unknown type. In other words, List\u003c?\u003e and List could serve the same purpose. But they don\u0026rsquo;t.\n1 2 3 4 5 6 7 8 9 10 11 public static void printListObject(List\u0026lt;Object\u0026gt; list) { for (Object element : list) { System.out.print(element + \u0026#34; \u0026#34;); } } public static void printListWildCard(List\u0026lt;?\u0026gt; list) { for (Object element: list) { System.out.print(element + \u0026#34; \u0026#34;); } } Given a list of Integers, say:\n1 List\u0026lt;Integer\u0026gt; li = Arrays.asList(1, 2, 3); printListObject(li) will not compile, and we\u0026rsquo;ll get this error:\n1 The method printListObject() is not applicable for the arguments (List\u0026lt;Integer\u0026gt;) Whereas printListWildCard(li) will compile and will output 1 2 3 to the console.\n# 3.2. Wildcards vs T 1 2 3 4 // 指定集合元素只能是T 类型 List\u0026lt;T\u0026gt; list=new ArrayList\u0026lt;T\u0026gt;(); // 集合元素可以是任意类型, 这种没有意义, wildcard 经常用在方法的形参声明中, 在这只是为了说明用法 List\u0026lt;?\u0026gt; list=new ArrayList\u0026lt;?\u0026gt;(); ？和 T 都表示不确定的类型，区别在于我们可以对 T 进行操作，但是对 ? 不行：\n1 2 T t = operate(); // 可以 ？ car = operate(); // 不可以 T 是一个 确定的 类型, 通常用于泛型类和泛型方法的定义, ？是一个 不确定 的类型, 通常用于声明泛型方法的形参, 返回值, 或创建变量如Class\u0026lt;?\u0026gt; xxx = cat.getClass(),\n如果还无法理解可以参考: java.lang.Object \u0026amp; java.lang.Class | 橘猫小八的鱼\n# 4. Type Erasure Java generics only exit during compile time. Java的泛型基本上都是在编译器这个层次上实现的，在生成的 bytecode 中不包含泛型中的类型信息的，使用泛型的时候加上类型参数，在编译器编译的时候会discard，这个过程成为type erasure\n# 4.1. 通过两个例子证明Java类型的类型擦除 原始类型相等 1 2 3 4 5 6 7 8 9 10 11 12 13 public class Test { public static void main(String[] args) { ArrayList\u0026lt;String\u0026gt; list1 = new ArrayList\u0026lt;String\u0026gt;(); list1.add(\u0026#34;abc\u0026#34;); ArrayList\u0026lt;Integer\u0026gt; list2 = new ArrayList\u0026lt;Integer\u0026gt;(); list2.add(123); System.out.println(list1.getClass() == list2.getClass()); } } 通过反射添加其它类型元素 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class Test { public static void main(String[] args) throws Exception { ArrayList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;Integer\u0026gt;(); list.add(1); //这样调用 add 方法只能存储整形，因为泛型类型的实例为 Integer list.getClass().getMethod(\u0026#34;add\u0026#34;, Object.class).invoke(list, \u0026#34;asd\u0026#34;); for (int i = 0; i \u0026lt; list.size(); i++) { System.out.println(list.get(i)); } } } # 4.2. 类型擦除后保留的原始类型 在上面，两次提到了原始类型，什么是原始类型？\n原始类型 就是擦除去了泛型信息, 最后在字节码中的类型变量的真正类型, 无论何时定义一个泛型，相应的原始类型都会被自动提供，类型变量擦除，并使用其bound type（unbounded type的变量用Object）替换,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class Pair\u0026lt;T\u0026gt; { private T value; public T getValue() { return value; } public void setValue(T value) { this.value = value; } } // Pair的原始类型为 public class Pair { private Object value; public Object getValue() { return value; } public void setValue(Object value) { this.value = value; } } 因为在Pair\u0026lt;T\u0026gt;中，T 是一个无限定的类型变量，所以用Object替换，其结果就是一个普通的类，但如果类型变量是bound，那么原始类型就用第一个边界的类型变量类替换。\n比如: Pair这样声明的话, 那么原始类型就是Comparable\n1 public class Pair\u0026lt;T extends Comparable\u0026gt; {} # 5. 类型擦除引起的问题及解决方法 # 5.1. 自动类型转换 因为类型擦除的问题，所以所有的泛型类型变量最后都会被替换为原始类型。\n既然都被替换为原始类型，那么为什么我们在获取get()的时候，不需要进行强制类型转换呢？因为get()已经帮我们cast了.\n看下ArrayList.get()方法：\n1 2 3 4 5 6 7 8 public E get(int index) { Objects.checkIndex(index, size); return elementData(index); } E elementData(int index) { return (E) elementData[index]; } # 5.2. 泛型类型变量不能是基本数据类型 1 2 ArrayList\u0026lt;int\u0026gt; arr; //error ArrayList\u0026lt;Integer\u0026gt; arr; 不能用类型参数替换基本类型。就比如，没有ArrayList\u0026lt;double\u0026gt;，只有ArrayList\u0026lt;Double\u0026gt;。因为当类型擦除后，ArrayList的原始类型变为Object，但是Object类型不能存储double值，只能引用Double的值。\n# 5.3. 泛型在静态方法和静态类中的问题 1 2 3 4 5 6 public class Test2\u0026lt;T\u0026gt; { public static T one; //编译错误 public static T show(T one){ //编译错误 return null; } } 因为泛型类中的泛型参数的实例化是在定义对象的时候指定的，而静态变量和静态方法不需要使用对象来调用。对象都没有创建，如何确定这个泛型参数是何种类型，所以当然是错误的。\n但是要注意区分下面的一种情况：\n1 2 3 4 5 public class Test2\u0026lt;T\u0026gt; { public static \u0026lt;T \u0026gt;T show(T one){ //这是正确的 return null; } } 因为这是一个泛型方法，在泛型方法中使用的T是自己在方法中定义的 T，而不是泛型类中的T\n参考:\nGenerics: How They Work and Why They Are Important https://www.cnblogs.com/wuqinglong/p/9456193.html 聊一聊-JAVA 泛型中的通配符 T，E，K，V，？ - 掘金 ","date":"2023-08-05T18:44:52Z","permalink":"https://blog.yorforger.cc/p/java-generics/","title":"Java Generics"},{"content":"关于 type safety, 一直没有一个确切的定义, 每个语言的作者对 type safety 可能有不同的理解, 我比较喜欢的是维基百科对 type safety 的定义, 即 type safety 阻止或者使 type errors 不容易发生, 而什么是 type errors 在下面这段话里也给出了定义:\nIn computer science, type safety is the extent to which a programming language discourages or prevents type errors. The behaviors classified as type errors are usually that result from attempts to perform operations on values that are not of the appropriate data type, e.g., adding a string to an integer when there\u0026rsquo;s no definition on how to handle this case. This classification is partly based on opinion. https://en.wikipedia.org/wiki/Type_safety\n感觉维基百科对类型安全的定义与 Why Rust 一书的作者观点不谋而合:\nIf a program has been written so that no possible execution can exhibit undefined behavior, we say that program is well defined. If a language’s type system ensures that every program is well defined, we say that language is type safe.\nundefined behavior: 在 c99 的定义中, undefined behaviors 有很多: 如分母为 0, 访问超出数组大小的位置 (c 是没有越界检查的), 让一个int能表示的最大数加一,\n1 2 3 4 5 int main(int argc, char **argv) { unsigned long a[1]; a[3] = 0x7ffff7b36cebUL; return 0; } According to C99, because this program accesses an element off the end of the array a, its behavior is undefined, meaning that it can do anything whatsoever.\nA carefully written C or C++ program might be well defined, but C and C++ are not type safe: the program shown earlier has no type errors, yet exhibits undefined behavior. By contrast, Python is type safe. Python is willing to spend processor time to detect and handle out-of-range array indices in a friendlier fashion than C:\n1 2 3 4 5 6 \u0026gt;\u0026gt;\u0026gt; a = [0] \u0026gt;\u0026gt;\u0026gt; a[3] = 0x7ffff7b36ceb Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; IndexError: list assignment index out of range \u0026gt;\u0026gt;\u0026gt; Python raised an exception, which is not undefined behavior: the Python documentation specifies that the assignment to a[3] should raise an IndexError exception, as we saw. Certainly, a module like ctypes that provides unconstrained access to the machine can introduce undefined behavior into Python, but the core language itself is type safe. Java, JavaScript, Ruby, and Haskell are similar in this way.\nNote that being type safe is independent of whether a language checks types at compile time or at runtime: C checks at compile time, and is not type safe; Python checks at runtime, and is type safe.\n以上讨论来自: Why Rust Chapter 1 , 感兴趣可以自己翻阅,\n","date":"2023-08-05T18:32:53Z","permalink":"https://blog.yorforger.cc/p/type-safety-from-why-rusy-jim-blandy/","title":"Type Safety from Why Rusy - Jim Blandy"},{"content":"两点很重要:\nArrays are objects in Java, we can find their length using the object property length. This is different from C/C++, where we find length using sizeof(). The length of an array is established when the array is created. After creation, its length is fixed. However, an array reference can be made to point to another array. 数组进行排序的话，可以使用 Arrays 类提供的 sort() 方法:\n1 2 int[] anArray = new int[] {5, 2, 1, 4, 8}; Arrays.sort(anArray); 有时要从数组中查找某个具体的元素, 如果数组提前进行了排序就可以使用二分查找法,\n1 2 int[] anArray = new int[] {1, 2, 3, 4, 5}; int index = Arrays.binarySearch(anArray, 4); 在没有范型出现的时候, 容器一般利用 Object 实现, 缺点显而易见, 每次取出数据都需要强制转换, 这就容易出现因忘记显示转换类型或转换错类型而导致的类型相关错误, 即降低了 Java 的 type safety, 如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 public class Main { public static void main(String []args){ MyContainer container = new MyContainer(6); container.addItem(\u0026#34;Hello\u0026#34;); container.addItem(42); container.addItem(true); // We have to cast and must cast the correct type to avoid ClassCastException! String str = (String) container.getItem(0); int number = (int) container.getItem(1); boolean bool = (boolean) container.getItem(2); System.out.println(str); System.out.println(number); System.out.println(bool); } } class MyContainer { private Object[] items; private int size; public MyContainer(int capacity) { size = 0; items = new Object[capacity]; } public int getSize() { return size; } public void addItem(Object item) { if (size \u0026lt; items.length) { items[size] = item; size++; } else { // Handle array resizing or throw an exception System.out.println(\u0026#34;Error: array is full.\u0026#34;); } } public Object getItem(int index) { if (index \u0026gt;= 0 \u0026amp;\u0026amp; index \u0026lt; items.length) { return items[index]; } else { // Handle index out of bounds or throw an exception return null; } } } ","date":"2023-08-05T16:14:53Z","permalink":"https://blog.yorforger.cc/p/java-array/","title":"Java Array"},{"content":" # 0. 项目结构 1 2 3 4 5 6 7 8 9 10 11 12 └── src ├── main │ ├── java │ │ └── david │ │ └── zhu │ │ ├── SpringSecurityDemoApplication.java │ │ ├── WebController.java │ │ └── WebSecurityConfig.java │ └── resources │ ├── application.yaml │ ├── static │ └── templates 例子地址: https://github.com/shwezhu/springboot-learning/tree/master/spring-security-get-started-demo\n# 1. 创建一个简单的 spring boot 项目 - Controller 类 创建一个 spring 项目思路, 首先要想到创建的一个类是 Controller, 因为我们要处理不同的 endpoints, 比如叫 WebController.java, 名子无所谓, 只要加上 @Controller/@RestController 的类都是 Controller, 然后考虑你的这个 Controller 是要返回简单的 json 对象还是 web 页面, 前者的话就选择 @RestController 修饰你的 Controller, 至于这两个注解的区别, 可以参考: Spring Boot项目及踩坑总结 Spring学习(一)\n创建一个简单的 spring boot 项目, 依赖只选择 spring web, 之后慢慢添加各种依赖, 创建一个简单的 Controller, 如下:\n由于没有使用 spring security, 现在这两个 endpoints 都可以通过 http://localhost:8080/private 或 http://localhost:8080 访问,\n# 2. 使用 spring security 保护 web application 在项目中添加 spring security 依赖, 此例使用的 gradle, 因此编辑 build.gradle, 添加:\n1 implementation \u0026#39;org.springframework.boot:spring-boot-starter-security\u0026#39; 若你用的maven, 可以编辑 pom.xml 进行添加依赖, 添加完后 reload 配置然后直接运行项目, 此时两个endpoints, / 和 /private 都默认受到了保护, 需要登录才能访问,\n1 2 3 4 5 6 7 8 9 # -v, --verbose: Makes the fetching more verbose/talkative. $ curl localhost:8080 -v ... \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:8080 \u0026gt; Accept: */* \u0026lt; HTTP/1.1 401 \u0026lt; Set-Cookie: JSESSIONID=9342C2B61BCD3507B78E3602EC448A59; Path=/; HttpOnly ... 从输出可以看到 http code: 401,\n401 Unauthorized is the status code to return when the client provides no credentials or invalid credentials.\n只添加了 spring security 依赖, 然后运行项目, 我们的页面就被保护, 不得不说 spring 默默地在背后帮我们做了很多, 之所以需要登录才能访问那两个 endpoints的原因是在 spring security中 everything is secure by default, 但我们可以修改这些配置,\n在进行下一步的时候要注意, 即使我们试图访问一个不存在的页面比如 http://localhost:8080/foo , 得到的依旧是 401 forbidden code, 即会直接跳出登录页面, 而不是告诉你 404, 页面不存在, 即所有访问此网址的请求都被 spring security 拦截, 只有你成功登录, 才能访问页面, 此时因为页面不存在, 你得到的是 404 code, 注意此时默认用户名为 user, 密码会在程序运行中断输出, 可以尝试一下, 具体拦截过程如下, 可见主要是 SecurityFilterChain 类的作用,\nThis section examines how form-based login works within Spring Security. First, we see how the user is redirected to the login form:\nThe preceding figure builds off our SecurityFilterChain diagram:\nFirst, a user makes an unauthenticated request to the resource (/private) for which it is not authorized. Spring Security’s AuthorizationFilter indicates that the unauthenticated request is Denied by throwing an AccessDeniedException. Since the user is not authenticated, ExceptionTranslationFilter initiates Start Authentication and sends a redirect to the login page with the configured AuthenticationEntryPoint. In most cases, the AuthenticationEntryPoint is an instance of LoginUrlAuthenticationEntryPoint. The browser requests the login page to which it was redirected. Something within the application, must render the login page. 过程可参考: Form Login :: Spring Security\n# 3. 自定义 spring security - WebSecurityConfig 类 接下来我们创建一个新的类用来修改 spring security 的默认配置, 即指定 spring security 的行为, 比如指定访问哪个page需要认证才行, 支持什么方式登录, 如 form 登录, 谷歌登录, 以及登出行为和获取用户信息(我们登录需要比对用户信息吧, 密码是否匹配等), 我把这个类命名为 WebSecurityConfig.java, 这也是第二个重要的类, 第一个是第一节我们讲到的 Controller,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @Configuration @EnableWebSecurity public class WebSecurityConfig { @Bean public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception { return http // authorizeHttpRequests(): Allows restricting access based upon the HttpServletRequest using RequestMatcher implementations (i.e. via URL patterns). .authorizeHttpRequests( authorizeHttp -\u0026gt; { // Method authenticated(): Specify that URLs are allowed by any authenticated user. authorizeHttp.requestMatchers(\u0026#34;/private\u0026#34;).authenticated(); // exempt all http request from authentication except for \u0026#34;/private\u0026#34; authorizeHttp.anyRequest().permitAll(); }) // Enable form login, if I need login, show me a form // without this, when you access page that needs authentication will get Whitelabel Error Page with code 403 .formLogin(withDefaults()) .build(); } // UserDetailsService, an interface, is used to retrieve user-related data. // It is used to retrieve user-related data from a data source, such as a database or a file for authentication and authorization purposes. // e.g., It has one method named loadUserByUsername() which can be overridden to customize the process of finding the user. // It is used by the DaoAuthenticationProvider to load details about the user during authentication. @Bean public UserDetailsService userDetailsService() { // We are using in memory user password for demo purpose return new InMemoryUserDetailsManager( User.builder() .username(\u0026#34;david\u0026#34;) // The {noop} prefix is a marker indicates that the password should be treated as plain text. .password(\u0026#34;{noop}asd\u0026#34;) .authorities(\u0026#34;ROLE_user\u0026#34;) .build() ); } } 此时访问 http://localhost:8080/private , 我们会被自动转到登录页面, 账号密码不再是上面那节由spring自动生成, 而是我们自己制定的, 可以看代码中的注释, 注意此时我们用的 InMemoryUserDetailsManager 一般是作为demo来展示, 即创建了一个临时用户用于 form 登录, 实际开发 要从数据库或文件中获取用户的密码信息来比对, 具体方法之后会讲,\n如果此时你按照第二节我们所做的去尝试访问一个不存在的页面, http://localhost:8080/foo 会得到 Whitelabel Error Page with code 403, 你可以尝试一下, 看看, 这就和第二节我们得到的结果不同, 在第二节中我们会被跳转到登录页面, 这是因为什么呢? 为什么结果会不相同?\n还记得上面我们说过 spring security 的设计哲学是 secure by default, 即在第二节中我们没有修改任何 spring security的行为, 只是添加了依赖, 所以此时访问所有的页面都需要登录, 即使是个不存在的页面, 但是在这节我们修改了 spring security的行为, 仔细看上面关于控制哪些endpoint不需要认证便可访问的代码你会发现我们并没有遵守这个原则, 我们正在做的是 expose everything, and secure some specific endpoints, 这是不对的, 我们要反过来写, 改成:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Bean public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception { return http .authorizeHttpRequests( authorizeHttp -\u0026gt; { authorizeHttp.requestMatchers(\u0026#34;/\u0026#34;).permitAll(); authorizeHttp.requestMatchers(\u0026#34;/error\u0026#34;).permitAll(); authorizeHttp.requestMatchers(\u0026#34;/favicon.ico\u0026#34;).permitAll(); // Method authenticated(): Specify that URLs are allowed by any authenticated user. authorizeHttp.anyRequest().authenticated(); }) // Enable form login, if I need login, show me a form // without this, when you access page that needs authentication will get Whitelabel Error Page with code 403 .formLogin(withDefaults()) .build(); } 此时才是 secure by default: expose some specific endpoints, and secure everything, 你再访问 http://localhost:8080/foo , 就会跳转到登录页面,\n# 4. Single sign-on (SSO) 可跳过 Using SSO enables we can use open ID, 比如平时登录按钮下的 sign in with Google, 这种, 就是依靠的 Open ID, 在 spring 实现这个很简单,\n首先需要添加依赖, 编辑 build.gradle dependencies 部分, 添加:\n1 implementation \u0026#39;org.springframework.boot:spring-boot-starter-oauth2-client\u0026#39; 然后第二步就是 enable oauth2-login, 还记得上面那节里我们如何使能 form login 的吗, 类似地, 我们只用在使能 form login 的地方添加一行代码, to tell the SecurityFilterChain that we also want to be able to login in with oauth2 login,\n1 2 3 4 5 // .... .formLogin(withDefaults()) .oauth2Login(withDefaults()) // 新加的代码 .build(); // ... 第三步就是要在谷歌注册相关信息, 即我们在用谷歌登录, 也算是用了谷歌相关的service, 不能说直接不经过谷歌相关认证就用了吧, 此时我们先把文件 java/resources/application.properties 修改成 application.yaml, 内容为:\n1 2 3 4 5 6 7 8 spring: security: oauth2: client: registration: google: client-id: client-secret: 然后剩下的就是从谷歌获取 client-id 和 client-secret, 具体配置可参考: Setting up OAuth 2.0 - Google Cloud Platform Console Help\n然后点 create, 即可获得 client-id 和 client-secret, 然后填入 application.yaml 中, 记住哦, 提交远程仓库的时候别忘把这个文件加入到 .gitignore 中, 然后运行程序就可以实现谷歌登录了,\nOAuth (short for \u0026ldquo;Open Authorization\u0026rdquo;) is an open standard for access delegation, commonly used as a way for internet users to grant websites or applications access to their information on other websites but without giving them the passwords. OAuth\n另外注意这个服务都是免费的, 不放心的话可以到 https://console.cloud.google.com/welcome/ 查看管理你的 project (看到有的一年订阅费要15k, 这是在自我安慰),\n# 5. 获取登录用户信息 - Authentication Object 这里就要提到一个重要的对象, 即 Authentication Object, 它代表 either the request to login or the result of a successful login request ,\nAuthentication: represents the user. Contains: ﻿﻿Principal: user \u0026ldquo;identity\u0026rdquo; (name, email\u0026hellip;) ﻿﻿GrantedAuthorities: \u0026ldquo;permissions\u0026rdquo; (roles,..) isAuthenticated(): almost always true, 如果不是 true, 那这个 request 基本会被 blocked, 那我们也无法获得这个 Authentication 了, 因为 SecurityFilterChain 的这个代码: authorizeHttp.anyRequest().authenticated(), 即除了那几个 endpoints 的请求不需要认证, 其他的都需要 authenticated, ﻿﻿details: details about the request ﻿﻿(Credentials): \u0026ldquo;password\u0026rdquo;, often null 了解更多关于 Authentication: Servlet Authentication Architecture :: Spring Security\n我们可以通过 Authentication object 来获取用户名密码等信息, 这也是第三个重要的类, 只不过这个类不是我们自定义的, 而是 spring 平台的, 了解更多: Authentication (Spring Security 3.0.8.RELEASE API) Authentication Object: Represents the token for an authentication request or for an authenticated principal once the request has been processed by the AuthenticationManager.authenticate(Authentication) method. Once the request has been authenticated, the Authentication will usually be stored in a thread-local SecurityContext managed by the SecurityContextHolder by the authentication mechanism which is being used. In Spring Security, the term \u0026ldquo;authenticated principal\u0026rdquo; refers to the authenticated user or entity. It represents the currently logged-in user or entity who has successfully completed the authentication process. In most cases, the framework transparently takes care of managing the security context and authentication objects for you. Authentication\n注意, authentication object 就在 SecurityContext, 它是 thread-local, 所以比如你的服务器 tomcat 有100个线程来处理 request, 每个线程内的 SecurityContext都不一样, 而且 SecurityContext global static, 除此之外, when a request comes in, runs on a thread, and when it\u0026rsquo;s done, some other request is going to use this thread, and the SecurityContext will be clean before this,\n所以你可以在任何地方获取它, 即如果有时候你有个 Controller, 然后有四五个方法 nested, 不用像下面那样通过参数 inject authentication object, 太麻烦了, 可以直接获得,\nvar authentication = SecurityContextHolder.getContext().getAuthentication();\nhttps://youtu.be/iJ2muJniikY?t=2085\n看看 authentication 对象的方法:\ngetPrincipal() The identity of the principal being authenticated. In the case of an authentication request with username and password, this would be the username. getCredentials() The credentials that prove the principal is correct. This is usually a password, but could be anything relevant to the AuthenticationManager. 一个Credentials输出: org.springframework.security.core.userdetails.User [Username=david, Password=[PROTECTED], Enabled=true, AccountNonExpired=true, credentialsNonExpired=true, AccountNonLocked=true, Granted Authorities=[ROLE_user]] 修改前面定义的 Controller, inject Authentication object to the /private endpoint,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @RestController public class WebController { @GetMapping(\u0026#34;/\u0026#34;) public String publicPage() { return \u0026#34;Hello David~\u0026#34;; } @GetMapping(\u0026#34;/private\u0026#34;) public String privatePage(Authentication authentication) { return \u0026#34;Hi ~[\u0026#34; + authentication.getName() + \u0026#34;]~, you\u0026#39;ve logged in. 🎉\u0026#34;; } } 然后运行项目, 登录后得到输出:\n1 Hi ~[david]~, you\u0026#39;ve logged in. 🎉 # 6. 查看 OpenID 用户邮箱 修改 Controller 代码, 新加一个自定义方法, getName()\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @RestController public class WebController { @GetMapping(\u0026#34;/\u0026#34;) public String publicPage() { return \u0026#34;Hello David~\u0026#34;; } @GetMapping(\u0026#34;/private\u0026#34;) public String privatePage(Authentication authentication) { return \u0026#34;Hi ~[\u0026#34; + getName(authentication) + \u0026#34;]~, you\u0026#39;ve logged in. 🎉\u0026#34;; } private static String getName(Authentication authentication) { return Optional.of(authentication.getPrincipal()) .filter(OidcUser.class::isInstance) .map(OidcUser.class::cast) .map(OidcUser::getEmail) .orElseGet(authentication::getName); } } 运行后选择谷歌登录, 输出: Hi ~[shehu@gmail.com]~, you\u0026rsquo;ve logged in. 🎉\n# 7. 通过 debug 查看 Authentication Object 设置断点,\n访问 /private 页面输入密码登录, 返回 IDE,\n下面是通过 ouath Google 登录之后的 Authentication 信息,\n# 8. 总结 总结大概从 25:00 左右开始,\n{% youtube iJ2muJniikY %}\n本文主要参考: https://youtu.be/iJ2muJniikY\n","date":"2023-08-04T19:16:49Z","permalink":"https://blog.yorforger.cc/p/spring-security-authentication-spring%E5%AD%A6%E4%B9%A0%E4%BA%94/","title":"Spring Security Authentication, Spring学习(五)"},{"content":" # 实现功能 访问 http://localhost:8080/ 的时候主页列出所有的猫猫信息(name+age), 可通过参数查找某个猫猫信息如 http://localhost:8080/?name=Max, 如果猫猫Max存在将会列出其信息, 不存在则进行提示, 通过实现以上功能展示如何通过controller往html模板插入数据, 形成动态html页面,\n注意此例子中 html 模板就是 index.html, 更准确的说法是, 通过在html的一些标签里添加一些Thymeleaf属性, 然后html就成了html模板,\n源码: https://github.com/shwezhu/springboot-learning/tree/master/thymeleaf-example\n# 项目结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 src ├── java │ └── com │ └── david │ ├── ThymeleafExampleApplication.java │ ├── bean │ │ └── Cat.java │ └── controller │ └── MainController.java └── resources ├── application.properties ├── static │ └── css │ └── main.css └── templates └── index.html # 添加依赖 在pom.xml中加入并reload\n1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-thymeleaf\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 注意, 从上面代码可以看出本例子用的是spring boot, 用spring配置起来会很麻烦,\n# 代码解释 理解了Thymeleaf的逻辑, 之后看着文档自己就能写了, 这里只讲一个, 对于下面代码, 当访问 http://localhost:8080/ 的时候, 得到页面如下:\n首先若不在html文件中使用Thymeleaf的时候, controller代码如下:\n1 2 3 4 5 6 7 8 @RestController public class GreetingController { @GetMapping(\u0026#34;/greeting\u0026#34;) public Greeting greeting(@RequestParam(value = \u0026#34;name\u0026#34;, defaultValue = \u0026#34;World\u0026#34;) String name) { // ... return new Greeting(); // 返回的Greeting会被spring自动转为json对象 } } 该controller是个简单的 restful api, 用来返回简单的json数据, 若不理解此段代码可以到第一个Spring Boot项目及踩坑总结 Spring学习(一) 查看相关注解的解释,\n通过thymeleaf向html插入数据, 此时的controller如下,\n1 2 3 4 5 6 7 8 @Controller public class MainController { @GetMapping(\u0026#34;/\u0026#34;) public String home(Model model) { model.addAttribute(\u0026#34;title\u0026#34;, \u0026#34;hello little kittens\u0026#34;); return \u0026#34;index\u0026#34;; // return index.html } } 对应的index.html代码如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;!DOCTYPE HTML\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34; xmlns:th=\u0026#34;http://www.thymeleaf.org\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Spring Boot Thymeleaf Example\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt; \u0026lt;span th:text=\u0026#34;\u0026#39;Hello, \u0026#39; + ${title}\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 可以注意到controller的方法只是多了一个参数即类 org.springframework.ui.Model 的对象, 然后通过调用其对象的addAttribute()方法向将要返回的html中动态地插入数据, 此例中插入的数据为字符串\u0026quot;hello little kittens\u0026quot;, 往哪插入? 当然是\u0026quot;title\u0026quot; 属性, 然后就可以在index.html预访问\u0026quot;title\u0026quot; 的值, 注意, 这里说是预防问是因为变量title的数据还没到, 需要controller的方法执行到return \u0026quot;index\u0026quot;; 的时候, 数据才算是真的到了 index.html, 数据处理的大致过程如下:\n客户端发送http请求, Spring框架调用controller对应的函数 controller对应方法的语句执行完毕(如查询数据库, 向Model对象添加属性), 函数返回字符串\u0026quot;index\u0026quot;, Spring框架拿到该函数返回的字符串 \u0026quot;index\u0026quot;, Spring将会在项目template文件夹下查找文件 index.html , 并配合thymeleaf之类的东西将 Model 对象的内容插入到index.html里对应的tag里, 如上面的例子就是把\u0026quot;hello little kittens\u0026quot;, 插入到标签 \u0026lt;span th:text=\u0026quot;'Hello, ' + ${title}\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;里, 然后Spring返回最终形成的 index.html 给客户端 大概就是这个过程, 但细节应该不是, 比如插入数据的时候是spring做的还是thymeleaf, 这个感兴趣可以去谷歌, 这里只是叙述大致的处理过程, 以便编写thymeleaf代码有思路,\n所以你懂了吗? controller方法通过返回字符串来指定要返回的html文件, 而我们想要插入到html中的数据则通过调用model.addAttribute()来指定, 这个过程就相当于制造零件, 而零件的组装, 也就是汽车的样子, 就是我们编写的“特殊的”index.html文件, 也就是所谓的模板, 大概就是这么个过程, 希望可以帮助到你,\n其他内容可参考: Introduction to Using Thymeleaf in Spring | Baeldung\n# 总结 总结下, Spring MVC, 到现在学了V和C, C即我们常写的Controller, V即是View, 就是每次 Controller 的method返回的那个view, 在Spring Boot目录结构一般如下,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ├── src │ ├── main │ │ ├── java │ │ │ └── com │ │ │ └── david │ │ │ ├── ThymeleafExampleApplication.java │ │ │ ├── bean │ │ │ └── controller │ │ └── resources │ │ ├── application.properties │ │ ├── static │ │ │ └── css │ │ │ └── main.css │ │ └── templates │ │ └── index.html 在HTML标签插入一些属性就成了Thymeleaf模板也就是项目中的 index.html, 如\u0026lt;span th:text=\u0026quot;${cat.name}\u0026quot; /\u0026gt;, 而controller通过设置Model 对象属性向index.html插入数据, 进而形成内容不同的 index.html 返回给客户端, 像是在搭积木,\n注意上面我们提到的 controller 只是个泛指, 实际上 controller 就是个普通的类, 被注解@Controller, @RestController, 修饰的就是 controller class, 然后该类的method一般被 @GetMapping(\u0026quot;/cat\u0026quot;), @PostMapping(\u0026quot;/cat/add\u0026quot;) 等注解修饰, 来处理不同的http请求, 即真正处理http请求并返回html文件的是 controller class的method,\n","date":"2023-08-01T13:55:48Z","permalink":"https://blog.yorforger.cc/p/thymeleaf-spring%E5%AD%A6%E4%B9%A0%E5%9B%9B/","title":"Thymeleaf, Spring学习(四)"},{"content":" # 1. ORM Object-Relational Mapping (ORM) is a technique that lets you query and manipulate data from a database using an object-oriented paradigm.\nWhen most people say “ORM” they are referring to the library that implements this technique.\nWhichever ORM library you choose, they all use the same principles. There are a lot of ORM libraries around here:\nJava: Hibernate. PHP: Propel or Doctrine (I prefer the last one). Python: the Django ORM or SQLAlchemy (My favorite ORM library ever). C#: NHibernate or Entity Framework 至于Library和Framework, 知道各自的一个例子就行了, 比如JQuery就是Library(就实现了一些功能封装成代码让你用), 而Spring就属于框架, 至于 Spring 到底是什么, 我也刚开始学, 没有搞明白呢,\n注意: JPA 是一套规范，不是一套产品，那么像 Hibernate, TopLink, JDO 他们是一套产品，如果说这些产品实现了这个 Jpa 规范，那么我们就可以叫他们为 Jpa 的实现产品。原文\n# 2. JDBC vs MySQL Connector JDBC stands for Java Database Connectivity: Java JDBC API\nSometimes, I always mistake MySQL driver for JDBC, a little funny, Lol. JDBC is part of JDK, which is java\u0026rsquo;s standard library, like code belowjava.sql.* belongs to Java SE API, which is also called JDBC API.\nSo what is MySQL driver, like com.mysql.cj.jdbc.Driver which we often add dependency to our maven project. Actually com.mysql.cj.jdbc.Driver is MySQL Connector, and we don\u0026rsquo;t use any of its code in our program, we just need to tell JDBC that which driver we use (Class.forName(\u0026quot;com.mysql.cj.jdbc.Driver\u0026quot;); ), and the query and insert statements are all executed by JDBC API (JDBC will commucate with mysql database with MySQL Connector, namely, mysql-jdbc-driver),\nAfter look at these codes below, you will understand,\nDatabase.java:\n1 2 3 4 5 6 7 package database; import java.sql.Connection; import java.sql.SQLException; public interface Database { public Connection connect() throws SQLException; } MysqlDatabase.java:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package database; // java.sql.* 属于 Java SE API, 这就是 JDBC API import java.sql.Connection; import java.sql.DriverManager; import java.sql.SQLException; public class MysqlDatabase implements Database{ private final String url, user, password; public MysqlDatabase(String url, String user, String password) { this.url = url; this.user = user; this.password = password; } @Override public Connection connect() throws SQLException { try { // You must load jdbc driver, otherwise you will get null for connection. Class.forName(\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;); } catch (ClassNotFoundException e) { // Instead of printing error simply, you should do some logging here System.err.println(\u0026#34;Cannot find sql drive: com.mysql.jdbc.Driver\u0026#34;); return null; } return DriverManager.getConnection(this.url, this.user, this.password); } } GetTemperatureServlet.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ... public Connection getConnection() { Database database = new MysqlDatabase(\u0026#34;jdbc:mysql://localhost:3306/greenhouse\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;778899\u0026#34;); Controller controller = new Controller(database); Connection conn = null; try { conn = controller.getConnection(); } catch (SQLException e) { logger.error(e.getMessage()); } return conn; } public JsonArray getData(String sql, HttpServletResponse response) throws IOException { ResultSet resultSet = null; PreparedStatement statement = null; ArrayList\u0026lt;DataEntity\u0026gt; results = new ArrayList\u0026lt;\u0026gt;(); Connection conn = getConnection(); try { statement = conn.prepareStatement(sql); resultSet = statement.executeQuery(); while (resultSet.next()) { results.add(new DataEntity(resultSet.getDouble(1), resultSet.getDate(2).toString())); } } catch (SQLException e) { ... } Gson gson = new Gson(); JsonElement jsonElement = gson.toJsonTree(results, new TypeToken\u0026lt;ArrayList\u0026lt;DataEntity\u0026gt;\u0026gt;(){}.getType()); ... } As code above, ResultSet, PreparedStatement and Connection they are all belong to java.sql.* which is part of java se api, namely, JDBC. So now you know that we don\u0026rsquo;t commucate with MySQL database with MySQL Connector directly, actually, we conmmucate with MySQL(retrive \u0026amp; insert data) with JDBC API(java.sql.*) and JDBC API commucate with MySQL Connector, and MySQL COnnector commucate with MySQL database.\nMySQL :: MySQL Connectors\nOverview (Java Platform SE 8 )\n# 3. JPA vs Hibernate JPA (Java Persistence API) is a standard for ORM (Object Relational Mapping) Hibernate is a JPA provider JDBC (Java Database Connectivity ) API belongs to Java SE API We have known what is JDBC in the paragraph above, let\u0026rsquo;s look at what is JPA.\nJPA is a standard for Object Relational Mapping (ORM). This is a technology which allows you to map between objects in code and database tables. This can hide the SQL from the developer so that all they deal with are Java classes, and the provider allows you to save them and load them magically. Mostly, XML mapping files or annotations on getters and setters can be used to tell the JPA provider which fields on your object map to which fields in the DB. The most famous JPA provider is Hibernate, so it\u0026rsquo;s a good place to start for concrete examples.\n所以JPA只是个Java的ORM标准, 而Hibernate实现了JPA,\nUnder the hood, Hibernate and most other providers for JPA write SQL and use JDBC API to read and write from and to the DB. Simply think, JPA is a Java ORM, and Hibernate implements JPA using JDBC API,\n参考:\njava - JPA or JDBC, how are they different? - Stack Overflow ","date":"2023-07-30T16:55:47Z","permalink":"https://blog.yorforger.cc/p/orm-vs-jpa-vs-hibernate-jdbc-vs-mysql-driver-spring%E5%AD%A6%E4%B9%A0%E4%B8%89/","title":"ORM vs JPA vs Hibernate \u0026 JDBC vs MySQL Driver, Spring学习(三)"},{"content":" # 1. Install Maven 1.Go to Maven Download site\n2.Set Environment Variables\n1 2 3 4 5 6 7 8 tar -xvf apache-maven-3.6.3-bin.tar.gz export M2_HOME=\u0026#34;/Users/shaowen/Downloads/Programs/apache-maven-3.9.1\u0026#34; PATH=\u0026#34;${M2_HOME}/bin:${PATH}\u0026#34; export PATH source .bash_profile mvn -version # 2. Maven Artifact In Maven terminology, the artifact is the resulting output of the maven build, generally a jar or war or other executable file. Artifacts in maven are identified by a coordinate system of groupId, artifactId, and version. Maven uses the groupId, artifactId, and version to identify dependencies (usually other jar files) needed to build and run your code. - https://stackoverflow.com/a/2487510/16317008\nAn artifact is a file, usually a JAR, that gets deployed to a Maven repository.\nA Maven build produces one or more artifacts, such as a compiled JAR and a \u0026ldquo;sources\u0026rdquo; JAR.\nEach artifact has a group ID (usually a reversed domain name, like com.example.foo), an artifact ID (just a name), and a version string. The three together uniquely identify the artifact.\nA project\u0026rsquo;s dependencies are specified as artifacts. - https://stackoverflow.com/a/2487511/16317008\n之前写的关于如何发布 war 到 tomcat 服务器的文章, 用的就是 maven build war, 当然 maven 也可以把我们的项目 build 成 jar 文件, 所以 artifact 在 maven 中的意思就是 maven 的输出, maven 所build的文件, 我们的 spring boot 项目也是一个 artifact, 与其说 spring boot 项目不如说是 maven 项目, 因为如过我们不用 gradle 的话, 那每个spring boot 项目的根目录下都会有一个 pom.xml 文件, 在其开头你可以看到 groupid, artifactid等标签,这不就是标注一个 artifact 的标签吗, 然后pom.xml 中的dependency标签里的依赖就是一个个的 maven artifacts, 这其实就是第三方把她们的maven项目打包成jar供我们使用, 我们只用添加依赖就好, 至于这些依赖(artifacts)下载安装后都保存在了 Maven repository, 具体把maven 项目build 成 war/jar package可参考:\n手动部署War包到Tomcat上之何为War 创建第一个Spring Boot项目及注意事项 # 3. Maven Repository Maven Repository有3种类型:\nLocal Repository Central Repository Remote Repository Maven Local Repository 是本机中的一个目录, 如果目录不存在, 执行maven时就会先创建它, 默认情况下, maven本地存储库是%USER_HOME%/.m2 , 我们可以在maven配置文件查看local repository, 位置在 maven 安装路径下, MAVEN_HOME/conf/settings:\n查看 .m2 的内容,\n1 2 3 4 5 6 7 8 $ ls ~/.m2/repository/ antlr commons-codec junit aopalliance commons-collections log4j logkit\tmysql .... $ ls .m2/repository/log4j/log4j/1.2.12 _remote.repositories log4j-1.2.12.jar.sha1 log4j-1.2.12.pom.sha1 log4j-1.2.12.jar log4j-1.2.12.pom 阅读更多: Maven-Repository存储库 # 4. Artifact \u0026amp; Group 创建好maven项目后, 在项目根目录下有个pom.xml文件, 这个就是maven的配置文件, 你可以在里面添加各种依赖, 比如想用mysql driver, gson, log4j等第三方库的时候, 直接在dependencies标签下添加:\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.32\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 那么问题来了, 上面dependence标签下的artifactId和groupId是什么呢? 仔细观察pom.xml内容你会发现, 在其开头也是这些奇怪的标签:\n1 2 3 4 5 \u0026lt;groupId\u0026gt;com.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ServletDemo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;ServletDemo\u0026lt;/name\u0026gt; \u0026lt;packaging\u0026gt;war\u0026lt;/packaging\u0026gt; 其中，groupId类似于Java的包名，通常是公司或组织名称，artifactId类似于Java的类名，通常是项目名称，再加上version，一个Maven工程就是由groupId，artifactId和version作为唯一标识,\n在Maven项目中，类的classpath路径是由Maven依赖管理机制决定的。Maven通过pom.xml管理项目依赖，当我们使用\u0026lt;dependency\u0026gt;声明一个依赖后，Maven会根据pom.xml文件中的配置将依赖下载到本地仓库，然后将其添加到项目的classpath中。\n# 5. 查看classpath Mac, Linux查看当前maven项目的classpath, 注意要在项目的根目录下输入此命令, 不然会提示错误找不到pom.xml文件,\n1 2 mvn -q exec:exec -Dexec.executable=echo -Dexec.args=\u0026#34;%classpath\u0026#34; /Users/shaowen/Codes/IDEA/ServletDemo/target/classes:/Users/shaowen/.m2/repository/com/mysql/mysql-connector-j/8.0.32/mysql-connector-j-8.0.32.jar:/Users/shaowen/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar 可以看到, classpath路径的第一个是/Users/shaowen/Codes/IDEA/ServletDemo/target/classes/, 这个文件夹下面就是我们project/src/main/java/下的类, 编译成的class文件 都放这下面了. 所以项目就是这么找到我们编写的类的,\n了解更多关于输出classpath: https://stackoverflow.com/a/45043803/16317008\n# 6. Common Maven Commands Maven Command Description mvn \u0026ndash;version Prints out the version of Maven you are running. mvn clean Clears the target directory into which Maven normally builds your project. mvn package Builds the project and packages the resulting JAR file into the target directory. mvn package -Dmaven.test.skip=true Builds the project and packages the resulting JAR file into the target directory - without running the unit tests during the build. mvn clean package Clears the target directory and Builds the project and packages the resulting JAR file into the target directory. mvn clean package -Dmaven.test.skip=true Clears the target directory and builds the project and packages the resulting JAR file into the target directory - without running the unit tests during the build. mvn verify Runs all integration tests found in the project. mvn clean verify Cleans the target directory, and runs all integration tests found in the project. mvn install Builds the project described by your Maven POM file and installs the resulting artifact (JAR) into your local Maven repository mvn install -Dmaven.test.skip=true Builds the project described by your Maven POM file without running unit tests, and installs the resulting artifact (JAR) into your local Maven repository mvn clean install Clears the target directory and builds the project described by your Maven POM file and installs the resulting artifact (JAR) into your local Maven repository mvn clean install -Dmaven.test.skip=true Clears the target directory and builds the project described by your Maven POM file without running unit tests, and installs the resulting artifact (JAR) into your local Maven repository mvn dependency:copy-dependencies Copies dependencies from remote Maven repositories to your local Maven repository. mvn clean dependency:copy-dependencies Cleans project and copies dependencies from remote Maven repositories to your local Maven repository. mvn clean dependency:copy-dependencies package Cleans project, copies dependencies from remote Maven repositories to your local Maven repository and packages your project. mvn dependency:tree Prints out the dependency tree for your project - based on the dependencies configured in the pom.xml file. mvn dependency:tree -Dverbose Prints out the dependency tree for your project - based on the dependencies configured in the pom.xml file. Includes repeated, transitive dependencies. mvn dependency:tree -Dincludes=com.fasterxml.jackson.core Prints out the dependencies from your project which depend on the com.fasterxml.jackson.core artifact. mvn dependency:tree -Dverbose -Dincludes=com.fasterxml.jackson.core Prints out the dependencies from your project which depend on the com.fasterxml.jackson.core artifact. Includes repeated, transitive dependencies. mvn dependency:build-classpath Prints out the classpath needed to run your project (application) based on the dependencies configured in the pom.xml file. 注意(1): mvn install 说的是把你的 maven 项目构建成 jar或者war, 然后放到 .m2/respository/ 下, 而不是下载安装 pom.xml 中的依赖到本地仓库, 具体输出:\n1 2 3 4 5 6 7 8 9 10 11 12 13 [INFO] Building jar: /Users/David/Codes/IDEA/my-app/target/my-app-0.0.1-SNAPSHOT.jar [INFO] [INFO] --- spring-boot:3.1.2:repackage (repackage) @ my-app --- [INFO] Replacing main artifact /Users/David/Codes/IDEA/my-app/target/my-app-0.0.1-SNAPSHOT.jar with repackaged archive, adding nested dependencies in BOOT-INF/. [INFO] [INFO] --- install:3.1.1:install (default-install) @ my-app --- [INFO] Installing /Users/David/Codes/IDEA/my-app/target/my-app-0.0.1-SNAPSHOT.jar to /Users/David/.m2/repository/com/example/my-app/0.0.1-SNAPSHOT/my-app-0.0.1-SNAPSHOT.jar [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 2.483 s [INFO] Finished at: 2023-07-30T13:56:58-03:00 [INFO] ------------------------------------------------------------------------ 注意(2):\nKeep in mind, that when you execute the clean goal of Maven, the target directory is removed, meaning you lose all compiled classes from previous builds. That means, that Maven will have to build all of your project again from scratch, rather than being able to just compile the classes that were changed since last build. This slows your build time down. However, sometimes it can be nice to have a clean, fresh build, e.g. before releasing your product to the world - mostly for your own \u0026ldquo;feeling\u0026rdquo; of knowing everything was built from scratch and working. - Maven Commands\n# 7. Executing Build Life Cycles, Phases and Goals When you run the mvn command you pass one or more arguments to it. These arguments specify either a build life cycle, build phase or build goal. For instance to execute the clean build life cycle you execute this command:\n1 mvn clean To execute the site build life cycle you execute this command:\n1 mvn site # 7.1. Executing the Default Life Cycle The default life cycle is the build life cycle which generates, compiles, packages etc. your source code.\nYou cannot execute the default build life cycle directly, as is possible with the clean and site. Instead you have to execute a specific build phase within the default build life cycle.\nThe most commonly used build phases in the default build life cycle are:\nBuild Phase Description validate Validates that the project is correct and all necessary information is available. This also makes sure the dependencies are downloaded. compile Compiles the source code of the project. test Runs the tests against the compiled source code using a suitable unit testing framework. These tests should not require the code be packaged or deployed. package Packs the compiled code in its distributable format, such as a JAR. install Install the package into the local repository, for use as a dependency in other projects locally. deploy Copies the final package to the remote repository for sharing with other developers and projects. Executing one of these build phases is done by simply adding the build phase after the mvn command, like this:\n1 mvn compile This example Maven command executes the compile build phase of the default build life cycle. This Maven command also executes all earlier build phases in the default build life cycle, meaning the validate build phase.\n# 7.2. Executing Build Phases You can execute a build phase located inside a build life cycle by passing the name of the build phase to the Maven command. Here are a few build phase command examples:\n1 2 3 mvn pre-clean mvn compile mvn package Maven will find out what build life cycle the specified build phase belongs to, so you don\u0026rsquo;t need to explicitly specify which build life cyle the build phase belongs to.\n本节来自: Maven Commands\n","date":"2023-07-30T14:11:30Z","permalink":"https://blog.yorforger.cc/p/maven%E7%9A%84%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4-spring%E5%AD%A6%E4%B9%A0%E4%BA%8C/","title":"Maven的相关概念与常用指令 Spring学习(二)"},{"content":" # 错误总结 (1) 所有Java代码都要在java目录下, 即不可删除java目录, 否则, 运行./mvnw clean package报错, 你可以自己试试\n1 2 3 4 5 6 7 8 9 ├── pom.xml ├── src │ ├── main │ │ ├── java │ │ │ └── com │ │ │ ├── MyAppApplication.java │ │ │ └── restservice │ │ │ ├── Greeting.java │ │ │ └── GreetingController.java (2) controller 等组件所在文件夹需要在主类的同级或者次级目录, 否则运行程序访问http://localhost:8080/greeting会出现错误:\n1 2 3 Whitelabel Error Page This application has no explicit mapping for /error, so you are seeing this as a fallback. Tue Jun 30 17:24:02 CST 2023 There was an unexpected error (type=Not Found, status=404). No message available 主类就是被@SpringBootApplication修饰的那个类,\n正确结构:\n1 2 3 4 5 6 7 8 9 10 11 12 ├── java │ └── com │ ├── MyAppApplication.java │ └── restservice │ ├── Greeting.java │ └── GreetingController.java # or ├── java │ └── com │ ├── MyAppApplication.java │ ├── Greeting.java │ └── GreetingController.java 错误结构:\n1 2 3 4 5 6 ├── java │ ├── com │ │ └── MyAppApplication.java │ └── restservice │ ├── Greeting.java │ └── GreetingController.java 具体可参考: spring - This application has no explicit mapping for /error - Stack Overflow\n# 新概念 Record Class 参照 Greeting.java 源码发现 Greeting 与传统的类不同, 没有构造函数, 更像是个函数,\nOne of the main advantages of records over POJOs is that they reduce the amount of boilerplate code needed to define a class. For example, a POJO might require a constructor, getters, setters, equals, hashCode, and toString methods, all of which must be manually defined. With records, all of these methods are automatically generated based on the record’s fields. - Java Records vs POJO\nPOJO Class: What Is a Pojo Class? | Baeldung\n# Jackson 可能会好奇, 为什么没有用 json 库, 直接返回是一个Greeting对象,但得到的内容却是 json 格式的信息呢? 教程里给的解释:\nThis application uses the Jackson JSON library to automatically marshal instances of type Greeting into JSON. Jackson is included by default by the web starter.\n原来是 spring boot 集成了 Jackson, 代码里没有用相关的代码是因为Spring Boot 用了 ObjectMapper ,\nWhen using JSON format, Spring Boot will use an ObjectMapper instance to serialize responses and deserialize requests. Spring Boot: Jackson ObjectMapper\n# 源码解释 项目创建过程可参考: Getting Started | Building a RESTful Web Service\n1 2 3 4 5 6 7 8 9 10 11 12 // MyApplicaiton.java package com; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class MyAppApplication { public static void main(String[] args) { SpringApplication.run(MyAppApplication.class, args); } } 1 2 3 // Greeting.java package com.restservice; public record Greeting(long id, String content) { } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // GreetingController.java package com.restservice; import java.util.concurrent.atomic.AtomicLong; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; // 此类被@RestController修饰, 当其所有methods返回一个类的对象时, spring将自动处理被返回的对象, 转为json格式返回 // @RestController = @Controller + @ResponseBody @RestController public class GreetingController { private static final String template = \u0026#34;Hello, %s!\u0026#34;; private final AtomicLong counter = new AtomicLong(); @GetMapping(\u0026#34;/greeting\u0026#34;) public Greeting greeting(@RequestParam(value = \u0026#34;name\u0026#34;, defaultValue = \u0026#34;World\u0026#34;) String name) { // 返回的Greeting会被spring自动转为json对象 return new Greeting(counter.incrementAndGet(), String.format(template, name)); } } The @GetMapping annotation ensures that HTTP GET requests to /greeting are mapped to the greeting() method. There are companion annotations for other HTTP verbs (e.g. @PostMapping for POST). There is also a @RequestMapping annotation that they all derive from, and can serve as a synonym (e.g. @RequestMapping(method=GET)). @RequestParam binds the value of the query string parameter name into the name parameter of the greeting() method. If the name parameter is absent in the request, the defaultValue of World is used. This code uses Spring @RestController annotation, which marks the class as a controller where every method returns a domain object instead of a view. @RestController = @Controller + @ResponseBody. @Controller is used to declare common web controllers which can return HTTP response but @RestController is used to create controllers for REST APIs which can return JSON. The Greeting object must be converted to JSON. Thanks to Spring’s HTTP message converter support, you need not do this conversion manually. Because Jackson 2 is on the classpath, Spring’s MappingJackson2HttpMessageConverter is automatically chosen to convert the Greeting instance to JSON. @SpringBootApplication is a convenience annotation that adds all of the following: @Configuration: Tags the class as a source of bean definitions for the application context. @EnableAutoConfiguration: Tells Spring Boot to start adding beans based on classpath settings, other beans, and various property settings. For example, if spring-webmvc is on the classpath, this annotation flags the application as a web application and activates key behaviors, such as setting up a DispatcherServlet. @ComponentScan: Tells Spring to look for other components, configurations, and services in the com/example package, letting it find the controllers. 最后一个没怎么看明白, 也不打算现在弄明白, 想着随着学习的深入, 慢慢的对 spring 有个更清晰的认识的时候, 就慢慢懂了,\n上面关于 @RestController 的解释: \u0026hellip; every method returns a domain object instead of a view, 这里的view指的是返回html文件, 也就是说若你想提供处理restful api然后返回简单的json数据服务, 那就用 @RestController 修饰该 controller, 然后让该controller的每个方法都返回一个对象, 剩下的交给 spring 框架, 它会自动帮你把返回的对象转为json对象然后返回给客户端,\nIf you use Maven, you can run the application by using ./mvnw spring-boot:run. Alternatively, you can build the JAR file with ./mvnw clean package and then run the JAR file, as follows:\n1 java -jar target/gs-rest-service-0.1.0.jar Now that the service is up, visit http://localhost:8080/greeting\n","date":"2023-07-29T16:47:46Z","permalink":"https://blog.yorforger.cc/p/%E5%88%9B%E5%BB%BAspring-boot%E9%A1%B9%E7%9B%AE%E5%8F%8A%E8%B8%A9%E5%9D%91%E6%80%BB%E7%BB%93-spring%E5%AD%A6%E4%B9%A0%E4%B8%80/","title":"创建Spring Boot项目及踩坑总结 Spring学习(一)"},{"content":" # Java Packages \u0026amp; API A package is a namespace that organizes a set of related classes and interfaces. Think of it as a folder in a file directory. We use packages to avoid name conflicts, and to write a better maintainable code. Packages are divided into two categories:\nBuilt-in Packages (packages from the Java API) User-defined Packages (create your own packages) # Built-in Packages The Java API is a library of prewritten classes, that are free to use, included in the Java Development Environment. The library contains components for managing input, database programming, and much much more. The complete list can be found at Oracles website: https://docs.oracle.com/javase/8/docs/api/.\nThe library (Java API) is divided into packages and classes. Meaning you can either import a single class (along with its methods and attributes), or a whole package that contain all the classes that belong to the specified package.\nJava API 就是所谓的 Built-in Packages咯, 直观感受下\n# User-defined Packages To create your own package, you need to understand that Java uses a file system directory to store them. Just like folders on your computer:\n1 2 3 └── src └── mypack └── Cat.java To create a package, use the package keyword:\n1 2 3 4 package mypack; class Cat { ... } 注意哦, package 应该在 src 目录下, 然后含有 main 函数的主类也应该在 src 下, 不然在导入package时是没办法找到 package的,\n# e.g., 注意看目录结构,\n注意看第一行, 声明所在的package\n若无声明\n最后注意, 真正的完整类名是包名.类名, JVM 只看完整类名, JVM 认为只要包名不同, 类就不同,\n另外package没有父子关系, java.util和java.util.zip是不同的包, 两者没有任何继承关系,\n# Import a Package To import a whole package, end the sentence with an asterisk sign (*). The following example will import ALL the classes in the java.util package:\n1 import java.util.*; # Import a Class If you find a class you want to use, for example, the Scanner class, which is used to get user input, write the following code:\n1 import java.util.Scanner; In the example above, java.util is a package, while Scanner is a class of the java.util package.\n参考:\nWhat Is a Package? (The Java™ Tutorials \u0026gt; Learning the Java Language \u0026gt; Object-Oriented Programming Concepts) Java Packages ","date":"2023-07-27T20:11:43Z","permalink":"https://blog.yorforger.cc/p/java-packages%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8/","title":"Java Packages介绍及使用"},{"content":"我的电脑有两个jdk, 一个是我自己下载的jdk17, 一个是电脑预安装的jdk19:\n1 2 3 4 5 ls /Library/Java/JavaVirtualMachines/jdk-19.jdk/Contents/Home/ LICENSE bin include legal man README conf jmods lib release ls ~/Downloads/Programs/jdk-17-0-3-1/Home/ LICENSE README bin conf include jmods legal lib release 都说JDK包括JRE, 然后JRE里面有JVM, 但是现在新版本的JDK里没有JRE文件夹了, JRE被单独安装了, 那么现在对于新版本的JDK来说, JRE是不是仍然属于JDK呢? 知道这个问题和现实就行了, 至于属不属于无所谓, 想怎么说就怎么说呗, 关键是我们得知道, 什么是JVM, 什么是JDK才是重要的.\nIn macOS, the JDK installation path is /Library/Java/JavaVirtualMachines/jdk-10.jdk/Contents/Home/. In macOS, the JRE installation path is /Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/. 但是我去找jre却没找到, 官方说 When you install JDK 10, the public JRE (Release 10) also gets installed automatically. 他说的这种安装是下载dmg文件, 而我下的是免安装版本的zip包, 解压出来就用了, 所以ummmm, 具体也不清楚以后再想吧\u0026hellip;\n","date":"2023-07-26T21:49:41Z","permalink":"https://blog.yorforger.cc/p/jdk-jre/","title":"JDK \u0026 JRE"},{"content":" # java \u0026amp; javac 两个指令\nTechnically, javac is the program that translates Java code into bytecode (.class file) - an intermediate format which is understandable by the Java Virtual Machine (JVM).\nAnd java is the program that starts the JVM, which in turn, loads the .class file, verifies the bytecode and executes it.\n-classpath classpath, or -cp classpath, specifies a list of directories, JAR files, and ZIP archives to search for class files.\nOn Windows, semicolons (;) separate entities in this list; on other platforms it is a colon (:).\nSpecifying classpath overrides any setting of the CLASSPATH environment variable. If the class path option isn\u0026rsquo;t used and classpath isn\u0026rsquo;t set, then the user class path consists of the current directory (.).\n注意上面说的, java 启动 JVM, 然后 JVM 导入字节码文件, 别理解错了, 至于 -cp 就是用来告诉 JVM 去哪搜索用程序中到的类的字节码文件.calss,\n# 手动编译并运行java程序 目录结构:\n1 2 3 4 5 ├── myproject │ └── src │ ├── Main.java │ └── animal │ └── Cat.java 代码内容:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Cat.java package animal; public class Cat { String name; public Cat(String name) { this.name = name; System.out.println(\u0026#34;mew~\u0026#34;); } } // Main.java import animal.Cat; public class Main { public static void main(String []args){ Cat cat = new Cat(\u0026#34;kitty\u0026#34;); } } 在src下编译:\n1 javac Main.java 编译后多出了两个字节码文件, 如下:\n1 2 3 4 5 6 7 ├── myproject │ └── src │ ├── Main.class │ ├── Main.java │ └── animal │ ├── Cat.class │ └── Cat.java 可以发现, 我们只是编译了Main.java, 被其用到的类 Cat.java 也被编译了, 然后在其它文件夹下执行该程序, 用 -cp 来指明 classpath, 即告诉 JVM 去哪找 user-defined class 字节码文件,\n1 2 $ java -cp myproject/src Main mew~ # How JVM Finds Classes 这个标题本来是\u0026quot;How the Java Launcher Finds Classes\u0026quot;,很具有迷惑性, 因为加载类的是 JVM, 下面也指出了是 JVM 加载类, 所以直接改了标题, 原文来自oracle,\nThe Java launcher, java, initiates the Java virtual machine. The virtual machine searches for and loads classes in this order:\nBootstrap classes - Classes that comprise the Java platform, including the classes in rt.jar and several other important jar files.\nExtension classes - Classes that use the Java Extension mechanism. These are bundled as .jar files located in the extensions directory.\nUser classes - Classes defined by developers and third parties that do not take advantage of the extension mechanism. You identify the location of these classes using the -classpath option on the command line (the preferred method) or by using the CLASSPATH environment variable.\nIn general, you only have to specify the location of user classes. Bootstrap classes and extension classes are found \u0026ldquo;automatically\u0026rdquo;.\n注意这里说的其实就是类加载的过程, 即 JVM 怎么查找并加载类对应的每个字节码文件, 而不是 java launcher, 它只是负责启动 JVM 的, -cp 的作用在这再次被指出, 可以参考上面的例子\n# How JVM Finds Bootstrap Classes Bootstrap classes are the classes that implement the Java 2 Platform. Bootstrap classes are in the rt.jar and several other jar files in the jre/lib directory. These archives are specified by the value of the bootstrap class path which is stored in the sun.boot.class.path system property. This system property is for reference only, and should not be directly modified.\n# How JVM Finds Extension Classes Extension classes are classes which extend the Java platform. Every .jar file in the extension directory, jre/lib/ext, is assumed to be an extension and is loaded using the Java Extension Framework. Loose class files in the extension directory will not be found. They must be contained in a .jar file (or .zip file). There is no option provided for changing the location of the extension directory.\nIf the jre/lib/ext directory contains multiple .jar files, and those files contain classes with the same name, such as:\n1 smart-extension1_0.jar contains class smart.extension.Smartsmart-extension1_1.jar contains class smart.extension.Smart the class that actually gets loaded is undefined.\n# How JVM Finds User Classes User classes are classes which build on the Java platform. To find user classes, the launcher refers to the user class path \u0026ndash; a list of directories, JAR archives, and ZIP archives which contain class files.\nA class file has a subpath name that reflects the class\u0026rsquo;s fully-qualified name. For example, if the class com.mypackage.MyClass is stored under /myclasses, then /myclasses must be in the user class path and the full path to the class file must be /myclasses/com/mypackage/MyClass.class. If the class is stored in an archive named myclasses.jar, then myclasses.jar must be in the user class path, and the class file must be stored in the archive as com/mypackage/MyClass.class.\nThe user class path is specified as a string, with a colon (:) separating the class path entries on Solaris, and a semi-colon (;) separating entries on Microsoft Windows systems. The java launcher puts the user class path string in the java.class.path system property. The possible sources of this value are:\nThe default value, \u0026ldquo;.\u0026rdquo;, meaning that user class files are all the class files in the current directory (or under it, if in a package). The value of the CLASSPATH environment variable, which overrides the default value. The value of the -cp or -classpath command line option, which overrides both the default value and the CLASSPATH value. The JAR archive specified by the -jar option, which overrides all other values. If this option is used, all user classes must come from the specified archive. 原文:\nHow Classes are Found ","date":"2023-07-26T17:52:40Z","permalink":"https://blog.yorforger.cc/p/%E6%89%8B%E5%8A%A8%E7%BC%96%E8%AF%91%E8%BF%90%E8%A1%8Cjava%E7%A8%8B%E5%BA%8F%E4%B9%8Bjvm%E5%8A%A0%E8%BD%BD%E7%B1%BB%E7%9A%84%E9%A1%BA%E5%BA%8F/","title":"手动编译运行Java程序之JVM加载类的顺序"},{"content":"These problems were at the forefront of the minds of HTTP/2 developers,\nThat said, unless you are implementing a web server (or a custom client) by working with raw TCP sockets, then you won’t see any difference: all the new, low-level framing is performed by the client and server on your behalf.\nIf you are unable to attend a meeting, you might ask someone to attend \u0026ldquo;on your behalf,\u0026rdquo; meaning that they will participate in the meeting and represent your interests and opinions in your absence.\nIn fact, there are some developers who advocate for building extremely fine‑grained 10–100 LOC services.\nNginx wrote a fantastic series on the various concepts of microservices, please give this a read.\nWhen the topic of high-performance, in-memory caching surfaces, Redis invariably stands out. Renowned for its speed, flexibility, and rich feature set, Redis has cemented its position as a top-tier caching solution. For Go developers keen on optimizing application performance, understanding Redis becomes pivotal.\nGo’s concurrency features, paired with Redis’s in-memory data structure store, allows developers to implement robust caching mechanisms, elevating the Go-Redis performance to new heights.\nDon\u0026rsquo;t sweat your english, I wouldn\u0026rsquo;t have pegged you for non-native if you hadn\u0026rsquo;t said anything ;)\nThus the performance increase is substantial.\nDuring today’s press conference in which Hollywood actors confirmed that they were going on strike, Duncan Crabtree-Ireland, SAG-AFTRA’s chief negotiator, revealed a proposal from Hollywood studios that sounds ripped right out of a Black Mirror episode.\nA groundbreaking AI proposal that \u0026hellip;.\nSAG strike live updates: Hollywood actors join LA and NYC picket lines as union begins first day of strike. 这里 picket lines 其实就是一行抗议的人在街上, 像线一样, 加入 picket lines 就是加入抗议\nLOS ANGELES, July 14 (Reuters) - Striking Hollywood actors joined film and television writers on picket lines on Friday, the first day of a dual work stoppage that has forced U.S. studios to shutter productions as workers battle over pay in the streaming TV era.\nLOS ANGELES: 洛杉矶 Stoppage: 罢工 Global temperatures were soaring to historic highs as the world\u0026rsquo;s two biggest carbon emitters, the United States and China, sought on Monday to reignite talks on climate change.\nsought: seek With scientists saying the target of keeping global warming within 1.5 degrees Celsius of pre-industrial levels is moving beyond reach, evidence of the crisis was everywhere.\nProlonged high temperatures in China are threatening power grids and crops and raising concerns about a repeat of last year\u0026rsquo;s drought, the most severe in 60 years.\nProlonged: Something prolonged is long and drawn-out — it\u0026rsquo;s taking longer than it should. A prolonged wait is usually annoying. Moscow halts grain deal after bridge to Crimea struck\n\u0026hellip;., just hours after a blast knocked out Russia\u0026rsquo;s bridge to Crimea in what Moscow called a strike by Ukrainian sea drones.\nstrike: 侵袭, 罢工 Tesla’s revenue soars amid rampant price cutting\nrampant price cutting If you do that, you\u0026rsquo;re essentially saying, \u0026ldquo;Air travel in the United Stated is a weak link problem.\u0026rdquo; We\u0026rsquo;re limited by how good are appalling New York airports are more than by how good are best airports are.\n","date":"2023-07-14T18:50:37Z","permalink":"https://blog.yorforger.cc/p/%E8%8B%B1%E8%AF%AD%E9%98%85%E8%AF%BB%E4%B8%80/","title":"英语阅读(一)"},{"content":"写在前面, 每个人的学习方法都不一样, 我的并不一定适合你, 你要自己通过刷题总结出来适合自己的高效方法, 知乎, 谷歌上有很多相关的文章, 要学会自己查资料, 不懂得要学会自己查, 这也是最重要的能力, 比如雅思备考方法, 雅思听力技巧, 然后网上也有很多论坛讨论怎么学习雅思, 比如微博上, 有雅思话题, 下面的评论就会有相关的讨论, 你通过自己的查找慢慢的去丰富自己的知识提高自己这才是最重要的\n不止是雅思, 其它问题也是一样, 刚高中毕业的时候遇到事情我也不知道具体怎么做, 第一件事想的是问别人, 那别人怎么知道的呢? 都是通过自己在网上查资料 查百度 查知乎 逛论坛 百度贴吧 这种 慢慢的知道的, 要学会利用互联网, 手机不止是用来打游戏和聊天看小视频的工具,\n# 听力 备考\n默写雅思王上的单词 练习\n雅思真题上的听力题 前两个月可 一天听1~2篇 根据自己情况 听力可以在 每日英语听力 软件上找到对应的录音 先不看 script 直接听 做一遍 然后若听不懂 再听一遍(根据自己情况) 然后进行精听 即 看着 script 一句一句的听 听不懂的 看看script 看看哪个单词没听懂, 是因为连读没听懂还是因为不认识那个单词? 这个过程就要吧不认识的单词发音 以及 特殊的连读 给记下来, 这也是最重要的一步 然后再不看 script 听一遍 直到不看 script 也可以完全听懂 可以在油管上找雅思(IELTS) 相关的英语主播 看他们的视频 练习听力 上面提到了连读, 轻听英语 软件上 有个节目叫 瞬间秒杀英语听力 可以每天听一节 然后记下来 慢慢的练习, 很好的一个节目\n# 口语 备考\n在油管上找一些雅思相关的外国人开的频道 看他们的视频 教你连读什么的(不要看中文老师教的) ETJ English (这个是英音 发音很好听, 你要是喜欢美音 也可以找美音相关的主播) English Speaking Success (没事的时候多看看这个频道的视频 涨知识 也可以练习听力了 你也可以找自己喜欢的主播) 练习\n背题库 背到自己感觉差不多后 (想象着对面是外国人这么问你 你怎么回答), 要回答的随意一点, 别那么紧张, 就像是正常说话, 因为这个考试其实测试的就是你的语言能力, 没有所谓的正确错误答案, 你只要表达出你的观点, 就像是在和朋友聊天, 越自然越好 口语题库网上有很多, 可以自己搜, 每三四个月题库就会更新一次, 这里给你一个: 2023年雅思口语题库5月-8月完整版（含答案）汇总_雅思_新东方在线\n# 阅读 至于阅读没啥好说的, 但也有做题技巧, 比如一般阅读材料会很长, 考试的时候不会全部读完, 一般的做法是先看问题, 然后迅速在原文中定位问题的位置, 阅读对应的位置, 但是你练习的时候肯定要每个句子都读, 遇到不会的单词都要记下来, 做错的题可以在网上找对应的解析, 看看自己为啥错了, 想想出题人的思路, 然后下次遇到类似问题要怎么做, 反思是最重要的,\n关于题的分析, 你可以下载个 雅思哥 , 上面有阅读对应的答案解析, 当然你也可以去查查其它的在网上, 学会利用网络查东西\u0026hellip;\n如: 雅思阅读部分如何提高？ - 知乎 这下面的回答可以多看几个, 有时候并不是点赞最多的写的就是最好的\n既然有了喜欢的东西就要努力去追, 三分钟热度, 最后只能是什么都做不到 sow nothing, reap nothing.\n加油!\n","date":"2023-07-02T20:40:25Z","permalink":"https://blog.yorforger.cc/p/%E5%A4%87%E8%80%83%E9%9B%85%E6%80%9D/","title":"备考雅思"},{"content":"使用 npm install xxx, 然后总是安装到 ~/node_modules/, 查了好多, 有的说用 npm config list 查看 global 的值, 改为 false 什么, 都没有用,\n卸载 node 和 npm 重装依旧不能解决,\n最后删除了 ~/node_modules/ 文件夹和 ~/ 下的所有 npm 相关文件夹, 然后进入项目文件夹, 重新运行 npm install xxx 后本地文件夹自动创建 node_modules, 安装成功.\n1 2 3 4 5 6 7 8 9 $ mkdir jslearn \u0026amp;\u0026amp; cd jslearn $ npm install cors added 3 packages in 466ms $ ls node_modules package-lock.json package.json # 你看不用 npm init 在知乎看到的回答:\nnpm的原理大概就是从当前目录往上找，找到哪个目录有node_modules就认为这才是真正的项目目录，所以东西全给装那里面去, 所以不仅仅是User根目录的问题，你得保证从你当前的目录开始一直到根目录都没有node_modules，npm才会“正常”地把东西放到当前目录下的node_modules里 原文\n另外:\n-S is shorthand for --save, and it adds the package you\u0026rsquo;re installing to the dependencies in your package.json file (which can be created with npm init). However, --save or -S is totally unnecessary if you\u0026rsquo;re using npm 5 or above since it\u0026rsquo;s done by default. https://stackoverflow.com/a/58475949/16317008\n","date":"2023-06-27T11:24:35Z","permalink":"https://blog.yorforger.cc/p/npm%E6%80%BB%E6%98%AF%E5%AE%89%E8%A3%85package%E5%88%B0%E5%85%A8%E5%B1%80%E9%97%AE%E9%A2%98-%E5%B7%B2%E8%A7%A3%E5%86%B3/","title":"npm总是安装package到全局问题 (已解决)"},{"content":" # 1. Objects are values in Javascript {} 就是个空对象, {} 在 Python 里是个 dictionary, 当然 dictionary 也是个对象,\n在说继承之前还要提一下 JS 对象与其它语言不同的地方, 举个例子,\n1 2 3 4 5 6 7 8 9 10 const student = { name: \u0026#39;John\u0026#39;, study: function () { return \u0026#39;study\u0026#39; } } console.log(student) // { name: \u0026#39;John\u0026#39;, study: [Function: study] } student.age = 99 // 这里没有出错竟然! console.log(student) // { name: \u0026#39;John\u0026#39;, study: [Function: study], age: 99 } 在其它面向对象语言里, 如果我们尝试赋值或者访问对象不存在的 field就会报错, 在 JS 里并不会, 当你尝试读取不存在的 feild 的时候 (包括 prototype chain, 下面会讨论) 才会报错,\n原因是, JS 的对象与其它语言如 Java, Python不同, Java里对象的 fields 是有类决定的, 类似所有对象的蓝图, JS并不是, JS里的对象不是由类创建的, 更像一个 map,\n我们知道 JS 的值分为两类, Primitive values: immutable, object values: mutable, 这里的 mutable 其实就是指 你可以随意给对象增加属性, 一个对象就是一个value,\n当尝试读取对象的属性时, 会触发 prototype chain, 看下面代码:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 const person = { name: \u0026#39;Jack\u0026#39;, sayHello: function () { console.log(\u0026#39;hello from \u0026#39; + this.name) }, } // 注意: 通过对象属性 __proto__ 访问对象 prototype const student = { age: 13, __proto__: person } console.log(student.name) // student没有name属性, 通过prototype chain访问其他对象的name属性 console.log(student) // { age: 13 } student.name = \u0026#39;John\u0026#39; // 为 student 创建新属性 name console.log(student.name) // student 此时已有name属性 直接访问其name属性 console.log(student) // { age: 13, name: \u0026#39;John\u0026#39; } student.sayHello() // 因student无sayHello函数, 通过prototype chain查找sayHello函数, 找到后, sayHello中的this被替换为student # 2. Prototype # 2.1. Ways to access prototype Every object value in JavaScript has a built-in property, which is called its prototype. The prototype is itself an object value, so the prototype will have its own prototype, making what\u0026rsquo;s called a prototype chain. The chain ends when we reach a prototype that has null for its own prototype.\n关于prototype有一个迷惑的地方, 通过obj.prototype访问到的并不是上面我们讨论的prototype, 只能通过obj.__proto__和全局函数Object.getPrototypeOf()来获取一个对象的prototype属性:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 const person = { name: \u0026#39;Jack\u0026#39;, greet: function () { return \u0026#39;Hi, I am\u0026#39; + this.name; } } console.log(person.prototype) console.log(person.__proto__) console.log(Object.getPrototypeOf(person)) // undefined // [Object: null prototype] {} // [Object: null prototype] {} 根据打印可以看出 person.prototype跟person.__proto__压根就不是一个东西,\n# 2.2. Prototype 的获取与赋值 obj.__proto__ 不是访问对象 prototype 属性的标准方法, 因为有的browser可能没实现, 所以访问(只读)一般用Object.getPrototypeOf(student), 通过创建该对象的时候为其 __proto__ 属性赋值的方法给一个对象的 prototype 赋值, 如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 const person = { name: \u0026#39;Jack\u0026#39;, sayHello: function () { console.log(\u0026#39;hello from \u0026#39; + this.name) }, } const student = { age: 13, // Check the assignment here: __proto__: person } console.log(Object.getPrototypeOf(student)) // { name: \u0026#39;Jack\u0026#39;, sayHello: [Function: sayHello] } // 或者创建对象后赋值, student.__proto__ = person // 注意若student没有__proto__属性则这里自动为student创建__proto__并赋值, 赋值后我们就可以访问了 // 所以这和上面我们说的不要通过 `obj.__proto__` 访问一个对象的 prototype 并不矛盾 # 3. Prototype Chain 的意义 Javascript is an object-based language, not a class-based language.\nIf you know Java or C++, you should be familiar with the inheritance concept. In this programming paradigm, a class is a blueprint for creating objects. If you want a new class to reuse the functionality of an existing class, you can create a new class that extends the existing class. This is called classical inheritance.\nJavaScript doesn’t use classical inheritance. Instead, it uses prototypal inheritance. In prototypal inheritance, an object “inherits” properties from another object via the prototype linkage.\nPrototype chain 的意义正是代替了传统语言中的继承机制, 只不过是对象之间的继承, 而不是类之间,\n# 4. Javascript 不存在真正的类 JS 不存在真正的类, 即使是ES6之后的类也不过是个语法糖 (syntactic sugar) 而已,\n1 2 3 4 5 6 7 8 9 10 class Person { constructor(name) { this.name = name; } getName() { return this.name; } } console.log(typeof Person); //function Unlike other programming languages such as Java and C#, JavaScript classes are syntactic sugar over the prototypal inheritance. In other words, ES6 classes are just special functions. But class declarations are not hoisted like function declarations.\n上面提到 obj.prototype 和 obj.__proto__ 不是一个东西, 当创建对象后 (除函数对象) 它们的 obj.prototype 是 undefined, 而创建函数对象后, 这个函数的func.prototype会被自动创建, 并且值为一个空对象{}:\n1 2 3 4 5 6 7 8 9 10 const func = function () { this.name = \u0026#39;John\u0026#39;; } const cap = { size: 36, price: \u0026#39;30CAD\u0026#39;, } console.log(func.prototype); //{} console.log(cap.prototype); //undefined 思考下面代码发生了什么:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function Person(name) { this.name = name; } Person.prototype.getName = function () { return this.name; }; console.log(Person.prototype); // { getName: [Function (anonymous)] } const john = new Person(\u0026#34;John Doe\u0026#34;); console.log(Object.getPrototypeOf(john)); // { getName: [Function (anonymous)] } console.log(john); // Person { name: \u0026#39;John Doe\u0026#39; } console.log(john.getName()); // John Doe 第一步发生在定义函数Person时:\nPerson.prototype 被创建, 并被赋值为一个空对象{}\nPerson.prototype 对象增加一个函数 getName()\n此时 Person.prototype 不再是个空对象, 而是有一个名为getName函数的对象{ getName: [Function (anonymous)] }\n之后三步发生在调用new Person(\u0026quot;John Doe\u0026quot;)时:\n新创建空对象{}并赋值给john 执行john.__proto__ = Person.prototype 执行函数Person, 函数体里的 this.name = name变为 john.name = name 综上: The john object is an instance of the Person through prototypal inheritance.\n参考:\nhttps://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/Object_prototypes https://stackoverflow.com/a/34948211/16317008 https://www.javascripttutorial.net/es6/javascript-class/ ","date":"2023-06-21T11:08:32Z","permalink":"https://blog.yorforger.cc/p/js-%E7%BB%A7%E6%89%BF%E4%B9%8B-prototype-chain-%E7%9A%84%E6%84%8F%E4%B9%89/","title":"JS 继承之 Prototype Chain 的意义"},{"content":" # 1. Javascript objects 在其他语言, 如C++,Java, 对象是一个类的实例(instance), 对象必须由对应类的constructor生成.\n可是在JS里面, 类的概念变得模糊了, 我很少用到 new, 但又有人说 “JavaScript is designed on a simple object-based paradigm”, I‘m confused now.\n什么是variable, type, type 和 object 的关系是什么? 什么又是value? 我尝试在迷惑中找出答案:\nJavascript is dynamic language, which means values have types, not variables.\nValues can have two main types: primitives (immutable), object.\n所以 object 是 data types 中的一种, 显然这和其他语言不一样, 那 object type 具体是什么呢?\nAn object is a collection of properties, and a property is an association between a name (or key) and a value. A cup is an object, with properties. A cup has a color, a design, weight, a material it is made of, etc. The same way, JavaScript objects can have properties, which define their characteristics.\n杯子有颜色, 容量等属性, 颜色比如蓝色, 容量1000ml, 蓝色, 1000ml就是所谓的 value, 这个 value 有 type (type 可以是 object 或 primitives), 前者蓝色是string, 后者1000是number类型. color=\u0026quot;bule\u0026quot; 这就是对象的一个属性. 突然感觉JS里的对象更像是其他语言里的类, 只是js的对象没有构造函数, 不能实例化.\n所以现在搞清楚了什么是 object type, 那就看看另外的一个 primitives type,\n# 2. Data types in JS 类型可以分为两大类 primitive values and objects (Java 的是 primitive types 和 reference type)\n1. Primitive values (immutable datum represented directly at the lowest level of the language)\nBoolean type Null type Undefined type Number type: 比如上面例子中, 杯子的property: 容量的value: 1000, 的type就是 Number type String type: 比如上面例子中, 杯子的property: 颜色的value: blue, 的type就是 String type 2. Objects (collections of properties)\n说白了 JS 中的对象就是个值, 只不过这个值的类型是对象, 到这也算是弄清了其中的关系, object, value, type, variable. 这一切从杯子开始举例, 便可以捋清关系. 变量有类型吗? 严格来说没有type, 是变量的值才有类型. 对象和变量的关系呢? 变量的值的类型可以是对象.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 let person = { firstName:\u0026#34;John\u0026#34;, lastName:\u0026#34;Doe\u0026#34;, age:50, eyeColor:\u0026#34;blue\u0026#34;, eat: (food)=\u0026gt;{} }; // or let person = new Object(); person.firstName = \u0026#34;John\u0026#34;; person.lastName = \u0026#34;Doe\u0026#34;; person.age = 50; person.eyeColor = \u0026#34;blue\u0026#34;; JS 的对象长得很像 json, 所以在以后的学习中不要与json弄混.\n# 3. \u0026ldquo;everything is an object in Javascript\u0026rdquo;, Really? No, not everything is an object in JavaScript. Many things that you interact with regularly (strings, numbers, booleans) are primitives, not objects. Unlike objects, primitive values are immutable. The situation is complicated by the fact that these primitives do have object wrappers (String, Number and Boolean); these objects have methods and properties while the primitives do not, but the primitives appear to have methods because JavaScript silently creates a wrapper object when code attempts to access any property of a primitive.\nFor example, consider the following code:\n1 2 var s = \u0026#34;foo\u0026#34;; var sub = s.substring(1, 2); // sub is now the string \u0026#34;o\u0026#34; Behind the scenes, s.substring(1, 2) behaves as if it is performing the following (approximate) steps:\nCreate a wrapper String object from s, equivalent to using new String(s) Call the substring() method with the appropriate parameters on the String object returned by step 1 Dispose of the String object Return the string (primitive) from step 2. A consequence of this is that while it looks as though you can assign properties to primitives, it is pointless because you cannot retrieve them:\n1 2 3 var s = \u0026#34;foo\u0026#34;; s.bar = \u0026#34;cheese\u0026#34;; alert(s.bar); // undefined This happens because the property is effectively defined on a String object that is immediately discarded.'\nNumbers and Booleans also behave this way. Functions, however, are fully-fledged objects, and inherit from Object (actually Object.prototype, but that\u0026rsquo;s another topic). Functions therefore can do anything objects can, including having properties:\n1 2 3 function foo() {} foo.bar = \u0026#34;tea\u0026#34;; alert(foo.bar); // tea Original post: https://stackoverflow.com/a/9110389/16317008\n# 4. JS Collections The key difference is that Objects only support string and Symbol keys where as Maps support more or less any key type. https://stackoverflow.com/a/37994079/16317008\nThe keys of an Object are Strings or Symbols, where they can be of any value for a Map. You can get the size of a Map easily, while you have to manually keep track of size for an Object. These three tips can help you to decide whether to use a Map or an Object:\nUse maps over objects when keys are unknown until run time, and when all keys are the same type and all values are the same type. Use maps if there is a need to store primitive values as keys because object treats each key as a string whether it\u0026rsquo;s a number value, boolean value or any other primitive value. Use objects when there is logic that operates on individual elements. It is useful to remember which operations on arrays mutate them, and which don’t. For example, push, pop, reverse, and sort will mutate the original array, but slice, filter, and map will create a new one.\nReferences: Keyed collections - JavaScript | MDN\n","date":"2023-06-20T20:51:31Z","permalink":"https://blog.yorforger.cc/p/objects-collections-in-javascript/","title":"Objects \u0026 Collections in Javascript"},{"content":" # 1. List # 1.1. Indexing right to left, use negative number, start from -1\nleft to right, use positive number, start form 0\n1 2 3 4 5 arr = [1, 2, 3] print(arr[-1]) # 3 print(arr[0]) # 1 print(arr[:-2]) # [1] print(arr[-4]) # error: out of range # 1.2. Reslice makes shallow copy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cat = { \u0026#34;name\u0026#34;: \u0026#34;Coco\u0026#34;, \u0026#34;age\u0026#34;: 3.5, } original_list = [cat, 2, 3, 4, 5] copied_list = original_list[:3] print(\u0026#34;Original list:\u0026#34;, original_list) print(\u0026#34;Copied list:\u0026#34;, copied_list) copied_list[1] = 999 copied_list[0][\u0026#34;name\u0026#34;] = \u0026#34;Bella\u0026#34; print(\u0026#34;Original list (after modification):\u0026#34;, original_list) print(\u0026#34;Copied list (after modification):\u0026#34;, copied_list) Output:\n1 2 3 4 Original list: [{\u0026#39;name\u0026#39;: \u0026#39;Coco\u0026#39;, \u0026#39;age\u0026#39;: 3.5}, 2, 3, 4, 5] Copied list: [{\u0026#39;name\u0026#39;: \u0026#39;Coco\u0026#39;, \u0026#39;age\u0026#39;: 3.5}, 2, 3] Original list (after modification): [{\u0026#39;name\u0026#39;: \u0026#39;Bella\u0026#39;, \u0026#39;age\u0026#39;: 3.5}, 2, 3, 4, 5] Copied list (after modification): [{\u0026#39;name\u0026#39;: \u0026#39;Bella\u0026#39;, \u0026#39;age\u0026#39;: 3.5}, 999, 3] The [:] makes a shallow copy of the array, hence allowing you to modify your copy without damaging the original. The reason this also works for strings is that in Python, Strings are arrays of bytes representing Unicode characters. What does [:] mean in Python?\nThis is similar to reslicing in Golang, but not same, in Gloang the new slice shares a same underlying array with it resliced from, whereas Python will create a new list object directly.\n# 1.3. Reslicing in Golang and Python Python: When you new_list = old_list[start:end], a new list object is created and the elements from the specified slice are copied over (shallow copy) to this new list. This means the original list and the new list are completely independent of each other – changing one will not affect the other.\n1 2 3 4 5 6 7 8 9 10 11 12 # Python Example original_list = [1, 2, 3, 4, 5] sliced_list = original_list[:3] original_list[0] = 99 sliced_list[1] = 101 print(\u0026#34;Original list:\u0026#34;, original_list) print(\u0026#34;Sliced list:\u0026#34;, sliced_list) # output Original list: [99, 2, 3, 4, 5] Sliced list: [1, 101, 3] Go: the new slice created from reslicing does not copy the elements. Instead, it refers to the same underlying array as the original slice.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func main() { originalSlice := []int{1, 2, 3, 4, 5} newSlice := originalSlice[:3] originalSlice[0] = 99 // This also changes newSlice[0] newSlice[1] = 101 // This also changes originalSlice[1] fmt.Println(\u0026#34;Original slice:\u0026#34;, originalSlice) fmt.Println(\u0026#34;New slice:\u0026#34;, newSlice) } // ouput Original slice: [99 101 3 4 5] New slice: [99 101 3] # 2. Dictionary 1 2 3 dict = {\u0026#39;a\u0026#39;: 1} print(dict[\u0026#39;b\u0026#39;]) # Raise a KeyError print(dict.get(\u0026#39;b\u0026#39;, 0)) # Print 0, with a default value e.g.,\n1 2 3 4 5 6 7 8 9 10 11 12 # calculate the frequency of the words def count_words(word_list): word_count = {} for word in word_list: if word in word_count: word_count[word] += 1 else: word_count[word] = 1 return word_count words = [\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;apple\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;apple\u0026#34;] total, count = count_words(words) # 3. Set Duplicates Not Allowed\nSet items are unordered, unchangeable, and do not allow duplicate values. Unordered\nSet items can appear in a different order every time you use them, and cannot be referred to by index or key. Unchangeable\nOnce a set is created, you cannot change its items, but you can remove items and add new items. 1 2 3 4 5 6 # because duplicates not allowed, we can use this nature to remove duplicated words def find_missing_words(word_list_1, word_list_2): set1 = set(word_list_1) set2 = set(word_list_2) mis_words = set1 - set2 return mis_words ","date":"2023-06-17T22:51:31Z","permalink":"https://blog.yorforger.cc/p/collections-in-python/","title":"Collections in Python"},{"content":" # 1. ECMAScript vs JavaScript ECMAScript = ES:\nECMAScript is a Standard for scripting languages. Languages like Javascript are based on the ECMAScript standard. JavaScript = JS:\nJavaScript is the most popular implementation of the ECMAScript Standard. The core features of Javascript are based on the ECMAScript standard, but Javascript also has other additional features that are not in the ECMA specifications/standard. Every browser has a JavaScript interpreter. ES5 = ECMAScript 5:\nES5 is a version of the ECMAScript. ES5 does not require a build step (transpilers) to transform it into something that will run in today\u0026rsquo;s browsers. ECMAScript version 5 was finished in December 2009, the latest versions of all major browsers (Chrome, Safari, Firefox, and IE) have implemented version 5. ES6 = ES2015:\nES2015 is a version of the ECMAScript (new/future one). Officially the name ES2015 should be used instead of ES6. There are quite a few transpilers that will export ES5 for running in browsers. TypeScript and CoffeeScript:\nBoth provides syntactic sugar on top of ES5 and then are transcompiled into ES5 compliant JavaScript. You write TypeScript or CoffeeScript then the transpiler transforms it into ES5 JavaScript. Source: https://stackoverflow.com/a/33748400/16317008\n# 2. CommonJS vs ES Modules CommonJS is an older module system for JavaScript that was designed for server-side JavaScript development with Node.js.\n1 2 3 4 5 6 7 8 9 // my-module.js module.exports cat = { age: 2, name: \u0026#34;Coco\u0026#34; }; // main.js const cat = require(\u0026#39;./my-module.js\u0026#39;); console.log(cat.name); // 42 ES modules are a standardized module system for JavaScript that was introduced in ES6. It provides a way to organize and reuse code in a modular and maintainable manner.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // foo.js function sayHello() { console.log(\u0026#39;hello\u0026#39;) } let a = 100 export {a, sayHello} // main.js import {a, sayHello} from \u0026#34;./foo\u0026#34;; console.log(a) sayHello() As of right now ES6 import, export is always compiled to CommonJS, so there is no benefit using one or other. Although usage of ES6 is recommended since it should be advantageous when native support from browsers released. The reason being, you can import partials from one file while with CommonJS you have to require all of the file. Source\nNote that you need to add \u0026quot;type\u0026quot;: \u0026quot;module\u0026quot; in package.json file if you want use ES modules syntax.\n# 3. ES2015 New Features ECMAScript 2015 introduces two new ways of declaring variables: let and const.\nArrow Functions and Lexical this\nJavaScript Classes\nECMAScript 2015 Promises\nLearn more: A Rundown of JavaScript 2015 features\n","date":"2023-06-13T10:36:28Z","permalink":"https://blog.yorforger.cc/p/ecmascript-vs-javascript-commonjs-vs-es-modules/","title":"ECMAScript vs JavaScript \u0026 CommonJS vs ES Modules"},{"content":" # 1. SSH 应对中间人攻击的方法 SSH 采用了公钥加密, 过程如下：\n（1）Remote Host 收到用户的登录请求, 把自己的公钥发给用户\n（2）用户使用这个公钥, 将登录密码加密后, 发送过去\n（3）Remote Host 用自己的私钥, 解密信息, 验证密码是否正确\n这个过程存在一个漏洞：如果有人截获了用户的登录请求，然后冒充 Remote Host，将伪造的公钥发给用户，那么用户很难辨别真伪。这就是 Man-in-the-middle attack, 应对方法有两种:\n利用公钥指纹人工进行对比验证, 上传公钥实现免密登录 接下来我们一一介绍这两种方法,\n# 2. 利用公钥指纹人工进行对比验证 看来面的例子, 在Mac上通过ssh连接远程的服务器, 第一次连接的时候会问下面提示:\n1 2 3 4 5 6 ssh root@144.202.16.29 The authenticity of host \u0026#39;144.202.16.29 (144.202.16.29)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:sa5vDYS0yhdMRXO6CgMrp9AcQoVQRiDw6TnzTKesnzQ. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes .... 这是 ssh 在提醒它无法确认 remost host 是不是就是你要连接的那个主机, 因为可能会发生中间人攻击嘛, 但知道它的公钥指纹是sa5vDYS0..., 问我们还要继续连接吗 (注意单词 establish 在这是 “认证确认” 的意思),\n那我们怎么知道远程主机的公钥指纹应该是多少？当然是去你的服务器上查看公钥指纹:\n1 2 [root@vultr ssh]# ssh-keygen -lf /etc/ssh/ssh_host_ed25519_key.pub 256 SHA256:sa5vDYS0yhdMRXO6CgMrp9AcQoVQRiDw6TnzTKesnzQ root@vultr.guest (ED25519) 注意如果你的服务器使用的是其他hash function生成的公钥指纹, 那你就要查看其他文件了:\n1 ssh-keygen -E md5 -lf /etc/ssh/ssh_host_ed25519_key.pub 可以发现输出内容与上面 ssh 警告的指纹相同, 所以我们要来接的这个是我们的真正主机,\n可能有人会说, 那我们买的服务器物理主机在谷歌阿里, 怎么去直接验证? 你可以在你购买VPS的网站上连接自己服务器保证你连接的一定是你的主机, 但我们只是测试, 所以你直接忽略 ssh 的提示警告, 输入yes, 连上服务器后去验证一下就好了, 因为肯定不会有中间人闲的蛋疼来攻击我们的连接吧? 几块钱一个月的服务器, 谁来攻击你,\n有人可能又会疑问, 那我们也可以直接去远程主机查看他的公钥啊, 为啥还要用个hash函数来生成它的指纹, 再去比对, 不是多此一举吗? 首先你没发现公钥的指纹很短吗? 我们去远程主机验证一般是用肉眼来比对吧, 那公钥那么长, 几百个字符, 很容易比对错, 而公钥指纹的主要目的就在于它很短, 方便我们比对,\n最后关于 ssh 输出的信息, 还有其它想说的, 根据输出:\n1 2 3 4 ssh root@144.202.16.29 The authenticity of host \u0026#39;144.202.16.29 (144.202.16.29)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:sa5vDYS0yhdMRXO6CgMrp9AcQoVQRiDw6TnzTKesnzQ. ... 该 remote host 上的 ssh 使用的公私钥是由 ED25519 算法生成的, ED25519 是非对称加密算法, 常见的非对称加密算法还有 RSA, 所以 RSA 和 ED25519 是并列的: Today, the RSA is the most widely used public-key algorithm for SSH key. But compared to Ed25519, it\u0026rsquo;s slower and even considered not safe if it\u0026rsquo;s generated with the key smaller than 2048-bit length. EdDSA is a digital signature scheme, Ed25519 is the EdDSA signature scheme using SHA-512 (SHA-2) and Curve25519. \u0026ndash;Wiki\n另外, 该公钥指纹是由 SHA256 hash function 生成的, 另外常见的 hash function 还有md5,\n# 2.1. 验证公私钥位置 这个时候我们在电脑终端输入yes, 然后就会提示输入密码 (比如root用户对应的密码), 然后系统会提示如下:\n1 2 3 4 Warning: Permanently added \u0026#39;144.202.16.29\u0026#39; (ED25519) to the list of known hosts. root@144.202.16.29\u0026#39;s password: Last login: Fri Apr 7 20:36:06 2023 [root@vultr ~]# ls 当远程主机的公钥被接受以后, 它会被保存在文件~/.ssh/known_hosts之中, 下次再连接这台主机, 系统就会认出它的公钥已经保存在本地了, 从而跳过警告部分, 直接提示输入密码, 我们来查看Mac上的输出:\n1 2 3 4 5 6 7 8 9 10 ls ~/.ssh id_rsa id_rsa.pub known_hosts known_hosts.old cat ~/.ssh/known_hosts github.com ssh-ed25519 AAAAC3NzaC1lZDI1N... github.com ecdsa-sha2-nistp256 AAAAE2VjZH.... github.com ssh-rsa AAAAB3NzaC1yc2EAAAADA.... 144.202.16.29 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOpG35RMxOKeeLbTfdWlPgToThzrm00sRpMRQs+pdYig 144.202.16.29 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBHP5xEl1122X1Vtc5LzqMp6vlvd4cHRD151ag61xXThvT7KM9vuUK23ol4LKoXMoivUH1SAcWandumVKG37zZfA= 再看看服务器上的输出,\n1 2 3 4 5 6 7 8 9 10 11 [root@vultr ssh]# ls /etc/ssh moduli ssh_host_dsa_key.pub ssh_host_ed25519_key.pub ssh_config ssh_host_ecdsa_key ssh_host_rsa_key sshd_config ssh_host_ecdsa_key.pub ssh_host_rsa_key.pub ssh_host_dsa_key ssh_host_ed25519_key [root@vultr ssh]# cat ssh_host_ed25519_key.pub ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOpG35RMxOKeeLbTfdWlPgToThzrm00sRpMRQs+pdYig root@vultr.guest [root@vultr ssh]# cat ssh_host_ecdsa_key.pub ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBHP5xEl1122X1Vtc5LzqMp6vlvd4cHRD151ag61xXThvT7KM9vuUK23ol4LKoXMoivUH1SAcWandumVKG37zZfA= root@vultr.guest 这两个一个是公钥, 一个是公钥的指纹, 可以看出和上面Mac的存储的内容是一样, 然后上面在服务器 ls /etc/ssh 的输出, 有 ssh_host_rsa_key.pub, ssh_host_ecdsa_key.pub 这就是使用不同的算法产生的不同的key,\n最后 Mac 上 ~/.ssh/known_hosts 的输出 github 那部分有 ssh-ed25519, ecdsa-sha2-nistp256, 这是什么呢?\nFor ssh-ed25519 and ecdsa-sha2-nistp256 which one is used for a given connection depends on the capabilities and preferences of the client, namely your ssh program. If you are using OpenSSH versions 6.5 to 8.1, then it prefers ecdsa then ed25519, and only 8.2 up prefers ed25519 first. Why does GitHub recommend ed25519 SSH key encryption scheme, but itself uses ECDSA? - Super User\necdsa-sha2-nistp256: Specifies the ECDSA algorithm with 256-bit key strength rsa: Specifies the public key algorithm rsa # 3. Public Key Authentication (上传公钥实现免密登陆) # 3.1. 过程分析 使用密码登录, 每次都必须输入密码, 非常麻烦, 好在SSH还提供了公钥登录, 可以省去输入密码的步骤, 具体验证过程如下:\nThe client generates a public/private key pair, typically with RSA or ECC. The client keeps the private key secret and registers the public key with the SSH server. When the client connects to the server, the server authenticates the client by checking if it has the corresponding public key registered for that client. The server will send a challenge message to the client, requesting authentication. The client will take the challenge message and use its private key to generate a digital signature. This proves that the client has the correct private key without revealing the key itself. The client sends the digital signature back to the server as a response to the challenge. The server verifies the signature using the client\u0026rsquo;s registered public key. If the signature is validated, the server knows the client has proven possession of the corresponding private key and grants it access. 注意关于验证过程, 不同 ssh 版本可能会有不同的实现, 你可能会看到有人说远程主机用 用户的公钥进行解密验证, 其实公钥并不可以用来解密, 别人指的应该是公钥可以用来验证数字签名, 即这种情况下私钥加密其实应该是私钥签名。 私钥 “加密” 以后，谁用公钥都可以打开，就已经失去了加密的意义，所以它只能起到一个“签名”的效果，来达到-大家知道这条信息是我，而且只有我发出的。\n记住公钥只能用来加密, 不可以用来解密, 不然就不叫公钥了, 所以是远程主机用 用户的公钥进行用户的验证数字签名, 总结公钥有俩功能:\n加密 验证数字签名 # 3.2. 具体操作 远程主机需要使用用户的公钥来验证用户的身份, 所以本地机器要生成公私钥:\n1 2 # generate ssh keys $ ssh-keygen 一路回车之后在~/.ssh/会新生成两个文件：id_rsa.pub和id_rsa, 前者是你的公钥, 后者是你的私钥, 这时再输入下面的命令, 将公钥传送到远程主机host上面:\n1 $ ssh-copy-id root@144.202.16.29 完成, 之后再登录就不需要输入密码了:\n1 $ ssh root@144.202.16.29 其实你也可以直接编辑远程主机 ~/.ssh/authorized_keys 文件, 把你本地主机的公钥的内容添加进去就行了, ssh-copy-id root@144.202.16.29 做的就是这件事. 下面我们会验证.\n# 4. authorized_keys file 上面 ssh-copy-id root@144.202.16.29 执行后, 本机公钥存储在了远程主机~/.ssh/authorized_keys:\n1 2 3 4 5 6 [root@vultr ~]# ls -a . .bash_history .bash_profile .cache .pki .tcshrc .. .bash_logout .bashrc .cshrc .ssh [root@vultr ~]# cat .ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvbHLCIxxUDDqktbqdrICPa+JDd3kEyowKpy9igugi7R+f/94UDBDJLmeu+K8wi90pjwq+mTM6bSPXBkjmYGibCPbUKk7RtrVx5FdR488PR7/ptMqQXJeQeMOIXvK2Lfnzay+rH5Fg/8z1+pd7cuHPq0bWm5LroGq+bYXVTIYgjKC5NDxPbQCY7zd4c0L+SvxlwqrJFvRBZKY41UBLywtuM8geluLWaGcbikX1K2hFVcZ7ETogG7eqdRBtbfx+JxhyRY1Od+snM88CSfuQkOgs4xQli3GrGttgY0f8BA65/pbixG9gAPkacEkexS997iuTP9BmwLmwWq1pw91c0yEQO1JnsbGHj/YfRhBV6s4FL8n5uVC0My64tisqA+8eZTeld8Zwem4XQGjoqwt2HYy1YXv0kOU8NyI0EGDz3fmqER3ex0cL+MqvWf/cnWQ6MRvGI3w/gL3+V8ueZv5qXpnY+ZH2UcrqEv7Xl74fkdqPYo53ySLQ9ZCiCitHgMjl3bk= shwezu@qq.com 在Mac上查看我自己的公钥, 是一样的:\n1 2 # cat id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvbHLCIxxUDDqktbqdrICPa+JDd3kEyowKpy9igugi7R+f/94UDBDJLmeu+K8wi90pjwq+mTM6bSPXBkjmYGibCPbUKk7RtrVx5FdR488PR7/ptMqQXJeQeMOIXvK2Lfnzay+rH5Fg/8z1+pd7cuHPq0bWm5LroGq+bYXVTIYgjKC5NDxPbQCY7zd4c0L+SvxlwqrJFvRBZKY41UBLywtuM8geluLWaGcbikX1K2hFVcZ7ETogG7eqdRBtbfx+JxhyRY1Od+snM88CSfuQkOgs4xQli3GrGttgY0f8BA65/pbixG9gAPkacEkexS997iuTP9BmwLmwWq1pw91c0yEQO1JnsbGHj/YfRhBV6s4FL8n5uVC0My64tisqA+8eZTeld8Zwem4XQGjoqwt2HYy1YXv0kOU8NyI0EGDz3fmqER3ex0cL+MqvWf/cnWQ6MRvGI3w/gL3+V8ueZv5qXpnY+ZH2UcrqEv7Xl74fkdqPYo53ySLQ9ZCiCitHgMjl3bk= shwezu@qq.com # 5. ssh_config vs sshd_config file I would like to change my SSH port running Linux CentOS 6. I also noticed there\u0026rsquo;s an /etc/ssh_config file along with /etc/sshd_config. What\u0026rsquo;s the difference between the two? Should I change both?\nThe sshd_config is the ssh daemon (or ssh server process) configuration file. As you\u0026rsquo;ve already stated, this is the file you\u0026rsquo;ll need to modify to change the server port.\nWhereas, the ssh_config file is the ssh client configuration file. The client configuration file only has bearing on when you use the ssh command to connect to another ssh host. So, in this case, you don\u0026rsquo;t need to modify it. It will be other client machines connecting to your server.\nSource: Should I modify only sshd_config, or also ssh_config?\nReferences:\nhttps://en.wikipedia.org/wiki/EdDSA https://en.wikipedia.org/wiki/Digital_signature https://superuser.com/a/1688126 https://security.stackexchange.com/questions/230708/should-i-be-using-ecdsa-keys-instead-of-rsa Man-in-the-middle attack How to check your SSH key fingerprint (verify the authenticity of the remote host) ","date":"2023-06-03T17:14:27Z","permalink":"https://blog.yorforger.cc/p/%E5%AE%9E%E7%8E%B0ssh%E5%85%8D%E5%AF%86%E7%99%BB%E9%99%86-ssh%E5%A6%82%E4%BD%95%E9%AA%8C%E8%AF%81%E8%BF%9C%E7%A8%8B%E4%B8%BB%E6%9C%BA%E7%9C%9F%E5%AE%9E%E6%80%A7/","title":"实现ssh免密登陆 ssh如何验证远程主机真实性"},{"content":"编码问题很常见, 有时候读文件打开是乱码, 有的语言说自己字符串采用unicode表示字符, 可又来utf-8编码, 这都是什么?\n# 1. 编码和乱码 (unicode vs ascii) 电脑只能看懂二进制数, 所以得想办法把人类语言用二进制表示, 这就是编码的目的. ASCII 规定数字65代表字符A, 66代表字符B依次类推, 所以ASCII表就是一个map, 每个字符对应一个数, 把字符按照预定规则对应到数字的过程就叫编码.\n到后来计算机普及, 全世界都使用不同语言, 出现了问题, 比如我们用36代表汉字牛, 韩国说用36代表字符\u0026amp;, 当我输入牛, 软件就把0010 0100存入磁盘, 然后我把文件传给韩国的朋友, 他们的程序认为36即0010 0100代表\u0026amp;, 这就产生了乱码: 软件尝试使用与文件编码不同的编码方式来解码文件,\n所以我们需要一个新的标准, 可以涵盖全世界字符的那种, 然后所有软件都遵守这个标准, 这样才能无差错沟通, 我发送一串二进制在我这代表字符A, 你的软件收到这串二进制后翻译出的也是字符A, 而不是B或C,\n这时候 Unicode 就出来了, 它就是使用0~0x10FFFF的数字来表示世界上所有的字符, 如汉字 在 的Unicode值是 0x5728, 注意0x代表值5728是十六进制, 又如字符 A 的Unicode值是0x41, 这里说一下, Unicode表示的字符里英文字符的值和ASCII表是相同的,\nUnicode 和 ASCII 都是字符集, 但是 ASCII 只包含 128 个字符, 而 Unicode 包含很多很多个字符.\n# 2. Unicode vs UTF-8 vs UTF-16 # Unicode 定义和目的： Unicode 是一个国际标准，用于不同系统和程序间统一表示文本数据。 它为世界上几乎所有的字符和文本符号分配了唯一的 code point。 Unicode standard describes how characters are represented by code points. A code point value is an integer in the range 0 to 0x10FFFF. In the standard and in this document, a code point is written using the notation U+265E to mean the character with value 0x265e (9,822 in decimal).\nCode point 范围：\n从 U+0000 到 U+10FFFF，包括了超过 100,000 个字符。 重点：\nUnicode 是字符集（character set），定义了字符和 code point 之间的映射，但不规定具体如何在计算机中存储这些code point。 # UTF-8 定义和特点：\nUTF-8（8-bit Unicode Transformation Format）是一种对 Unicode code point 进行编码的方式。 它是一种可变长度的字符编码方法，使用 1 到 4 个字节来表示一个 Unicode code point。 优势：\n兼容性好，ASCII 编码的字符在 UTF-8 中保持单字节形式，与传统 ASCII 编码兼容。 在存储英文文本时空间效率高。因为英文字符在 UTF-8 中只占用 1 个字节。 # UTF-16 定义和特点：\nUTF-16（16-bit Unicode Transformation Format）是另一种对 Unicode code point进行编码的格式。 使用 2 个或 4 个字节来表示一个 Unicode code point。 在UTF-16中，字符可以用一个或两个16位的 code units 来表示. 特性：\n在处理某些语言（如中文、日文、韩文）时可能比 UTF-8 更加空间高效。 因为多数汉字在 UTF-16 中只占用 2 个字节。 # 主要区别 编码长度： UTF-8 是可变长度的，从 1 到 4 个字节不等。 UTF-16 通常使用 2 个或 4 个字节。 兼容性： UTF-8 与传统 ASCII 编码完全兼容。 UTF-16 与 ASCII 不兼容。 空间效率： 对于主要包含 ASCII 字符的文本，UTF-8 更加高效。 对于包含大量非西方字符的文本，UTF-16 可能更加高效。 总的来说，Unicode 是一个广泛的字符集，定义了全球各种字符的 code point。而 UTF-8 和 UTF-16 是这些code point在计算机存储和传输中的具体编码实现方式。选择哪种编码方式取决于特定的应用场景和空间效率需求。\n关于 utf-8 这里举个例子, 汉字汉的Unicode值是两字节即 6C49, 二进制为: 0110 1100 0100 1001, 因为 utf-8 规定汉字占 3 字节, 因此选择第三行进行编码, 根据上标经过utf-8编码变成 11100110 10110001 10001001, 因此最终写入文件的是其三字节的 utf-8 encoding 11100110 10110001 10001001, 而不是其Unicode 0110 1100 0100 1001 , 可以使用 xxd 查看文件的16进制内容,\n1 2 3 4 $ xxd text.md 00000000: e6b1 89 $ file text.md text.md: Unicode text, UTF-8 text e6b1 89 = 11100110 10110001 10001001\n1 2 3 4 5 6 Binary format of bytes in sequence 1st Byte 2nd Byte 3rd Byte 4th Byte Number of Free Bits Maximum Expressible Unicode Value 0xxxxxxx 7 007F hex (127) 110xxxxx 10xxxxxx (5+6)=11 07FF hex (2047) 1110xxxx 10xxxxxx 10xxxxxx (4+6+6)=16 FFFF hex (65535) 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx (3+6+6+6)=21 10FFFF hex (1,114,111) 读到一篇文章总结的很好 分享片段:\n在Unicode出现之前，所有的字符集都是和具体编码方案绑定在一起的，都是直接将字符和最终字节流绑定死了，例如ASCII编码系统规定使用7比特来编码ASCII字符集；GB2312以及GBK字符集，限定了使用最多2个字节来编码所有字符，并且规定了字节序。这样的编码系统通常用简单的查表，也就是通过代码页就可以直接将字符映射为存储设备上的字节流了。例如下面这个例子：\n这种方式的缺点在于，字符和字节流之间耦合得太紧密了，从而限定了字符集的扩展能力。假设以后火星人入住地球了，要往现有字符集中加入火星文就变得很难甚至不可能了，而且很容易破坏现有的编码规则。\n因此Unicode在设计上考虑到了这一点，将字符集和字符编码方案分离开:\n也就是说，虽然每个字符在Unicode字符集中都能找到唯一确定的编号（字符码，又称Unicode码），但是决定最终字节流的却是具体的字符编码。例如同样是对Unicode字符“A”进行编码，UTF-8字符编码得到的字节流是0x41，而UTF-16（大端模式）得到的是0x00 0x41。\n# 3. 修改文件编码方式 如果你直接把utf-8编码的文件转为其它编码比如gbk, 那转换之后你的文件肯定是乱码, 因为在你写入一些内容比如汉到你的文本文件, 此时这个文件的编码方式为utf-8, 那你保存此文件后, 此文件的内容已经是经过utf-8编码二进制数, 即:11100110 10110001 10001001也就是e6b1 89就是上面的汉字汉, 此时你硬要把文件的编码方式改为gbk, 而gbk采用完全与utf-8不同的编码方式(2字节1个字符), 此时当其他软件是图打开你这个文本文件时, 就会查看你文件的编码信息, 他们看到是gbk编码, 那就会把11100110 10110001 10001001即e6b1 89中的前两个字节解释为一个字符, 然后他们查找11100110 10110001即e6b1, 那肯定匹配不到汉, 就会把11100110 10110001解释为不可打印字符或者英文或者其它语言,,,\n但也可以实现不同编码的安全转换, 一个思路是, 假如知道文件是用的utf-8编码, 所以我们先把该文件的字符转换为unicode code point, 然后再利用gbk进行编码这些unicode code, 具体做法如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import sys file_path = \u0026#39;article_1.md\u0026#39; with open(file_path, encoding=\u0026#34;utf-8\u0026#34;) as f: utf_8_str = f.read() if utf_8_str is None: print(\u0026#34;Contents of Text cannot be None!\u0026#34;) sys.exit() else: gbk_str = utf_8_str.encode(\u0026#34;gbk\u0026#34;) with open(\u0026#34;article_2.md\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(gbk_str) f.encoding = \u0026#34;gbk\u0026#34; 注意, 上面代码utf_8_str = f.read(), 此时utf_8_str已经是unicode code, 第二我们写如文件时, 要以二进制写入, 不然你写入的就是长得像16进制数的字符串, 而不是真正的写入二进制数据,\n# 4. Python 中的编码 电脑只能存储二进制数, 而python也有个bytes类用来代表二进制字符串,\n所以这里有两个概念, string和bytes string, 他们是不同的类, 拥有的函数不同, 对于一个普通的string, 它有个函数叫encode(), 该函数的返回类型是bytes, 如下:\n1 2 3 4 5 6 bytes_str = \u0026#39;AB\u0026#39;.encode(\u0026#39;ascii\u0026#39;) print(type(bytes_str)) print(bytes_str.hex()) \u0026lt;class \u0026#39;bytes\u0026#39;\u0026gt; 4142 然后对于bytes类, 有个函数叫decode(), 该函数的返回类型为str:\n1 2 3 4 5 bytes_str = \u0026#39;AB\u0026#39;.encode(\u0026#39;ascii\u0026#39;) a = bytes_str.decode(\u0026#39;ascii\u0026#39;) print(type(a)) \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; 所以在往一个文件写入的时候想好是直接写入二进制还是写入编码后的字符串, 即注意文件的打开方式:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 文件的内容: 汉在 # 文件的编码方式: gbk, 每个汉字2字节 with open(\u0026#34;article_2.md\u0026#34;, \u0026#34;rb\u0026#34;) as f: bin_str = f.read() print(type(bin_str)) print(bin_str) \u0026lt;class \u0026#39;bytes\u0026#39;\u0026gt; b\u0026#39;\\xba\\xba\\xd4\\xda\u0026#39; with open(\u0026#34;article_2.md\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#39;gbk\u0026#39;) as f: text_str = f.read() print(type(text_str)) print(text_str) \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; 汉在 因为article_2.md采用gbk编码方式, 而python默认是utf-8解码, 所以这里需要指定编码方式, 否则肯定乱码或者出错,\n写入也要注意打开文件的方式:\n1 2 3 4 bin_str = \u0026#39;AB\u0026#39;.encode(\u0026#39;utf-8\u0026#39;) with open(\u0026#34;text.md\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(bin_str) f.encoding = \u0026#34;utf-8\u0026#34; 查看文件的内容:\n1 2 $ xxd text.md 00000000: 4142 AB 如果我们不指定以二进制写入, 则会报错:\n1 2 3 4 5 6 bin_str = \u0026#39;AB\u0026#39;.encode(\u0026#39;utf-8\u0026#39;) with open(\u0026#34;text.md\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(bin_str) f.encoding = \u0026#34;utf-8\u0026#34; # error: TypeError: write() argument must be str, not bytes 注意, python挺好有报错, 但是有的语言比如C的接口, 就不会提醒, 你写入什么就是什么, 如果你打开方式不是二进制, 然后写入了二进制的字符串, 那结果就是, 这些二进制的字符串被当作普通字符串写入文件\u0026hellip;\n","date":"2023-06-01T22:13:25Z","permalink":"https://blog.yorforger.cc/p/%E8%AF%B4%E8%AF%B4%E7%BC%96%E7%A0%81-encoding/","title":"说说编码 - encoding"},{"content":"强迫症, 读博客的时候中英字符或者标点符号后没有空格紧紧的贴在一块看着很不舒服, 于是想着能不能写个脚本来处理一下文章, bash太难用了(是我太菜), 于是用Python简单实现了一下,\n一些芝士在这说一下,\n# 1. Immutable Object in Python Java虽然string是对象但primitive类型还不是对象, 与Java不同Python里如integers, floats, and strings都是对象, 并且与integers, floats一样, python里string对象也是immutable, 所以直接“修改”string会导致python重新创建新的string对象 销毁旧的:\n1 2 3 4 5 6 a = \u0026#39;hello\u0026#39; print(id(a)) # 4378912048 a += \u0026#39; world\u0026#39; print(id(a)) # 4380683504 因此每次修改(添加空格)都会重新重建一个新的string, 考虑到文章有的内容也不短, 那开销会不小, 具体做法分析参考: https://stackoverflow.com/q/1228299/16317008\n# 2. 编码 另外是关于编码的问题, Unicode和utf-8,\nPython的string使用unicode来表示字符,\nPython’s string type uses the Unicode Standard for representing characters, which lets Python programs work with all these different possible characters.\nUnicode就是赋予字符一个唯一的value, 如 \u0026lsquo;a\u0026rsquo;的unicode value是 0061, \u0026lsquo;汉\u0026rsquo;的unicode value是6C49,\nUnicode (https://www.unicode.org/) is a specification that aims to list every character used by human languages and give each character its own unique code.\n而utf-8就是一个编码解码规范, 给我们一个unicode value如6C49, 我们把这个unicode value翻译成哪个字符呢? 根据不同的编码规则, 6C49可以被翻译成不同的字符, 甚至两个, 因为有的编码比如ascii就是采用固定1子节编码, 通过查ascii表可知6C会被解释成字母l, 49会被解释成数字1, 但若采取utf-8进行解码, 6C49就会被解释成汉字汉, utf-8是一种可变长度的编码,\n如果还不理解ascii, utf-8, unicode的关系, 看下面\nUTF-8 is an encoding scheme. Other encoding schemes include UTF-16 and UTF-32. In modern times, ASCII is now a subset of UTF-8. UTF-8 is backwards compatible with ASCII.\n我们打开一个文本文件的时候需要指定其编码, 文本文件对你的电脑来说就是一串二进制数, 如果你不告诉他让他使用哪种编码方法来翻译这段字符, 那python程序怎么知道把这段二进制翻译成什么呢?\n我们linux下可以使用xxd以16进制输出指定文件内容, 加入我们有个md文件内容为\n1 汉 使用xxd查看其二进制数据,\n1 2 $ xxd add_whitespace/article.md 00000000: e6b1 89 真奇怪, 为什么一个汉字是3字节? 我们在python中明明使用两个字节表示的汉呀:\n1 2 \u0026gt;\u0026gt;\u0026gt; print(\u0026#39;\\u6C49\u0026#39;) 汉 我查了一个gbk编码一个汉字是2字节, 若使用utf-8编码一个汉字则是3字节, 也就是说text.md可能使用的是utf-8编码, 验证一下:\n1 2 $ file text.md text.md: Unicode text, UTF-8 text 果然, 那为什么上面我们用两字节\\u6C49就表示出汉了呢? 这就又说到了上面的编码方式utf-8, \\u6C49只是Unicode code point, 并不是文件实际存的东西, 我们要存储这个汉字, 还需要使用编码方式, 比如utf-8, 我们看看utf-8编码unicode code的过程,\n1 2 3 4 5 6 7 Binary format of bytes in sequence 1st Byte 2nd Byte 3rd Byte 4th Byte Number of Free Bits Maximum Expressible Unicode Value 0xxxxxxx 7 007F hex (127) 110xxxxx 10xxxxxx (5+6)=11 07FF hex (2047) 1110xxxx 10xxxxxx 10xxxxxx (4+6+6)=16 FFFF hex (65535) 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx (3+6+6+6)=21 10FFFF hex (1,114,111) 所以, 上面的6C49经过utf-8编码变成:\n1 2 # 6C49的二进制:0110 1100 0100 1001 11100110 10110001 10001001 把这个串二进制数转为16进制: e6b1 89与上面xxd输出一样, 这样也验证了utf-8编码unicode的过程,\n最后需要注意, 如果你直接把utf-8编码的文件转为其它编码比如gbk, 那转换之后你的文件肯定是乱码, 因为在你写入一些内容比如汉到你的文本文件, 此时这个文件的编码方式为utf-8, 那你保存此文件后, 此文件的内容已经是经过utf-8编码的unicode code, 即:11100110 10110001 10001001也就是e6b1 89就是上面的汉字汉, 此时你硬要把文件的编码方式改为gbk, 而gbk采用完全与utf-8不同的编码方式(2字节1个字符), 此时当其他软件是图打开你这个文本文件时, 就会查看你文件的编码信息, 他们看到是gbk编码, 那就会把11100110 10110001 10001001即e6b1 89中的前两个字节解释为一个字符, 然后他们查找11100110 10110001即e6b1, 那肯定匹配不到汉, 就会把11100110 10110001解释为不可打印字符或者英文或者其它语言,,,\n一些工具可以实现不同编码的安全转换, 一个思路是, 我们知道这个文件是用的utf-8编码, 所以我们先把该文件的字符转换为unicode code, 然后再利用gbk进行编码这些unicode code, 具体做法如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import sys file_path = \u0026#39;article_1.md\u0026#39; with open(file_path, encoding=\u0026#34;utf-8\u0026#34;) as f: utf_8_str = f.read() if utf_8_str is None: print(\u0026#34;Contents of Text cannot be None!\u0026#34;) sys.exit() else: gbk_str = utf_8_str.encode(\u0026#34;gbk\u0026#34;) with open(\u0026#34;article_2.md\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(gbk_str) f.encoding = \u0026#34;gbk\u0026#34; 注意, 上面代码utf_8_str = f.read(), 此时utf_8_str已经是unicode code, 第二我们写如文件时, 要以二进制写入, 不然你写入的就是长得像16进制数的字符串, 而不是真正的写入二进制数据,\n# 3. 实现 最后, python是值传递, 即我们无法传递一个变量给函数, 然后让函数修改它的值, 所以只能使用全局变量和类包装的形式来修改一个变量的值, 准确来说修改的只是变量的指向, 别忘了integer是不可变的, 所以我们执行a+=1的时候其实是python创建了个新的integer对象然后让变量a指向了一个新的integer value, 注意python里的变量都可以理解为reference, 这个reference不是c++里的reference, 而是Java里的reference类型, 别忘了哦, Java的变量分为两种, primitive和reference, 不知道可以参考: C Go Java Python内存结构及对比 | 橘猫小八的鱼\n上面说道使用全局变量和类包装的形式来修改一个变量的指向, 可是python有采用局部变量大于全局变量的约定, 即要是想在函数里修改全局变量, 我们必须在函数前声明global xxx, 这若是修改多个就很不美观, 所以最终采用使用包装类的方式来修改“全局”变量,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Text: ch_flag = False en_flag = False back_quote_flag = False i = 0 text = None def add_space(before=True): if before: Text.text = Text.text[:Text.i] + \u0026#39; \u0026#39; + Text.text[Text.i:] else: Text.text = Text.text[:Text.i + 1] + \u0026#39; \u0026#39; + Text.text[Text.i + 1:] Text.en_flag = False Text.ch_flag = False Text.i += 1 若使用全局变量:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ch_flag = False en_flag = False back_quote_flag = False i = 0 text = None def add_space(before=True): global text global i global en_flag global ch_flag if before: text = text[:i] + \u0026#39; \u0026#39; + text[i:] else: text = text[:i + 1] + \u0026#39; \u0026#39; + text[i + 1:] en_flag = True ch_flag = True i += 1 太丑了,\n# 4. 程序入口 __name__==\u0026quot;__main__\u0026quot; if __name__==\u0026quot;__main__\u0026quot;: 就相当于Python 模拟的程序入口, 由于模块之间相互引用, 不同模块可能都有这样的定义, 而入口程序只能有一个, 选中哪个入口程序取决于 __name__的值:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import sys def main(): # Check that exactly 1 command line argument was passed if len(sys.argv) != 2: print(\u0026#34;Error: Incorrect number of arguments.\u0026#34;) print(\u0026#34;Usage: python file_path.py path/to/file.txt\u0026#34;) exit(1) file_path = sys.argv[1] print(f\u0026#34;The file path is: {file_path}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Python sets the global __name__ of a module equal to \u0026quot;__main__\u0026quot; if the Python interpreter runs your code in the top-level code environment:\n“Top-level code” is the first user-specified Python module that starts running. It’s “top-level” because it imports all other modules that the program needs. (Source)\n1 2 3 4 5 6 7 8 9 10 print(\u0026#34;look here\u0026#34;) print(__name__) if __name__ == \u0026#39;__main__\u0026#39;: print(\u0026#34;I\u0026#39;m test.py\u0026#34;) # print: look here __main__ I\u0026#39;m test.py # 5. 异常处理 1 2 3 4 5 6 7 8 9 10 file_path = sys.argv[1] try: with open(file_path, encoding=\u0026#34;utf-8\u0026#34;) as f: Text.text = f.read() if Text.text is None: print(\u0026#34;Contents of Text cannot be None!\u0026#34;) else: formatting() except OSError as e: print(f\u0026#34;{type(e)}: {e}\u0026#34;) 源码: https://github.com/shwezhu/PythonLearn/blob/main/add_whitespace/whitespace.py\n参考:\nUnicode HOWTO — Python 3.11.3 documentation encoding - What is the difference between UTF-8 and Unicode? - Stack Overflow utf 8 - ASCII vs Unicode + UTF-8 - Stack Overflow 写 Python 脚本，一定要加上这个！-python写脚本 What Does if name == \u0026ldquo;main\u0026rdquo; Do in Python? – Real Python What is a good way to handle exceptions when trying to read a file in python? - Stack Overflow ","date":"2023-06-01T10:41:24Z","permalink":"https://blog.yorforger.cc/p/%E7%94%A8python%E8%84%9A%E6%9C%AC%E7%BB%99%E6%96%87%E7%AB%A0%E4%B8%AD%E8%8B%B1%E5%AD%97%E7%AC%A6%E4%BB%A5%E5%8F%8A%E7%AC%A6%E5%8F%B7%E9%97%B4%E5%8A%A0%E7%A9%BA%E6%A0%BC/","title":"用Python脚本给文章中英字符以及符号间加空格"},{"content":" # 0. Parrallelism is not concurrency Parallelism is about multiple tasks or subtasks of the same task that literally run at the same time on a hardware with multiple computing resources like multi-core processor, while concurrency is about multiple tasks which start, run, and complete in overlapping time periods, in no specific order.\n# 1. Your program starts When your Go program starts up, it’s given a Logical Processor (P) for every virtual core that is identified on the host machine. If you have a processor with multiple hardware threads per physical core (Hyper-Threading), each hardware thread will be presented to your Go program as a virtual core. To better understand this, take a look at the system report for my MacBook Pro.\nHyper-Threading: one physical core corresponds to two virtual cores. And one virtual core can run one hadware thread at a same time.\nYou can see I have a single processor with 4 physical cores. What this report is not exposing is the number of hardware threads I have per physical core. The Intel Core i7 processor has Hyper-Threading, which means there are 2 hardware threads per physical core. This will report to the Go program that 8 virtual cores are available for executing OS Threads in parallel.\nTo test this, consider the following program:\n1 2 3 4 5 func main() { // NumCPU returns the number of logical // CPUs usable by the current process. fmt.Println(runtime.NumCPU()) } When I run this program on my local machine, the result of the NumCPU() function call will be the value of 8. Any Go program I run on my machine will be given 8 P’s.\nEvery P is assigned an OS Thread (“M”). The ‘M’ stands for machine. This Thread is still managed by the OS and the OS is still responsible for placing the Thread on a Core for execution, as explained in the last post. This means when I run a Go program on my machine, I have 8 threads available to execute my work, each individually attached to a P.\nEvery Go program is also given an initial Goroutine (“G”), which is the path of execution for a Go program. A Goroutine is essentially a Coroutine but this is Go, so we replace the letter “C” with a “G” and we get the word Goroutine. You can think of Goroutines as application-level threads and they are similar to OS Threads in many ways. Just as OS Threads are context-switched on and off a core, Goroutines are context-switched on and off an M.\nThe last piece of the puzzle is the run queues. There are two different run queues in the Go scheduler: the Global Run Queue (GRQ) and the Local Run Queue (LRQ). Each P is given a LRQ that manages the Goroutines assigned to be executed within the context of a P. (这句话也就说明了 多个goroutine映射到一个kernel thread, 减少kernel thread上下文切换的次数.) These Goroutines take turns being context-switched on and off the M assigned to that P. The GRQ is for Goroutines that have not been assigned to a P yet. There is a process to move Goroutines from the GRQ to a LRQ that we will discuss later.\nM和P是一一对应的关系, M机器即用来运行Goroutine的, 而P拥有一个runqueue, 这个runqueue里面有多个等待被M执行的goroutine, 在M上只能同时运行一个goroutine, 所以当go scheduler要switch 一个goroutine的时候, 这时候进行goroutine上下文切换, 注意这里不是对kernel thread, 即M进行上下文切换,\n根据图可以看出, 一个kernel thread维护一个runqueue, 请参考: https://youtu.be/YHRO5WQGh0k\n另外的我电脑是芯片是M1, 信息如下:\n1 Total Number of Cores: 8 (4 performance and 4 efficiency) 执行上面Go程序, 仍为8, 推断M1没有利用Hyper-Threading技术, 这个技术是Intel的, 当然其他制造商也有类似技术,\n# 2. Cooperating scheduler As we discussed in the first post, the OS scheduler is a preemptive scheduler. Essentially that means you can’t predict what the scheduler is going to do at any given time. The kernel is making decisions and everything is non-deterministic. Applications that run on top of the OS have no control over what is happening inside the kernel with scheduling unless they leverage synchronization primitives like atomic instructions and mutex calls.\nThe Go scheduler is part of the Go runtime, and the Go runtime is built into your application. This means the Go scheduler runs in user space, above the kernel. The current implementation of the Go scheduler is not a preemptive scheduler but a cooperating scheduler. Being a cooperating scheduler means the scheduler needs well-defined user space events that happen at safe points in the code to make scheduling decisions.\nWhat’s brilliant about the Go cooperating scheduler is that it looks and feels preemptive. You can’t predict what the Go scheduler is going to do. This is because decision making for this cooperating scheduler doesn’t rest in the hands of developers, but in the Go runtime. It’s important to think of the Go scheduler as a preemptive scheduler and since the scheduler is non-deterministic, this is not much of a stretch.\n# 3. Goroutine states Just like Threads, Goroutines have the same three high-level states. These dictate the role the Go scheduler takes with any given Goroutine. A Goroutine can be in one of three states: Waiting, Runnable or Executing.\nWaiting: This means the Goroutine is stopped and waiting for something in order to continue. This could be for reasons like waiting for the operating system (system calls) or synchronization calls (atomic and mutex operations). These types of latencies are a root cause for bad performance.\nRunnable: This means the Goroutine wants time on an M so it can execute its assigned instructions. If you have a lot of Goroutines that want time, then Goroutines have to wait longer to get time. Also, the individual amount of time any given Goroutine gets is shortened as more Goroutines compete for time. This type of scheduling latency can also be a cause of bad performance.\nExecuting: This means the Goroutine has been placed on an M and is executing its instructions. The work related to the application is getting done. This is what everyone wants.\n# 4. Context switching The Go scheduler requires well-defined user-space events that occur at safe points in the code to context-switch from. These events and safe points manifest themselves within function calls. Function calls are critical to the health of the Go scheduler. Today (with Go 1.11 or less), if you run any tight loops that are not making function calls, you will cause latencies within the scheduler and garbage collection. It’s critically important that function calls happen within reasonable timeframes.\nNote: There is a proposal for 1.12 that was accepted to apply non-cooperative preemption techniques inside the Go scheduler to allow for the preemption of tight loops.\nThere are four classes of events that occur in your Go programs that allow the scheduler to make scheduling decisions. This doesn’t mean it will always happen on one of these events. It means the scheduler gets the opportunity.\nThe use of the keyword go Garbage collection System calls Synchronization and Orchestration System calls\nIf a Goroutine makes a system call that will cause the Goroutine to block the M, sometimes the scheduler is capable of context-switching the Goroutine off the M and context-switch a new Goroutine onto that same M. However, sometimes a new M is required to keep executing Goroutines that are queued up in the P. How this works will be explained in more detail in the next section.\nSynchronization and Orchestration\nIf an atomic, mutex, or channel operation call will cause the Goroutine to block, the scheduler can context-switch a new Goroutine to run. Once the Goroutine can run again, it can be re-queued and eventually context-switched back on an M.\n# 5. Asynchronous system calls When the OS you are running on has the ability to handle a system call asynchronously, something called the network poller can be used to process the system call more efficiently. This is accomplished by using kqueue (MacOS), epoll (Linux) or iocp (Windows) within these respective OS’s.\nNetworking-based system calls can be processed asynchronously by many of the OSs we use today. This is where the network poller gets its name, since its primary use is handling networking operations. By using the network poller for networking system calls, the scheduler can prevent Goroutines from blocking the M when those system calls are made. This helps to keep the M available to execute other Goroutines in the P’s LRQ without the need to create new Ms. This helps to reduce scheduling load on the OS.\nThe best way to see how this works is to run through an example.\nFigure 3 shows our base scheduling diagram. Goroutine-1 is executing on the M and there are 3 more Goroutines waiting in the LRQ to get their time on the M. The network poller is idle with nothing to do.\nIn figure 4, Goroutine-1 wants to make a network system call, so Goroutine-1 is moved to the network poller and the asynchronous network system call is processed. Once Goroutine-1 is moved to the network poller, the M is now available to execute a different Goroutine from the LRQ. In this case, Goroutine-2 is context-switched on the M.\nIn figure 5, the asynchronous network system call is completed by the network poller and Goroutine-1 is moved back into the LRQ for the P. Once Goroutine-1 can be context-switched back on the M, the Go related code it’s responsible for can execute again. The big win here is that, to execute network system calls, no extra Ms are needed. The network poller has an OS Thread and it is handling an efficient event loop.\n# 6. Synchronous system calls What happens when the Goroutine wants to make a system call that can’t be done asynchronously? In this case, the network poller can’t be used and the Goroutine making the system call is going to block the M. This is unfortunate but there’s no way to prevent this from happening. One example of a system call that can’t be made asynchronously is file-based system calls. If you are using CGO, there may be other situations where calling C functions will block the M as well.\nNote: The Windows OS does have the capability of making file-based system calls asynchronously. Technically when running on Windows, the network poller can be used.\nLet’s walk through what happens with a synchronous system call (like file I/O) that will cause the M to block.\nFigure 6 is showing our basic scheduling diagram again but this time Goroutine-1 is going to make a synchronous system call that will block M1.\nIn figure 7, the scheduler is able to identify that Goroutine-1 has caused the M to block. At this point, the scheduler detaches M1 from the P with the blocking Goroutine-1 still attached. Then the scheduler brings in a new M2 to service the P. At that point, Goroutine-2 can be selected from the LRQ and context-switched on M2. If an M already exists because of a previous swap, this transition is quicker than having to create a new M.\nIn figure 8, the blocking system call that was made by Goroutine-1 finishes. At this point, Goroutine-1 can move back into the LRQ and be serviced by the P again. M1 is then placed on the side for future use if this scenario needs to happen again.\n到这我们也可以理解为什么说P是连接M和G的桥梁, P维护一个runqueue, 里面存的是等待被M执行G, M执行G时从和他连接的那个P的runqueue里拿G, 至于谁更新P的runqueue, 谁负责把G context switch onto M或者off M, 是P自己呢?还是Go Scheduler负责呢?\n这里也注意一下, M 可以被 (OS) contex-switched M off/onto the Core, G可以被 contex-switched M off/onto the M, 至于被谁, 那肯定是Go Scheduler了, 上面提到了: The Go scheduler requires well-defined user-space events that occur at safe points in the code to context-switch from. context-switch谁? 当然是 context-switch G onto/off from M 了,\nSource:\nScheduling In Go : Part II - Go Scheduler GopherCon 2018: Kavya Joshi - The Scheduler Saga ","date":"2023-05-28T18:06:20Z","permalink":"https://blog.yorforger.cc/p/go-scheduler-2/","title":"Go Scheduler (2)"},{"content":"Source: Scheduling In Go : Part I - OS Scheduler\n# 1. OS scheduler Your program is just a series of machine instructions that need to be executed one after the other sequentially. To make that happen, the operating system uses the concept of a Thread. It’s the job of the Thread to account for and sequentially execute the set of instructions it’s assigned. Execution continues until there are no more instructions for the Thread to execute. This is why I call a Thread, “a path of execution”.\nEvery program you run creates a Process and each Process is given an initial Thread. Threads have the ability to create more Threads. All these different Threads run independently of each other and scheduling decisions are made at the Thread level, not at the Process level. Threads can run concurrently (each taking a turn on an individual core), or in parallel (each running at the same time on different cores). Threads also maintain their own state to allow for the safe, local, and independent execution of their instructions.\nThe OS scheduler is responsible for making sure cores are not idle if there are Threads that can be executing. It must also create the illusion that all the Threads (that can execute) are executing at the same time. In the process of creating this illusion, the scheduler needs to run Threads with a higher priority over lower priority Threads. However, Threads with a lower priority can’t be starved of execution time. The scheduler also needs to minimize scheduling latencies as much as possible by making quick and smart decisions.\nTo understand all of this better, it’s good to describe and define a few concepts that are important.\n# 2. Program counter - PC The program counter (PC), which is sometimes called the instruction pointer (IP), is what allows the Thread to keep track of the next instruction to execute. In most processors, the PC points to the next instruction and not the current instruction.\n# 3. Thread states Another important concept is Thread state, which dictates the role the scheduler takes with the Thread. A Thread can be in one of three states: Waiting, Runnable or Executing.\nWaiting: This means the Thread is stopped and waiting for something in order to continue. This could be for reasons like, waiting for the hardware (disk, network), the operating system (system calls) or synchronization calls (atomic, mutexes). These types of latencies are a root cause for bad performance.\nRunnable: This means the Thread wants time on a core so it can execute its assigned machine instructions.\nExecuting: \u0026hellip;\n# 4. Types of work There are two types of work a Thread can do. The first is called CPU-Bound and the second is called IO-Bound.\nCPU-Bound: This is work that never creates a situation where the Thread may be placed in Waiting states. This is work that is constantly making calculations. A Thread calculating Pi to the Nth digit would be CPU-Bound.\nIO-Bound: This is work that causes Threads to enter into Waiting states. This is work that consists in requesting access to a resource over the network or making system calls into the operating system. A Thread that needs to access a database would be IO-Bound. I would include synchronization events (mutexes, atomic), that cause the Thread to wait as part of this category.\nThe term CPU-bound describes a scenario where the execution of a task or program is highly dependent on the CPU. In contrast, a task or program is I/O bound if its execution is dependent on the input-output system and its resources, such as disk drives and peripheral devices.\n# 5. Context switching If you are running on Linux, Mac or Windows, you are running on an OS that has a preemptive scheduler. This means a few important things. First, it means the scheduler is unpredictable when it comes to what Threads will be chosen to run at any given time. Thread priorities together with events, (like receiving data on the network) make it impossible to determine what the scheduler will choose to do and when. Second, it means you must never write code based on some perceived behavior that you have been lucky to experience but is not guaranteed to take place every time. It is easy to allow yourself to think, because I’ve seen this happen the same way 1000 times, this is guaranteed behavior. You must control the synchronization and orchestration of Threads if you need determinism in your application.\nThe physical act of swapping Threads on a core is called a context switch. A context switch happens when the scheduler pulls an Executing thread off a core and replaces it with a Runnable Thread. The Thread that was selected from the run queue moves into an Executing state. The Thread that was pulled can move back into a Runnable state (if it still has the ability to run), or into a Waiting state (if was replaced because of an IO-Bound type of request).\nContext switches are considered to be expensive because it takes times to swap Threads on and off a core. The amount of latency incurrent during a context switch depends on different factors but it’s not unreasonable for it to take between ~1000 and ~1500 nanoseconds. Considering the hardware should be able to reasonably execute (on average) 12 instructions per nanosecond per core, a context switch can cost you ~12k to ~18k instructions of latency. In essence, your program is losing the ability to execute a large number of instructions during a context switch.\nIf you have a program that is focused on IO-Bound work, then context switches are going to be an advantage. Once a Thread moves into a Waiting state, another Thread in a Runnable state is there to take its place.** This allows the core to always be doing work. This is one of the most important aspects of scheduling. Don’t allow a core to go idle if there is work (Threads in a Runnable state) to be done. 还记得文章第一个加粗的那段话吗, OS Scheduler的作用, 然后再看看段句话,\nIf your program is focused on CPU-Bound work, then context switches are going to be a performance nightmare. Since the Thead always has work to do, the context switch is stopping that work from progressing. This situation is in stark contrast with what happens with an IO-Bound workload\n# 6. Less is more In the early days when processors had only one core, scheduling wasn’t overly complicated. Because you had a single processor with a single core, only one Thread could execute at any given time. The idea was to define a scheduler period and attempt to execute all the Runnable Threads within that period of time. No problem: take the scheduling period and divide it by the number of Threads that need to execute.\nAs an example, if you define your scheduler period to be 1000ms (1 second) and you have 10 Threads, then each thread gets 100ms. If you have 100 Threads, each Thread gets 10ms. However, what happens when you have 1000 Threads? Giving each Thread a time slice of 1ms doesn’t work because the percentage of time you’re spending in context switches will be significant related to the amount of time you’re spending on application work.\nWhat you need is to set a limit on how small a given time slice can be. In the last scenario, if the minimum time slice was 10ms and you have 1000 Threads, the scheduler period needs to increase to 10000ms (10 seconds). What if there were 10,000 Threads, now you are looking at a scheduler period of 100000ms (100 seconds). At 10,000 threads, with a minimal time slice of 10ms, it takes 100 seconds for all the Threads to run once in this simple example if each Thread uses its full time slice.\nBe aware this is a very simple view of the world. There are more things that need to be considered and handled by the scheduler when making scheduling decisions. You control the number of Threads you use in your application. When there are more Threads to consider, and IO-Bound work happening, there is more chaos and nondeterministic behavior. Things take longer to schedule and execute.\nThis is why the rule of the game is “Less is More”. Less Threads in a Runnable state means less scheduling overhead and more time each Thread gets over time. More Threads in a Runnable state mean less time each Thread gets over time. That means less of your work is getting done over time as well.\n# 7. Find the balance There is a balance you need to find between the number of cores you have and the number of Threads you need to get the best throughput for your application. When it comes to managing this balance, Thread pools were a great answer. I will show you in part II that this is no longer necessary with Go. I think this is one of the nice things Go did to make multithreaded application development easier.\nPrior to coding in Go, I wrote code in C++ and C# on NT. On that operating system, the use of IOCP (IO Completion Ports) thread pools were critical to writing multithreaded software. As an engineer, you needed to figure out how many Thread pools you needed and the max number of Threads for any given pool to maximize throughput for the number of cores that you were given.\nWhen writing web services that talked to a database, the magic number of 3 Threads per core seemed to always give the best throughput on NT. In other words, 3 Threads per core minimized the latency costs of context switching while maximizing execution time on the cores. When creating an IOCP Thread pool, I knew to start with a minimum of 1 Thread and a maximum of 3 Threads for every core I identified on the host machine.\nIf I used 2 Threads per core, it took longer to get all the work done, because I had idle time when I could have been getting work done. If I used 4 Threads per core, it also took longer, because I had more latency in context switches. The balance of 3 Threads per core, for whatever reason, always seemed to be the magic number on NT.\nWhat if your service is doing a lot of different types of work? That could create different and inconsistent latencies. Maybe it also creates a lot of different system-level events that need to be handled. It might not be possible to find a magic number that works all the time for all the different work loads. When it comes to using Thread pools to tune the performance of a service, it can get very complicated to find the right consistent configuration.\n# 8. Cache lines Accessing data from main memory has such a high latency cost (100~300 clock cycles) that processors and cores have local caches to keep data close to the hardware threads that need it. Accessing data from caches have a much lower cost (3~40 clock cycles) depending on the cache being accessed. Today, one aspect of performance is about how efficiently you can get data into the processor to reduce these data-access latencies. Writing multithreaded applications that mutate state need to consider the mechanics of the caching system.\nData is exchanged between the processor and main memory using cache lines. A cache line is a 64-byte chunk of memory that is exchanged between main memory and the caching system. Each core is given its own copy of any cache line it needs, which means the hardware uses value semantics. This is why mutations to memory in multithreaded applications can create performance nightmares.\nReferences:\nScheduling In Go : Part I - OS Scheduler\nGuide to the “Cpu-Bound” and “I/O Bound” Terms | Baeldung on Computer Science\nRelated articles:\n多核 CPU 之 Hyper Threading | 橘猫小八的鱼\n并发学习之 Context Switching | 橘猫小八的鱼\n并发学习之线程进程及Hyper-Threading | 橘猫小八的鱼\n","date":"2023-05-28T17:48:19Z","permalink":"https://blog.yorforger.cc/p/go-scheduler-1/","title":"Go Scheduler (1)"},{"content":" # 1. CPU structure Single core CPU:\nThe CPU core consists of three parts: ALU, CU and Memory (Register + Cache), The multiple cores CPU has more than one core (ALU, CU, Memory (Register + Cache)) to execute instructions:\n# 2. Hyper-threading A single physical core with hyper-threading or simultaneous multithreading technology appears as two logical cores to an operating system. The CPU is still a single CPU, so it’s a little bit of a cheat. This can speed things up somewhat — if one virtual CPU is stalled and waiting, the other virtual CPU can borrow its execution resources.\nMost processors can use a process called simultaneous multithreading or, if it’s an Intel processor, Hyper-threading (the two terms mean the same thing) to split a core into virtual cores, which are called threads. For example, AMD CPUs with four cores use simultaneous multithreading to provide eight threads, and most Intel CPUs with two cores use Hyper-threading to provide four threads.\nSome apps take better advantage of multiple threads than others. Lightly-threaded apps, like games, don\u0026rsquo;t benefit from a lot of cores, while most video editing and animation programs can run much faster with extra threads.\nNote: Strictly speaking, only Intel processors have hyper-threading, however, the term is sometimes used colloquially to refer to any kind of simultaneous multithreading.\nThe Windows Task Manager shows this fairly well. Here, for example, you can see that this system has one actual CPU (socket) and 8 cores. Simultaneous multithreading makes each core look like two CPUs to the operating system, so it shows 16 logical processors.\n# Logical core vs OS thread OS Thread（操作系统线程）：\n线程由操作系统内核管理，它可以调度线程在不同的 CPU 核心或逻辑处理器上运行。 线程的调度和管理涉及 context switching, priority scheduling Hyper-threading（超线程）：\nHyper-threading 是 Intel 提供的一种硬件级别的技术，它允许单个物理 CPU 核心模拟出两个逻辑处理器。 当启用 Hyper-threading 时，操作系统会看到比实际物理核心数更多的处理器。例如，一个拥有 4 个物理核心的 CPU 可能会显示为 8 个逻辑处理器。 在操作系统管理线程的过程中，它会将多个 OS thread 分配给可用的 CPU 核心，包括通过 Hyper-threading 技术创建的逻辑处理器。这个分配过程考虑了多个因素，包括线程的优先级、CPU 亲和性（affinity）、以及核心的当前负载情况。因此，操作系统线程与 Hyper-threading 是协同工作的两个不同层面的概念：一个属于软件层面（操作系统管理），另一个属于硬件层面（CPU 架构）。\n参考:\nCPU Basics: What Are Cores, Hyper-Threading, and Multiple CPUs? Differences Between Core and CPU | Baeldung on Computer Science What Is a CPU Core? A Basic Definition | Tom\u0026rsquo;s Hardware ","date":"2023-05-28T15:47:18Z","permalink":"https://blog.yorforger.cc/p/hyper-threading-physical-threads/","title":"Hyper-Threading \u0026 Physical Threads"},{"content":" # 1. Javascript Taken from JavaScript | MDN but this applies for all language with GC:\nLow-level languages like C, have manual memory management primitives such as malloc() and free(). In contrast, JavaScript automatically allocates memory when objects are created and frees it when they are not used anymore (garbage collection). This automaticity is a potential source of confusion: it can give developers the false impression that they don\u0026rsquo;t need to worry about memory management.\nRegardless of the programming language, the memory life cycle is pretty much always the same:\nAllocate the memory you need Use the allocated memory (read, write), each variable exists as long as there are references to it. Release the allocated memory when it is not needed anymore (Usually done by GC) The second part is explicit in all languages. The first and last parts are explicit in low-level languages but are mostly implicit in high-level languages like JavaScript.\n# 1.1. Allocation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 const n = 123; // allocates memory for a number const s = \u0026#34;azerty\u0026#34;; // allocates memory for a string const o = { a: 1, b: null, }; // allocates memory for an object and contained values // (like object) allocates memory for the array and // contained values const a = [1, null, \u0026#34;abra\u0026#34;]; function f(a) { return a + 2; } // allocates a function (which is a callable object) // function expressions also allocate an object someElement.addEventListener( \u0026#34;click\u0026#34;, () =\u0026gt; { someElement.style.backgroundColor = \u0026#34;blue\u0026#34;; }, false, ); # 1.2. Using values Using values basically means reading and writing in allocated memory. This can be done by reading or writing the value of a variable or an object property or even passing an argument to a function.\n# 1.3. Release when the memory is not needed anymore The majority of memory management issues occur at this phase. The most difficult aspect of this stage is determining when the allocated memory is no longer needed.\nLow-level languages require the developer to manually determine at which point in the program the allocated memory is no longer needed and to release it.\nSome high-level languages, such as JavaScript, utilize a form of automatic memory management known as garbage collection (GC). The purpose of a garbage collector is to monitor memory allocation and determine when a block of allocated memory is no longer needed and reclaim it.\nLearn more: Memory management - JavaScript | MDN\n# 2. Python Memory management in Python involves a private heap containing all Python objects and data structures. The management of this private heap is ensured internally by the Python memory manager. The Python memory manager has different components which deal with various dynamic storage management aspects, like sharing, segmentation, preallocation or caching.\nEverything is an object in Python, even types such as int and str.\nLearn more: Memory Management — Python 3.12.0 documentation\n# 3. Golang From a correctness standpoint, you don\u0026rsquo;t need to know. Each variable in Go exists as long as there are references to it. The storage location chosen by the implementation is irrelevant to the semantics of the language.\nThe storage location does have an effect on writing efficient programs. When possible, the Go compilers will allocate variables that are local to a function in that function\u0026rsquo;s stack frame. However, if the compiler cannot prove that the variable is not referenced after the function returns, then the compiler must allocate the variable on the garbage-collected heap to avoid dangling pointer errors. Also, if a local variable is very large, it might make more sense to store it on the heap rather than the stack.\nIn the current compilers, if a variable has its address taken, that variable is a candidate for allocation on the heap. However, a basic escape analysis recognizes some cases when such variables will not live past the return from the function and can reside on the stack.\n# 4. Java Learn more: https://davidzhu.xyz/post/java/basics/005-memory-structure/\n# 5. C/C++ C has three different pools of memory:\nstatic: global variable storage, permanent for the entire run of the program. stack: local variable storage (automatic, continuous memory). heap: dynamic storage (large pool of memory, not allocated in contiguous order). # 5.1. Static memory Static memory persists throughout the entire life of the program, and is usually used to store things like global variables, or variables created with the static clause. If a variable is declared outside of a function, it is considered global, meaning it is accessible anywhere in the program. Global variables are static, and there is only one copy for the entire program. Inside a function the variable is allocated on the stack. It is also possible to force a variable to be static using the static clause. For example, the same variable created inside a function using the static clause would allow it to be stored in static memory.\n1 static int theforce; # 5.2. Stack memory The stack is used to store variables used on the inside of a function (including the main() function). It’s a LIFO, “Last-In,-First-Out”, structure. Every time a function declares a new variable it is “pushed” onto the stack. Then when a function finishes running, all the variables associated with that function on the stack are deleted, and the memory they use is freed up. This leads to the “local” scope of function variables.\nNote that there is generally a limit on the size of the stack – which can vary with the operating system (for example OSX currently has a default stack size of 8MB). If a program tries to put too much information on the stack, stack overflow will occur. Stack overflow happens when all the memory in the stack has been allocated, and further allocations begin overflowing into other sections of memory. Stack overflow also occurs in situations where recursion is incorrectly used.\nthe stack grows and shrinks as variables are created and destroyed stack variables only exist whilst the function that created them exists # 5.3. Heap Memory The heap is the diametrical opposite of the stack. The heap is a large pool of memory that can be used dynamically – it is also known as the “free store”. This is memory that is not automatically managed in C/C++ – you have to explicitly allocate (using functions such as malloc), and deallocate (e.g. free) the memory. Failure to free the memory when you are finished with it will result in what is known as a memory leak – memory that is still “being used”, and not available to other processes. Unlike the stack, there are generally no restrictions on the size of the heap (or the variables it creates), other than the physical size of memory in the machine. Variables created on the heap are accessible anywhere in the program.\n# 6. Conclusion Most of languages are designed with stack and heap, the concept of stack and heap are not mentioned in Javascript, but some concepts like the function stack frame, heap are shared among the modern languages designs. Our goal is to grab the lifetime of objects so that can write good and robust codes, not stack and heap.\nReferences:\nMemory in C – the stack, the heap, and static – The Craft of Coding When will a string be garbage collected in java - Stack Overflow Choosing a GC Algorithm in Java | Baeldung Golang Memory Management: Allocation Efficiency in Go Services Memory Management — Python 3.11.3 documentation Memory Management in Python - Honeybadger Developer Blog CPython 🚀 Visualizing memory management in Golang | Technorage methods - Is Java \u0026ldquo;pass-by-reference\u0026rdquo; or \u0026ldquo;pass-by-value\u0026rdquo;? - Stack Overflow Stack vs heap allocation of structs in Go, and how they relate to garbage collection - Stack Overflow python - How do I pass a variable by reference? - Stack Overflow Garbage Collector Design 🚀 Demystifying memory management in modern programming languages | Technorage ","date":"2023-05-27T19:59:17Z","permalink":"https://blog.yorforger.cc/p/c-go-java-python%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84%E5%8F%8A%E5%AF%B9%E6%AF%94/","title":"C Go Java Python内存结构及对比"},{"content":" # 1. Thread A thread is a segment or part of a process that executes some tasks of the process. A process can have multiple threads which can run concurrently within the process. Each thread has its own thread stack but multiple threads of a process share a common heap area of that process.\n# 1.1. Thread stack Each thread has its own call stack, \u0026ldquo;call stack\u0026rdquo; and \u0026ldquo;thread stack\u0026rdquo; are the same thing. Calling it a \u0026ldquo;thread stack\u0026rdquo; just emphasizes that the call stack is specific to the thread.\nThe stack is used to store variables used on the inside of a function (including the main() function). It’s a LIFO, “Last-In,-First-Out”, structure. Every time a function declares a new variable it is “pushed” onto the stack. Then when a function finishes running, all the variables associated with that function on the stack are deleted, and the memory they use is freed up. This leads to the “local” scope of function variables. The stack is a special region of memory, and automatically managed by the CPU – so you don’t have to allocate or deallocate memory. Stack memory is divided into successive frames where each time a function is called, it allocates itself a fresh stack frame.\nNote that there is generally a limit on the size of the stack – which can vary with the operating system (for example OSX currently has a default stack size of 8MB). If a program tries to put too much information on the stack, stack overflow will occur. Stack overflow happens when all the memory in the stack has been allocated, and further allocations begin overflowing into other sections of memory. Stack overflow also occurs in situations where recursion is incorrectly used.\n# 2. Process A program is a set of instructions. It is stored on a disk of a computer and hence it is Passive. When the same program is loaded into the main memory and the OS assigns some heap memory to this program(application) is under execution is called a Process. Hence a process is a program under execution. So we can say it is Active. A process can create child processes by using the fork system calls.\n# 3. Relationship between a thread and a CPU core A CPU core is a physical processing unit in a computer’s central processing unit (CPU) that can execute instructions independently. A thread, on the other hand, is a unit of execution within a process, which represents a sequence of instructions that can be executed independently by a CPU.\nIn general, the number of threads that can be executed simultaneously on a CPU is limited by the number of cores available in the CPU. Each core can execute one thread at a time, so having multiple cores allows for multiple threads to be executed in parallel, potentially leading to improved performance.\nHowever, the relationship between threads and CPU cores is more complex than just one-to-one mapping.\nIn modern computer systems, threads can be scheduled dynamically on different cores by the operating system, and a single core can switch between multiple threads in order to maximize the utilization of available resources and CPU cores.\nAdditionally, some systems may also use techniques such as hyper-threading, where a single physical core is treated as multiple virtual cores, potentially allowing for even more threads to be executed simultaneously.\nNote that **simultaneous not equals to parallel. **\nFrom this can also see the importance of those basic undergraduate courses, the principles of computer composition of a lot of content, including the CPU architecture, registers, buses, memory structure, how the CPU reads commands from the registers, which provides the basis for future operating system courses. For example, now we are learning about threads, processes, which are all part of the operating system curriculum, and hyper-threading, if you don\u0026rsquo;t know how the CPU handles instructions and how it waits for the bus to send data, how can you understand the interrupt system very well? Golang is very popular now, it is very good at concurrency, Goroutine is very lightweight, but why is goroutine lightweight? You\u0026rsquo;re probably going to get asked that in an interview, right? These are context switches, and you can\u0026rsquo;t understand why goroutines are so powerful without learning the above, but that\u0026rsquo;s just one example. This is just one example. Just one concurrency problem, and that\u0026rsquo;s a lot of knowledge and lessons. The rest of the course such as the network, compilation principles, are very important, may not have an immediate effect, but they will be the future to support you the most solid foundation of the building.\nRelated article:\nContext Switching - David\u0026rsquo;s Blog\nHyper-Threading \u0026amp; Physical Threads - David\u0026rsquo;s Blog\nReferences:\nProcess and Thread Context Switching, Do You Know the Difference? java - Difference between \u0026ldquo;call stack\u0026rdquo; and \u0026ldquo;thread stack\u0026rdquo; - Stack Overflow Memory in C – the stack, the heap, and static – The Craft of Coding ","date":"2023-05-27T18:05:16Z","permalink":"https://blog.yorforger.cc/p/thread-stack-and-cpu-cores/","title":"Thread Stack and CPU Cores"},{"content":" # 1. Context switch In a CPU, the term \u0026ldquo;context\u0026rdquo; refers to the data in the registers and program counter (PC) at a specific moment in time. A register holds the current CPU instruction. A program counter, also known as an instruction address register, is a small amount of fast memory that holds the address of the instruction to be executed immediately after the current one.\nIn computing, a context switch is the process of storing the state of a process or thread, so that it can be restored and resume execution at a later point, and then restoring a different, previously saved, state. Two steps, the first step is to store the state of the thread and then restore the state of another.\n# 2. Two data structure: PCB \u0026amp; TCB The \u0026lsquo;state\u0026rsquo; mentioned above is thread or process related information, stored in PCB (Process) and TCB (Thread) respectively.\n# 2.1 Process control block (PCB) A process control block (PCB) contains information about the process, i.e. registers, PID, priority, etc. The process table is an array of PCBs, that means logically contains a PCB for all of the current processes in the system.\nProcess State – new, ready, running, waiting, dead; Process Number (PID) – unique identification number for each process (also known as Process ID); Program Counter (PC) – a pointer to the address of the next instruction to be executed for this process; CPU Registers – register set where process needs to be stored for execution for running state; # 2.2 Thread control block (TCB) An example of information contained within a TCB is:\nThread Identifier: Unique id (tid) is assigned to every new thread Stack pointer: Points to thread\u0026rsquo;s stack in the process Program counter (PC): Points to the current program instruction of the thread State of the thread (running, ready, waiting, start, done) Thread\u0026rsquo;s register values Pointer to the Process control block (PCB) of the process that the thread lives on # 3. Cost of context switch Switching from one process to another requires a certain amount of time for doing the administration – saving and loading registers and memory maps, updating various tables and lists, etc.\nFor example, in the Linux kernel, context switching involves loading the corresponding process control block (PCB) stored in the PCB table in the kernel stack to retrieve information about the state of the new process. CPU state information including the registers, stack pointer, and program counter as well as memory management information like segmentation tables and page tables (unless the old process shares the memory with the new) are loaded from the PCB for the new process. To avoid incorrect address translation in the case of the previous and current processes using different memory, the translation lookaside buffer (TLB) must be flushed. This negatively affects performance because every memory reference to the TLB will be a miss because it is empty after most context switches.\nFurthermore, analogous context switching happens between user threads, notably green threads, and is often very lightweight, saving and restoring minimal context. In extreme cases, such as switching between goroutines in Go, a context switch is equivalent to a coroutine yield, which is only marginally more expensive than a subroutine call.\n# 4. When context switch happens System calls: when a process makes any system calls, the OS switches the mode of the kernel and saves that process in context, and executes the system call.\nInterrupt handling: Modern architectures are interrupt driven. This means that if the CPU requests data from a disk, for example, it does not need to busy-wait until the read is over; it can issue the request (to the I/O device) and continue with some other task. When the read is over, the CPU can be interrupted (by a hardware in this case, which sends interrupt request to PIC) and presented with the read. For interrupts, a program called an interrupt handler is installed, and it is the interrupt handler that handles the interrupt from the disk.\nUser and Kernel Mode switching: this trigger is used when the OS needed to switch between the user mode and kernel mode.\n# 5. Performance Context switching itself has a cost in performance, due to running the task scheduler, TLB flushes, and indirectly due to sharing the CPU cache between multiple tasks. Switching between threads of a single process can be faster than between two separate processes, because threads share the same virtual memory maps, so a TLB flush is not necessary.\n# 6. Conclusion program counter (PC): processor register, stores the address of next instruction to be executed. context switch: store state, restore state causes of context siwtch system call interrupt handling: CPU requests data from a disk References:\nContext switch Process control block Thread control block Program counter Context Switch in Operating System - GeeksforGeeks Scheduling In Go : Part I - OS Scheduler ","date":"2023-05-27T16:29:15Z","permalink":"https://blog.yorforger.cc/p/context-switching/","title":"Context Switching"},{"content":"最近看线程总是看到 goroutine managed by go runtime, \u0026hellip; runtime, 也看到问题 Does Go has a runtime? 刚好Go在官方FAQs有回答, 就在这记录一下,\nDoes Go have a runtime?\nGo does have an extensive library, called the runtime, that is part of every Go program. The runtime library implements garbage collection, concurrency, stack management, and other critical features of the Go language. Although it is more central to the language, Go\u0026rsquo;s runtime is analogous to libc, the C library.\nIt is important to understand, however, that Go\u0026rsquo;s runtime does not include a virtual machine, such as is provided by the Java runtime. Go programs are compiled ahead of time to native machine code (or JavaScript or WebAssembly, for some variant implementations). Thus, although the term is often used to describe the virtual environment in which a program runs, in Go the word “runtime” is just the name given to the library providing critical language services.\n所以, runtime在go里指的就是它的标准库,\n另外说一下什么是runtime library, it\u0026rsquo;s a collection of functions, 它是static lib和dynamic lib的集合, 运行时是什么? 即程序运行的时候, 所以不要觉得编译的链接阶段就把静态库和我们的代码链接到一起了, 就认为静态库不属于runtime library, 难道你运行的时候不用静态库里的printf函数吗? 静态动态库只是和程序结合的时间不一样, 但运行的时候都会用到,\n所以C Runtime Library里的函数就很多了, 就是我们能用到的那些C的头文件里的函数呗, 你如: printf, malloc, abs, min, assert, 可以去微软看看他们的C Library的实现, 感受下微软实现的Runtime Library的函数都是哪些: C runtime library reference | Microsoft Learn\n那libc是什么? 就是C的标准库, 或者说C的运行时库, 其实这你都可以认为是同义词, 不用太多纠结, 关键是知道编译的过程做了什么, 没人非要你区分哪个库具体包含啥, 重要的是分清楚静态库.a, lib, 动态库.so, .dll参与了编译和运行时的哪个阶段.\nMacOS下C标准库位置: /Library/Developer/CommandLineTools/SDKs/MacOSX12.3.sdk/usr/include\n参考:\nGo FAQs C runtime library reference | Microsoft Learn 想到之前读的石河子的一本书, 程序员的自我修养, 不仅感叹写的真的很好, 只是那时候才大一, 看不懂, 在这再说一下:\n程序如何使用操作系统提供的API(system call)? 在一般的情况下，一种语言的开发环境往往会附带有语言库（Language Library也可以说是标准库,运行时库）。这些库就是对操作系统的API的包装，比如我们经典的C语言版“Hello World”程序，它使用C语言标准库的“printf”函数来输出一个字符串，“printf”函数对字符串进行一些必要的处理以后，最后会调用操作系统提供的API。各个操作系统下，往终端输出字符串的API都不一样，在Linux下，它是一个“write”的系统调用，而在Windows下它是“WriteConsole”系统API。标准库函数(运行库)依赖的是system call。库里面还带有那些很常用的函数，比如C语言标准库里面有很常用一个函数取得一个字符串的长度叫strlen()，该函数即遍历整个字符串后返回字符串长度，这个函数并没有调用任何操作系统的API，也没有做任何输入输出。但是很大一部分库函数(运行库)都是要调用操作系统的API的.\n“Any problem in computer science can be solved by another layer of indirection.”\n每个层次之间都须要相互通信，既然须要通信就必须有一个通信的协议，我们一般将其称为接口（Interface），接口的下面那层是接口的提供者，由它定义接口；接口的上面那层是接口的使用者，它使用该接口来实现所需要的功能.\n运行时库(标准库, static library, dynamic library) 依赖 system call, 它提供头文件(stdio.h, math.h)供我们使用. 所以它很重要, 它在应用层和操作系统中间. 我们使用它提供的接口(printf())和操作系统进行交流(通过system call).\n我们的软件体系中，位于最上层的是应用程序，比如我们平时用到的网络浏览器、Email客户端、多媒体播放器、图片浏览器等。从整个层次结构上来看，开发工具与应用程序是属于同一个层次的，因为它们都使用一个接口，那就是操作系统应用程序编程接口（Application Programming Interface, 就是标准库的头文件）。应用程序接口(头文件)的提供者是运行库，什么样的运行库提供什么样的API，比如Linux下的Glibc库提供POSIX的API；Windows的运行库提供Windows API，最常见的32位Windows提供的API又被称为Win32。\n运行库使用操作系统提供的系统调用接口（System call Interface），系统调用接口在实现中往往以软件中断（Software Interrupt）的方式提供，比如Linux使用0x80号中断作为系统调用接口，Windows使用0x2E号中断作为系统调用接口（从Windows XP Sp2开始，Windows开始采用一种新的系统调用方式）。\n操作系统内核层对于硬件层来说是硬件接口的使用者，而硬件是接口的定义者，硬件的接口定义决定了操作系统内核，具体来讲就是驱动程序如何操作硬件，如何与硬件进行通信。这种接口往往被叫做硬件规格（Hardware Specification），硬件的生产厂商负责提供硬件规格，操作系统和驱动程序的开发者通过阅读硬件规格文档所规定的各种硬件编程接口标准来编写操作系统和驱动程序。\nISO and Standards\n先来看看ISO: The International Organization for Standardization, ISO每年discuss然后得出语言的standards, C99, C11 standards也就是这么来的, standards主要介绍了两部分内容, 语言本身功能和该语言的standard library. 对, 只是介绍, 剩下的实现由其他人完成(glibc, MSVCRT是c standard library的实现). 即ISO出版standards, standard包括standard library,\n比如99年他们讨论的结果就是这个C99 standard: ISO/IEC 9899:1999(E) \u0026ndash; Programming Languages \u0026ndash; C), C99 standard内容包括两部分:\nthe C/C++ features and functionalities;\nthe C/C++ API — a collection of classes, functions and macros that developers use in their C/C++ programs. It is called the Standard Library.\nStatic Library and Dynamic Library\nShared libraries(dynamic library) are.so (or in Windows .dll, or in OS X .dylib) files. All the code relating to the library is in this file, and it is referenced by programs using it at run-time. A program using a shared library only makes reference to the code that it uses in the shared library.\nStatic libraries are .a (or in Windows .lib) files. All the code relating to the library is in this file, and it is directly linked into the program at compile time. A program using a static library takes copies of the code that it uses from the static library and makes it part of the program.\n在这说一下libc.a文件:\n源代码经过预处理, 编译, 汇编后编程一个.o目标文件, 这个目标文件需要和其他libraries(libc.a是由多个.o目标文件压缩得到的)进行最后一步 链接 才能得到最终的可执行文件.\nLinux下, 人们用ar程序把很多目标文件压缩到了静态库libc.a(glibc的c标准库), 比如输入输出有printf.o，scanf.o；文件操作有fread.o，fwrite.o；时间日期有date.o，time.o；内存管理有malloc.o等。\n为什么静态运行库里面一个目标文件只包含一个函数？比如libc.a里面printf.o只有printf()函数、strlen.o只有strlen()函数，为什么要这样组织？\n链接器在链接静态库的时候是以目标文件为单位的。比如我们引用了静态库中的printf()函数，那么链接器就只会把库中包含printf()函数的那个目标文件链接进来，由于运行库有成百上千个函数，如果把这些函数都放在一个目标文件中\u0026hellip;\n如果把整个链接过程比作一台计算机，那么ld链接器就是计算机的CPU，所有的目标文件、库文件就是输入，链接结果输出的可执行文件就是输出，而链接控制脚本正是这台计算机的“程序”，它控制CPU的运行，以“程序”要求的方式将输入加工成所须要的输出结果.\n参考:\nDifference between static and shared libraries? C Static libraries, How they work; Why and how we use them and how to create one Implementation of Standard Library\n我们知道C有个分配内存的函数, 还有创建线程, 输入输出(stdio.h), 这些都必须依赖system call. 所以第三方厂商实现standard library的时候, 要根据不同的OS来实现不同版本, 因为它们有不同的system call.\nGNU/Linux implementation\nThe GNU C Library和glibc是同义词, 是C的runtime library也是standard library.\n其实这个概念有必要澄清一下: runtime library包括static library, daynamic library. runtime library更像是个泛指.\nprintf()和malloc()等函数的声明在头文件stdio.h, stdlib.h中, 而它们的具体实现就在静态库.a中 (libc.a是由多个.o目标文件压缩得到的), 即头文件 + 对应实现 = standard library.\n严格来说library和header不是一个东西, library是目标文件, header就是.h文件. 但是我们都说standard library包括包括printf等函数, 然后这些函数又被声明在这些标准头文件中. 所以这个library,\n标准库为你写代码提供一些现成的方法、宏等使用，这些方法被封装在动态链接库或静态链接库或直接在头文件里，然后由头文件声明供程序员使用。\nMacOS下c的头文件位置: /Library/Developer/CommandLineTools/SDKs/MacOSX12.3.sdk/usr/include\nMac and iOS Implementation\nOn Mac and iOS the C Standard Library implementation is part of libSystem, a core library located in /usr/lib/libSystem.dylib. LibSystem includes other components such as the math library, the thread library and other low-level utilities. 注意我查了一下, mac上的C标准库好像不在说的这个文件夹了, 有人说在/usr/lib/system/我看了下, 但不知道是不是他们实现的C标准库.\nWindows Implementation\nOn Windows the implementation of the Standard Libraries has always been strictly bound to Visual Studio, the official Microsoft compiler. They use to call it C/C++ Run-time Library (CRT) and it covers both implementations.\n参考: https://www.internalpointers.com/post/c-c-standard-library\n","date":"2023-05-25T18:50:14Z","permalink":"https://blog.yorforger.cc/p/%E8%AF%B4%E8%AF%B4c%E7%9A%84%E7%BC%96%E8%AF%91%E5%8A%A8%E6%80%81%E9%9D%99%E6%80%81%E5%BA%93%E5%8F%8Ago%E7%9A%84runtime/","title":"说说C的编译动态静态库及Go的Runtime"},{"content":"有人认为Worker Pool在go里是anti-pattern, 不管怎样, 先实现一个简单版本来帮助理解Worker Pool的概念, 实现之前我们先看看传统的线程池相关的:\n线程池的概念: 预先创建多个线程，线程池里的线程等待处理新来的任务，处理完之后线程并不会被销毁而是等待下一个任务。 使用线程池的原因: 创建和销毁线程都消耗系统资源，如果你的程序需要频繁地创建和销毁线程那这时候就可以考虑使用线程池来提高程序的性能。 注意以上这两点针对是OS级线程的创建销毁, goroutine模型并不是这样, 很轻量, 这也是不需要 wroker poll 的原因,\n另外线程池 thread pool 是不是 worker pool 看法不一, 下面这个描述感觉最贴切:\nThe worker pool pattern is a design in which a fixed number of workers are given a stream of tasks to process in a queue. Go Worker Pools\n至于thread pool, 普遍看法是:\nA thread pool reuses previously created threads to execute current tasks and offers a solution to the problem of thread cycle overhead and resource thrashing. Thread Pools Java\n所以这么看, worker pool也算是一个basic thread pool, 即reuses previously created threads to execute current tasks,\n用Go实现Worker Pool利用的是buffered channel的两个特性:即满的时候写入操作阻塞, 空的时候读取操作阻塞,\n具体方法是提前创建多个goroutines, 然后这些线程持续监听同一个buffered channel, 如果该channel是空的, 那他们就阻塞等待下一个任务的到来:\n1 2 3 4 // var keysChannel = make(chan int, 6) for key := range keysChannel { resultsChannel \u0026lt;- doResearch(key) } 上面这个程序就是遍历一个buffered channel, 如果该channel是空的, 那该段代码就会阻塞, 直到新的数据写入keysChannel ,\n1 2 3 4 5 6 key := 0 for { time.Sleep(time.Second) keysChannel \u0026lt;- key key++ } 上面这段程序即模拟持续产生数据写入到buffered channel: keysChannel, 若keysChannel满了, 那keysChannel \u0026lt;- key就会阻塞,\n之前看到一句话描述线程池说 once the threads finish the task assigned, they make themselves available again for the next task, 这句话在这就很有迷惑性, 其它语言不知道线程池具体怎么实现, 但是在go里根据上面我们讨论的, 根本没有所谓的make themselves (threads) available again for the next task, 即线程一直都在监听, 只是他们完成一个任务后就会接着完成下一个, 直到他们监听的那个buffered channel为空, 这时候他们是阻塞状态,\n另外实现线程池还用到了一个struct, sync.WaitGroup , 主要需要了解它的三个函数, wg.Add(), wg.Done(), wg.Wait(), 其中wg.Add(1)的意思使wg的counter加1, wg.Done()使counter减1, 最后wg.Wait()阻塞直到counter为0, 我们一般的逻辑是创建多个线程的时候, 每创建一个线程就调用wg.Add(1)使counter++, 当线程要被销毁的时候调用wg.Done()是counter\u0026ndash;, 然后在main线程里的最后调用wg.Wait(), 即等待所有线程执行完毕程序结束,\n注意该实现并不是oo的实现,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) var keysChannel = make(chan int, 6) var resultsChannel = make(chan string, 3) func doResearch(key int) string{ // assume this research operation consumes a lot of time time.Sleep(time.Second * 2) return fmt.Sprintf(\u0026#34;One research finished, original key is: %v\u0026#34;, key) } func createWorkerPool(numOfWorker int) { var wg sync.WaitGroup // create numOfWorker of workers for i := 0; i \u0026lt; numOfWorker; i++ { wg.Add(1) // create a goroutine that looks like can be \u0026#34;reused\u0026#34; by listening keysChannel until keysChannel is closed go func(wg *sync.WaitGroup) { // run forever until keysChannel is closed // because when keysChannel is empty, this code get blocked not break loop for key := range keysChannel { resultsChannel \u0026lt;- doResearch(key) } // worker() is a function represents a goroutine, before return, we should make wg-- wg.Done() }(\u0026amp;wg) } wg.Wait() close(resultsChannel) } // when use channel, you have to figure out if you need to remind of other goroutines, // if yes, figure out when to close // when use sync.WaitGroup three operations you need to do: wg--, wg.Add(1), wg.wait() // and don\u0026#39;t forget to do wg--, otherwise the wg.wait() will never return func main() { // 1. keep listening the resultChannel until resultsChannel is closed done := make(chan bool) go func(done chan bool) { for result := range resultsChannel { fmt.Printf(\u0026#34;%v\\n\u0026#34;, result) } done \u0026lt;- true }(done) // 2. keep generating key every 1 sec // Imagine this function can continuously generate a request from a client every sec // And this request will be sent to keysChannel and will be processed by // our one of our workers that keep listening the keysChannel go func() { key := 0 for { time.Sleep(time.Second) keysChannel \u0026lt;- key key++ } }() // 3. create worker pool, and until wg.Wait() in the last line of this function returns createWorkerPool(3) \u0026lt;-done // It\u0026#39;s OK to leave a Go channel open forever and never close it. // When the channel is no longer used, it will be garbage collected. // Close a channel only when it is essential to // inform the receiving goroutines that all data has been transmitted. // close(done) } 参考:\nBuffered Channels and Worker Pools in Go - golangbot.com ","date":"2023-05-25T14:13:13Z","permalink":"https://blog.yorforger.cc/p/go%E5%B9%B6%E5%8F%91%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AAworker-pool/","title":"Go并发学习之实现一个Worker Pool"},{"content":" # 1. Operators as functions Consider the following example:\n1 2 3 int x { 2 }; int y { 3 }; std::cout \u0026lt;\u0026lt; x + y \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; When you see the expression x + y, you can translate this in your head to the function call operator+(x, y) (where operator+ is the name of the function).\nNow consider what happens if we try to add two objects of a program-defined class:\n1 2 3 Mystring string1 { \u0026#34;Hello, \u0026#34; }; Mystring string2 { \u0026#34;World!\u0026#34; }; std::cout \u0026lt;\u0026lt; string1 + string2 \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; What would you expect to happen in this case? The intuitive expected result is “Hello, World!” . However, because Mystring is a program-defined type, the compiler does not have a built-in version of the plus operator that it can use for Mystring operands. So in this case, it will give us an error. In order to make it work like we want, we’d need to write an overloaded function to tell the compiler how the + operator should work with two operands of type Mystring.\n# 2. Overloading the arithmetic operators using friend functions 使用 friend function 的好处是我们可以直接访问类的私有成员, 另外注意 friend function 并不是类的成员函数,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #include \u0026lt;iostream\u0026gt; class Cents { private: int m_cents {}; public: Cents(int cents) : m_cents{ cents } { } // add Cents + Cents using a friend function // 另外friend function并不是类的成员函数, 但你仍可以在类里面实现它 而不是像我们这样在外面实现 // 如果是成员函数重载, 这里只用一个参数就行了, 因为*this会默认为左操作数, 具体文章下面部分会讲 friend Cents operator+(const Cents\u0026amp; c1, const Cents\u0026amp; c2); int getCents() const { return m_cents; } }; // note: this function is not a member function! // 可以看到, 在类里已经声明的性质(friend, static, virtual) 不用再在外面声明了 Cents operator+(const Cents\u0026amp; c1, const Cents\u0026amp; c2) { // use the Cents constructor and operator+(int, int) // we can access m_cents directly because this is a friend function return c1.m_cents + c2.m_cents; } int main() { Cents cents1{ 6 }; Cents cents2{ 2 }; Cents centsSum{ cents1 + cents2 }; std::cout \u0026lt;\u0026lt; \u0026#34;I have \u0026#34; \u0026lt;\u0026lt; centsSum.getCents() \u0026lt;\u0026lt; \u0026#34; cents.\\n\u0026#34;; return 0; } # Not everything can be overloaded as a friend function The assignment (=), subscript ([]), function call (()), and member selection (-\u0026gt;) operators must be overloaded as member functions, because the language requires them to be.\n# 3. Overloading operators using normal functions Using a friend function to overload an operator is convenient because it gives you direct access to the internal members of the classes you’re operating on. However, if you don’t need that access, you can write your overloaded operators as normal functions. 这个normal function既不是friend function, 也不是member function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #include \u0026lt;iostream\u0026gt; class Cents { private: int m_cents{}; public: Cents(int cents) : m_cents{ cents } {} int getCents() const { return m_cents; } }; // note: this function is not a member function nor a friend function! // 注意哦, 这个函数既不是friend function, 也不是成员函数 Cents operator+(const Cents\u0026amp; c1, const Cents\u0026amp; c2) { // use the Cents constructor and operator+(int, int) // we don\u0026#39;t need direct access to private members here return Cents{ c1.getCents() + c2.getCents() }; } int main() { Cents cents1{ 6 }; Cents cents2{ 8 }; Cents centsSum{ cents1 + cents2 }; std::cout \u0026lt;\u0026lt; \u0026#34;I have \u0026#34; \u0026lt;\u0026lt; centsSum.getCents() \u0026lt;\u0026lt; \u0026#34; cents.\\n\u0026#34;; return 0; } Because the normal and friend functions work almost identically (they just have different levels of access to private members), we generally won’t differentiate them. The one difference is that the friend function declaration inside the class serves as a prototype as well. With the normal function version, you’ll have to provide your own function prototype:\nCents.h:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #ifndef CENTS_H #define CENTS_H class Cents { private: int m_cents{}; public: Cents(int cents) : m_cents{ cents } {} int getCents() const { return m_cents; } }; // Need to explicitly provide prototype for operator+ so uses of operator+ in other files know this overload exists Cents operator+(const Cents\u0026amp; c1, const Cents\u0026amp; c2); #endif # 4. Overloading operators using member functions You learned how to overload the arithmetic operators using friend functions. You also learned you can overload operators as normal functions. Many operators can be overloaded in a different way: as a member function.\nOverloading operators using a member function is very similar to overloading operators using a friend function. When overloading an operator using a member function:\nThe overloaded operator must be added as a member function of the left operand. The left operand becomes the implicit *this object. (比如+需要左右两个操作数, 如果你用成员函数来重载它, 那重载函数operator+只需要一个参数即+右边的操作数, 左边的默认为*this, 可以看下面的实现, 注意与上面friend function对比) All other operands become function parameters. 注意与上面friend function的实现做对比:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #include \u0026lt;iostream\u0026gt; class Cents { private: int m_cents {}; public: Cents(int cents) : m_cents { cents } { } // Overload Cents + int Cents operator+ (int value); int getCents() const { return m_cents; } }; // note: this function is a member function! // the cents parameter in the friend version is now the implicit *this parameter // 只有一个参数~, 和friend function一样, 可以访问私有成员, 因为人家是成员函数啊, 肯定可以访问 Cents Cents::operator+ (int value) { return Cents { m_cents + value }; } int main() { Cents cents1 { 6 }; Cents cents2 { cents1 + 2 }; std::cout \u0026lt;\u0026lt; \u0026#34;I have \u0026#34; \u0026lt;\u0026lt; cents2.getCents() \u0026lt;\u0026lt; \u0026#34; cents.\\n\u0026#34;; return 0; } ","date":"2023-05-22T23:03:12Z","permalink":"https://blog.yorforger.cc/p/c-operator-overloading/","title":"C++ Operator Overloading"},{"content":"地图\n地图就是个二维数组board, 在Go里面叫Slice of slices, 该地图(二维数组)存储bool类型, 定义如下和结构如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // height = 4, width = 3 func main() { // Definition board := make([][]bool, height) for i := range board { board[i] = make([]bool, width) } // Test board[2][1] = true for y := range board{ for _, v := range board[y] { fmt.Printf(\u0026#34;%v \u0026#34;, v) } println() } } false false false false false false false true false false false false 打印蛇🐍身体\n蛇的身体由多个节点组成, 节点是个struct类型, 如下:\n1 2 3 4 type node struct { x int y int } 把地图设计成存储布尔类型的数组的目的就是方便打印蛇的身体, 也是实现的关键, 即每次循环board(代表地图的二维数组), 遇到false就打印空白, 遇到true就打印一个黑色方块▪️代表蛇的身体:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func draw() { fmt.Print(\u0026#34;\\033[H\\033[2J\u0026#34;) // original: \u0026#39;for y, _ := range board { ... }\u0026#39; for y := range board { for i, v := range board[y] { if !v \u0026amp;\u0026amp; (food.x != i || food.y != y) { fmt.Print(\u0026#34;□ \u0026#34;) } else if !v \u0026amp;\u0026amp; food.x == i \u0026amp;\u0026amp; food.y == y { fmt.Print(\u0026#34;★ \u0026#34;) } else { fmt.Print(\u0026#34;■ \u0026#34;) } } fmt.Println() } } 这样我们只需要写个函数根据蛇的身体(多个节点)来修改(更新)board里面每个元素的值, 然后再写个函数(也就是上面的draw())根据board来画地图就好了, 注意画地图的时候也画了蛇的身体:\n1 2 3 4 5 6 7 8 9 10 func update() { // 这里访问二维数组的时候第一个是高度, 第二个是宽度 // 即board[0][2]代表第1个数组的第3个元素, 所以我们写成 board[v.y][v.x] = true // 而不是board[v.x][v.y] = true for _, v := range snake{ board[v.y][v.x] = true } // draw map and snake and food draw() } 让蛇动起来\n让蛇动起来的方法有多种, 在这里比如每次reset地图board的数据为false, 然后再根据蛇的身体snake更新board, 但这样效率稍低, 因为每次我们都需要遍历board去reset, 仔细想一下蛇每次移动, 其实就是最前面的头往前移动一格, 尾巴往前移动一格, 中间的身体看着是不变的, 所以我们在实现的时候也是有三步:\n1 根据当前方向direction修改snake第一个元素的坐标, 实现蛇头前进一格 1 2 3 4 5 6 7 8 9 10 11 // change the head position switch direction { case right: snake[0].x++ case left: snake[0].x-- case up: snake[0].y-- case down: snake[0].y++ } 2 遍历snake, 使第i个元素的坐标等于第i-1个元素的坐标(i\u0026gt;=1),中间的身体看着是不动的, 其实每个节点都前进了 1 2 3 4 5 6 7 8 length := len(snake) // \u0026#34;move\u0026#34; all node of snake except the head if length \u0026gt; 1 { // make the value of each (i)th element equal to the (i-1)th value for i := len(snake) - 1; i \u0026gt;= 1; i-- { snake[i] = snake[i-1] } } 3 设置位置在蛇的最后一个节点即尾巴(snake[len - 1].x, snake[len - 1].y)的board元素为false, 因为在第2步尾巴已经变成倒数第二个元素的坐标了, 所以我们要把旧的尾巴坐标位置设置为false 即这样只遍历了snake(一维数组)来实现更新地图.\n总结\n这次的小应用, 熟悉了slice的用法, 也熟悉了很多语法比如:=和var, 尤其是遍历slice以及loop的写法, 比如无限循环可写成:\n1 2 3 for { ... } 其次是导入第三方模块, 如这里读取键盘我们使用的是第三方库, 安装方法很简单, 即在项目根目录执行:\n1 $ go get github.com/eiannone/keyboard 源码: https://gist.github.com/shwezhu/3def0433eb5656deebf07dc32e6eecc1\n","date":"2023-05-20T19:59:09Z","permalink":"https://blog.yorforger.cc/p/%E7%94%A8go%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E8%B4%AA%E5%90%83%E8%9B%87/","title":"用Go实现一个简单的贪吃蛇"},{"content":"经常看到左值右值, 刚好有讲到, 摘取一些记录在这, 原文: 9.2 — Value categories (lvalues and rvalues) – Learn C++\n# 1. Rvalue expression vs Lvalue expression # 1.1. The properties of an expression To help determine how expressions should evaluate and where they can be used, all expressions in C++ have two properties: a type and a value category.\n# 1.2. The type of an expression The type of an expression is equivalent to the type of the value, object, or function that results from the evaluated expression. For example:\n1 2 3 4 int main() { auto v1 { 12 / 4 }; // int / int =\u0026gt; int auto v2 { 12.0 / 4 }; // double / int =\u0026gt; double } Note that the type of an expression must be determinable at compile time (otherwise type checking and type deduction wouldn’t work) \u0026ndash; however, the value of an expression may be determined at either compile time (if the expression is constexpr) or runtime (if the expression is not constexpr).\n# 1.3. The value category of an expression Now consider the following program:\n1 2 3 4 5 int main() { int x{}; x = 5; // valid: we can assign 5 to x 5 = x; // error: can not assign value of x to literal value 5 } How does the compiler know which expressions can legally appear on either side of an assignment statement?\nThe answer lies in the second property of expressions: the value category. The value category of an expression (or subexpression) indicates whether an expression resolves to a value, a function, or an object of some kind.\nPrior to C++11, there were only two possible value categories: lvalue and rvalue.\nIn C++11, three additional value categories (glvalue, prvalue, and xvalue) were added to support a new feature called move semantics.\n# 1.4. Lvalue and rvalue expressions An lvalue (pronounced “ell-value”) is an expression that evaluates to an identifiable object or function (or bit-field).\nEntities (such as an object or function) with identities can be accessed via an identifier, reference, or pointer, and typically have a lifetime longer than a single expression or statement. 这里的identifier就是变量名和reference, pointer并列. 比如int x = 3; , x就是identifier, 3就是object.\nAn rvalue (pronounced “arr-value”) is an expression that is not an l-value. Commonly seen rvalues include literals (except C-style string literals, which are lvalues) and the return value of functions and operators. Rvalues aren’t identifiable (meaning they have to be used immediately), and only exist within the scope of the expression in which they are used.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include \u0026lt;iostream\u0026gt; int return5() { return 5; } int main() { int x{ 5 }; // 5 is an rvalue expression const double d{ 1.2 }; // 1.2 is an rvalue expression int y { x }; // x is a modifiable lvalue expression const double e { d }; // d is a non-modifiable lvalue expression int z { return5() }; // return5() is an rvalue expression (since the result is returned by value) int w { x + 1 }; // x + 1 is an rvalue expression int q { static_cast\u0026lt;int\u0026gt;(d) }; // the result of static casting d to an int is an rvalue expression return 0; } You may be wondering why return5(), x + 1, and static_cast\u0026lt;int\u0026gt;(d) are rvalues: the answer is these expressions produce temporary values that are not identifiable objects.\nNow we can answer the question about why x = 5 is valid but 5 = x is not: an assignment operation requires the left operand of the assignment to be a modifiable lvalue expression, and the right operand to be an rvalue expression. The latter assignment (5 = x) fails because the left operand expression 5 isn’t an lvalue. 这里说一下, 原文中提到lvalue expression又分为modifiable和non-modifiable, 后者就是加上const修饰的, 感兴趣可以去原文查看,\n1 2 3 4 5 6 int main() { int x{}; // Assignment requires the left operand to be a modifiable lvalue expression and the right operand to be an rvalue expression x = 5; // valid: x is a modifiable lvalue expression and 5 is an rvalue expression 5 = x; // error: 5 is an rvalue expression and x is a modifiable lvalue expression } # 1.5. l-value to r-value conversion We said above that the assignment operator expects the right operand to be an rvalue expression, so why does code like this work?\n1 2 3 4 5 int main() { int x{ 1 }; int y{ 2 }; x = y; // y is a modifiable lvalue, not an rvalue, but this is legal } The answer is because lvalues will implicitly convert to rvalues, so an lvalue can be used wherever an rvalue is required.\n# 2. Lvalue reference variables Modern C++ contains two types of references: lvalue references, and rvalue references. Rvalue references are covered in the chapter on move semantics (chapter M).\nAn lvalue reference (commonly just called a reference since prior to C++11 there was only one type of reference) acts as an alias for an existing lvalue (such as a variable). 说白了我们平时使用的引用都是lvalue reference,\n1 2 3 4 5 6 int // a normal int type int\u0026amp; // an lvalue reference to an int object double\u0026amp; // an lvalue reference to a double object int x { 5 }; // x is a normal integer variable int\u0026amp; ref { x }; // ref is an lvalue reference variable that can now be used as an alias for variable x # 2.1. Dangling references When an object being referenced is destroyed before a reference to it, the reference is left referencing an object that no longer exists. Such a reference is called a dangling reference. Accessing a dangling reference leads to undefined behavior.\n# 3. 总结 assignment operation, left operand, right operand rvalue expression, lvalue expression, modifiable lvalue expression, non-modifiable lvalue expression value category of an expression, type of an expression lvalue references, and rvalue references dangling reference ","date":"2023-05-17T11:27:07Z","permalink":"https://blog.yorforger.cc/p/%E5%B7%A6%E5%80%BC%E5%92%8C%E5%8F%B3%E5%80%BC/","title":"左值和右值"},{"content":" Make any member function that does not modify the state of the class object const, so that it can be called by const objects. Default parameters for member functions should be declared in the class definition (in the header file), where they can be seen by whomever #includes the header. For classes used in only one file that aren’t generally reusable, define them directly in the single .cpp file they’re used in. For classes used in multiple files, or intended for general reuse, define them in a .h file that has the same name as the class. Trivial member functions (trivial constructors or destructors, access functions, etc…) can be defined inside the class. Non-trivial member functions should be defined in a .cpp file that has the same name as the class. Member functions defined inside the class definition are implicitly inline. Inline functions are exempt from the one definition per program part of the one-definition rule. So why not put everything in a header file? First, as mentioned above, defining members inside the class definition clutters up your class definition. Second, if you change any of the code in the header, then you’ll need to recompile every file that includes that header. This can have a ripple effect, where one minor change causes the entire program to need to recompile (which can be slow). If you change the code in a .cpp file, only that .cpp file needs to be recompiled! There are a few cases where it might make sense to violate the best practice of putting the class definition in a header and non-trivial member functions in a code file.\nSeparating the class definition and class implementation is very common for libraries that you can use to extend your program. Throughout your programs, you’ve #included headers that belong to the standard library, such as iostream, string, vector, array, and other. Notice that you haven’t needed to add iostream.cpp, string.cpp, vector.cpp, or array.cpp into your projects. Your program needs the declarations from the header files in order for the compiler to validate you’re writing programs that are syntactically correct. However, the implementations for the classes that belong to the C++ standard library are contained in a precompiled file that is linked in at the link stage. You never see the code.\nOutside of some open source software (where both .h and .cpp files are provided), most 3rd party libraries provide only header files, along with a precompiled library file. There are several reasons for this: 1) It’s faster to link a precompiled library than to recompile it every time you need it, 2) a single copy of a precompiled library can be shared by many applications, whereas compiled code gets compiled into every executable that uses it (inflating file sizes), and 3) intellectual property reasons (you don’t want people stealing your code).\nHaving your own files separated into declaration (header) and implementation (code file) is not only good form, it also makes creating your own custom libraries easier. Creating your own libraries is beyond the scope of these tutorials, but separating your declaration and implementation is a prerequisite to doing so.\nReferences: 13.11 — Class code and header files – Learn C++\n","date":"2023-05-16T22:36:06Z","permalink":"https://blog.yorforger.cc/p/c-class-definition-rules/","title":"C++ Class Definition Rules"},{"content":"RAII (Resource Acquisition Is Initialization) is a programming technique whereby resource use is tied to the lifetime of objects with automatic duration (e.g. non-dynamically allocated objects). In C++, RAII is implemented via classes with constructors and destructors. A resource (such as memory, a file or database handle, etc…) is typically acquired in the object’s constructor (though it can be acquired after the object is created if that makes sense). That resource can then be used while the object is alive. The resource is released in the destructor, when the object is destroyed. The primary advantage of RAII is that it helps prevent resource leaks (e.g. memory not being deallocated) as all resource-holding objects are cleaned up automatically.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class IntArray { private: int* m_array{}; int m_length{}; public: IntArray(int length) { assert(length \u0026gt; 0); m_array = new int[static_cast\u0026lt;std::size_t\u0026gt;(length)]{}; m_length = length; } ~IntArray() { // Dynamically delete the array we allocated earlier delete[] m_array; } ... }; The IntArray class above is an example of a class that implements RAII \u0026ndash; allocation in the constructor, deallocation in the destructor. std::string and std::vector are examples of classes in the standard library that follow RAII \u0026ndash; dynamic memory is acquired on initialization, and cleaned up automatically on destruction.\nReferences:\n13.8 — Overlapping and delegating constructors – Learn C++\n13.9 — Destructors – Learn C++\n","date":"2023-05-16T20:32:04Z","permalink":"https://blog.yorforger.cc/p/c-raii-resource-acquisition-is-initialization/","title":"C++ RAII (Resource Acquisition Is Initialization)"},{"content":" # Returns a address of local variable Find codes below:\n1 2 3 4 func f() *int { v := 1 return \u0026amp;v } New to golang from cpp so don\u0026rsquo;t understand why can do this, this is returning a local variable\u0026rsquo;s address 😱. And find a doc talks about this:\nHow do I know whether a variable is allocated on the heap or the stack?\nFrom a correctness standpoint, you don\u0026rsquo;t need to know. Each variable in Go exists as long as there are references to it. The storage location chosen by the implementation is irrelevant to the semantics of the language.\nThe storage location does have an effect on writing efficient programs. When possible, the Go compilers will allocate variables that are local to a function in that function\u0026rsquo;s stack frame. However, if the compiler cannot prove that the variable is not referenced after the function returns, then the compiler must allocate the variable on the garbage-collected heap to avoid dangling pointer errors. Also, if a local variable is very large, it might make more sense to store it on the heap rather than the stack.\nIn the current compilers, if a variable has its address taken, that variable is a candidate for allocation on the heap. However, a basic escape analysis recognizes some cases when such variables will not live past the return from the function and can reside on the stack.\nYou probably think this quite like java\u0026rsquo;s reference type ( java variable has two type: primitive and reference type), but you are wrong, a avriable actually another name of a address in golang. We say that each variable in Go exists as long as there are references to it, means we can reach to the value stored in the address which variable point to. Variable has value, its just a address.\nFor a mental model, you can treat variable names as references, which exists till their scope exists.\nFor implementation, Go\u0026rsquo;s variables are NOT references - for reference, use a pointer.\nThese variables can be allocated on the stack, or on the heap. Both have pros and cons, the compiler decides. For correctness, it does not make any difference. \u0026ldquo;You don\u0026rsquo;t have to know\u0026rdquo;.\nSource: https://www.reddit.com/r/golang/comments/s0m2h9/comment/hs2kvyo/?utm_source=share\u0026utm_medium=web2x\u0026context=3\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func foo1() *int { a := 3 fmt.Println(\u0026amp;a) return \u0026amp;a } func foo2() int { b := 3 fmt.Println(\u0026amp;b) return b } func main() { pa := foo1() fmt.Println(pa) fmt.Println(\u0026#34;------------------\u0026#34;) b := foo2() fmt.Println(\u0026amp;b) } 0x1400011a018 0x1400011a018 ------------------ 0x1400011a038 0x1400011a030 ","date":"2023-05-15T22:52:03Z","permalink":"https://blog.yorforger.cc/p/lifetime-of-a-local-variable-go/","title":"Lifetime of a Local Variable - Go"},{"content":" # 1. Member initialization list A member initialization list can also be used to initialize members that are classes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include \u0026lt;iostream\u0026gt; class A { public: explicit A(int x = 0) { std::cout \u0026lt;\u0026lt; \u0026#34;A \u0026#34; \u0026lt;\u0026lt; x \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } }; class B { private: A m_a{}; public: // call A(int) constructor to initialize member m_a explicit B(int y) : m_a{ y - 1 } { std::cout \u0026lt;\u0026lt; \u0026#34;B \u0026#34; \u0026lt;\u0026lt; y \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } }; int main() { B b{ 5 }; return 0; } A 4 B 5 When variable b is constructed, the B(int) constructor is called with value 5. Before the body of the constructor executes, m_a is initialized, calling the A(int) constructor with value 4. This prints “A 4”. Then control returns back to the B constructor, and the body of the B constructor executes, printing “B 5”.\n这里注意, 如果你不使用member initializer lists对变量进行初始化的话, 那你在构造函数体内只能对他们进行赋值, 而不是初始化, 这就意味着, 人家的构造函数已经调用了, 你又进行了一次赋值, 这就涉及到了拷贝, 析构. 什么? 你不相信, 要验证一下?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 // Cat.h #pragma once #include \u0026lt;string\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;utility\u0026gt; class Cat { private: int age{}; std::string name{}; public: Cat() : age(0){ std::cout \u0026lt;\u0026lt; \u0026#34;Cat: Default constructor is called\u0026#34; \u0026lt;\u0026lt; std::endl; }; Cat(int age, std::string name) : age(age), name(std::move(name)) { std::cout \u0026lt;\u0026lt; \u0026#34;Cat: Constructor with two parameter is called\u0026#34; \u0026lt;\u0026lt; std::endl; } Cat(const Cat\u0026amp; other) { std::cout \u0026lt;\u0026lt; \u0026#34;Cat: Copy Constructor called\u0026#34; \u0026lt;\u0026lt; std::endl; } Cat\u0026amp; operator=(const Cat\u0026amp; other) { std::cout \u0026lt;\u0026lt; \u0026#34;Cat: Assignment Operator called\u0026#34; \u0026lt;\u0026lt; std::endl; return *this; } ~Cat() { std::cout \u0026lt;\u0026lt; \u0026#34;Cat: Destructor is called\u0026#34; \u0026lt;\u0026lt; std::endl; } }; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // main.cpp #include \u0026lt;iostream\u0026gt; #include \u0026lt;utility\u0026gt; #include \u0026#34;Cat.h\u0026#34; class B { private: Cat cat_2{}; public: // call A(int) constructor to initialize member m_a explicit B(int age, const std::string\u0026amp; name){ cat_2 = Cat(age, name); std::cout \u0026lt;\u0026lt; \u0026#34;B: Default constructor is called\u0026#34; \u0026lt;\u0026lt; std::endl; } }; int main() { B b{ 1, \u0026#34;kitty\u0026#34; }; return 0; } Cat: Default constructor is called Cat: Constructor with two parameter is called Cat: Assignment Operator called Cat: Destructor is called B: Default constructor is called Cat: Destructor is called 如果改成下面这样, 其它不变:\n1 2 3 4 5 6 7 explicit B(int age, const std::string\u0026amp; name) : cat{age, name}{ std::cout \u0026lt;\u0026lt; \u0026#34;B: Default constructor is called\u0026#34; \u0026lt;\u0026lt; std::endl; } Cat: Constructor with two parameter is called B: Default constructor is called Cat: Destructor with parameter is called Member initializer lists allow us to initialize our members rather than assign values to them. This is the only way to initialize members that require values upon initialization, such as const or reference members, and it can be more performant than assigning values in the body of the constructor. Member initializer lists work both with fundamental types and members that are classes themselves.\n# 2. Initializer list order Perhaps surprisingly, variables in the initializer list are not initialized in the order that they are specified in the initializer list. Instead, they are initialized in the order in which they are declared in the class.\nFor best results, the following recommendations should be observed:\nDon’t initialize member variables in such a way that they are dependent upon other member variables being initialized first (in other words, ensure your member variables will properly initialize even if the initialization ordering is different). Initialize variables in the initializer list in the same order in which they are declared in your class. This isn’t strictly required so long as the prior recommendation has been followed, but your compiler may give you a warning if you don’t do so and you have all warnings turned on. References:\n13.5 — Constructors – Learn C++ 13.2 — Classes and class members – Learn C++ ","date":"2023-05-15T18:56:01Z","permalink":"https://blog.yorforger.cc/p/c-constructors/","title":"C++ Constructors"},{"content":" # 1. Uninitialized variables Uninitialized variables就是你定义它了但是却没有给它赋过值(即通过assignment操作或者initialization操作), 如下a就是个uninitialized variable:\n1 int a; 为什么要说这个呢, C++编译器在这有点狗, 如果你定义局部变量的时候没有给它值, 如上面的变量a, 那编译器也并不会给a一个特定的初始值, 然后你运行程序编译器也不会报错:\n1 2 3 4 5 6 7 int b; int main() { int a; cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl; // 打印72433004 cout \u0026lt;\u0026lt; b \u0026lt;\u0026lt; endl; // 打印0 return 0; } Java的话是直接不允许你使用未定义的局部变量, 会报错, 如下:\n1 2 3 4 5 public static void main(String []args) { int a; System.out.println(a); } // error: java: variable a might not have been initialized golang就会省事一些, 不论局部全局, 你若定义的时候没有给值, 编译器会自动给个初始值, Golang明确说明:\nVariables declared without an explicit initial value are given their zero value.\n对比完不同的语言, 那我们继续上面的说C++, 为什么这么问题很严重, 这也是所谓的野指针(wild pointer)问题,\n假如这时候你定义的是个指针, 然后没有初始化, 这个时候该指针的值是”随机“的(其实页并不是随机, 即编译器为该指针变量分的那块内存里原本有什么, 那指针的值就是什么), 如果你此时尝试访问这个指针就可能会造成未定义行为(undefined behavior), 因为你不知道该指针的“随机”指向哪块内存, 你修改了该块内存的内容就有可能让你的的程序崩溃crush, 如下:\n1 2 3 int* p; *p = 6; // 有时候程序会崩溃, 有时候程序会正常运行, 取决于p指向的哪块内存地址 Unlike some programming languages, C/C++ does not initialize most variables to a given value (such as zero) automatically. Thus when a variable is given a memory address to use to store data, the default value of that variable is whatever (garbage) value happens to already be in that memory address! A variable that has not been given a known value (usually through initialization or assignment) is called an uninitialized.\n最后对于上面未初始化的变量a打印了一个“随机”的值, 可能会有疑问, 看到一篇文章解释的不错, 分享一下:\n1 2 3 4 5 6 7 8 #include \u0026lt;iostream\u0026gt; int main() { // define an integer variable named x int x; // this variable is uninitialized because we haven\u0026#39;t given it a value // print the value of x to the screen std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // who knows what we\u0026#39;ll get, because x is uninitialized } In this case, the computer will assign some unused memory to x. It will then send the value residing in that memory location to std::cout, which will print the value (interpreted as an integer). But what value will it print? The answer is “who knows!”, and the answer may (or may not) change every time you run the program. When the author ran this program in Visual Studio, std::cout printed the value 7177728 one time, and 5277592 the next. Feel free to compile and run the program yourself (your computer won’t explode).\n# 2. Undefined behavior Using the value from an uninitialized variable is our first example of undefined behavior. Undefined behavior is the result of executing code whose behavior is not well-defined by the C++ language. In this case, the C++ language doesn’t have any rules determining what happens if you use the value of a variable that has not been given a known value. Consequently, if you actually do this, undefined behavior will result.\nCode implementing undefined behavior may exhibit any of the following symptoms:\nYour program produces different results every time it is run. Your program consistently produces the same incorrect result. Your program behaves inconsistently (sometimes produces the correct result, sometimes not). Your program seems like it’s working but produces incorrect results later in the program. Your program crashes, either immediately or later. Your program works on some compilers but not others. Your program works until you change some other seemingly unrelated code. References:\n1.6 — Uninitialized variables and undefined behavior – Learn C++ ","date":"2023-05-15T09:57:02Z","permalink":"https://blog.yorforger.cc/p/%E6%9C%AA%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E5%8F%98%E9%87%8F%E5%92%8C%E6%9C%AA%E5%AE%9A%E4%B9%89%E8%A1%8C%E4%B8%BA/","title":"未初始化的变量和未定义行为"},{"content":"请先阅读(若链接无效可以搜索站内文章): Java中变量(Variables)和引用(References)的区别 - David\u0026rsquo;s Blog\n# 1. JVM Run-Time Data Areas JVN内存结构说的就是Run-Time Data Areas. The Run-Time Data Areas of JVM is vary from different JVM specifications. 不同版本的JVM介绍: https://docs.oracle.com/javase/specs/index.html\n说这个之前缕清JVM的内存结构: JVM memory is divided into multiple parts: Heap Memory, Non-Heap Memory.\n# 1.1. Heap The Java Virtual Machine has a heap that is shared among all Java Virtual Machine threads. The heap is the run-time data area from which memory for all class instances and arrays is allocated.\n# 1.2. Method Area The Java Virtual Machine has a method area that is shared among all Java Virtual Machine threads. The method area is analogous to the storage area for compiled code of a conventional language or analogous to the \u0026ldquo;text\u0026rdquo; segment in an operating system process. It stores per-class structures such as the run-time constant pool, field and method data, and the code for methods and constructors.\nAlthough the method area is logically part of the heap, simple implementations may choose not to either garbage collect or compact it. Method Area属于heap, 但有的JVM实现可能并不会用gc清理这块区域.\n三点:\nMethos Area 属于heap, 当然是线程共享 run-time constant pool属于Method Area run-time constant pool -\u0026gt; Method Area -\u0026gt; Heap # 1.3. Run-Time Constant Pool A run-time constant pool is a per-class or per-interface run-time representation of the constant_pool table in a class file. Run-time constant pool就是class file里的constant_pool table.\n总结: run-time constant pool -\u0026gt; Method Area -\u0026gt; Heap, Each instance of the Java virtual machine has one method area and one heap. These areas are shared by all threads running inside the virtual machine. When the virtual machine loads a class file, it parses information about a type from the binary data contained in the class file. It places this type information into the method area. As the program runs, the virtual machine places all objects the program instantiates onto the heap. 下图描述了一个class file有一个constant pool, 然后被存放在Method Area, 但该图并不准确, 因为Method Area其实是属于Heap的.\n# 1.4. Java Virtual Machine Stacks 以上三个都是heap里面的东西, 现在是stack\nEach Java Virtual Machine thread has a private Java Virtual Machine stack, created at the same time as the thread. A Java Virtual Machine stack stores frames. A Java Virtual Machine stack is analogous to the stack of a conventional language such as C: it holds local variables and partial results, and plays a part in method invocation and return.\n# 1.5. The pc Register Each Java Virtual Machine thread has its own pc (program counter) register. At any point, each Java Virtual Machine thread is executing the code of a single method, namely the current method for that thread.\n以上内容参考自: The Java Virtual Machine Specification, Java SE 19 Edition\nKeep in mind that the Java virtual machine contains a separate runtime constant pool for each class and interface it loads:\n# 1.6. 补充 上面提到了frame和dynamic linking, 下面做个补充:\nFrame: A frame is used to store data and partial results, as well as to perform dynamic linking, return values for methods, and dispatch exceptions. A new frame is created each time a method is invoked. A frame is destroyed when its method invocation completes, whether that completion is normal or abrupt (it throws an uncaught exception). Frames are allocated from the Java Virtual Machine stack of the thread creating the frame. Each frame has its own array of local variables, its own operand stack, and a reference to the run-time constant pool of the class of the current method.\nDynamic Linking: When you compile a Java program, you get a separate class file for each class or interface in your program. Although the individual class files may appear to be independent, they actually harbor symbolic connections to one another and to the class files of the Java API. When you run your program, the Java virtual machine loads your program\u0026rsquo;s classes and interfaces and hooks them together in a process of dynamic linking. As your program runs, the Java virtual machine builds an internal web of interconnected classes and interfaces.\nA class file keeps all its symbolic references in one place, the constant pool. Each class file has a constant pool, and each class or interface loaded by the Java virtual machine has an internal version of its constant pool called the runtime constant pool(可以看出runtime constant pool和class file里的constant pool table是对应的). The runtime constant pool is an implementation-specific data structure that maps to the constant pool in the class file. Thus, after a type is initially loaded, all the symbolic references from the type reside in the type\u0026rsquo;s runtime constant pool.\n以上内容参考自: https://www.artima.com/insidejvm/ed2/linkmod.html\n# 2. String Pool 在哪? 到底存不存在String Pool? 看了一下Java SE 18的JVM实现标准里的Runtime Data Area, 里面并没有提到String Pool这个名词, 但是提到了Method Area中的Runtime Constant Pool, 感觉String应该在这里. 至于String在哪, 毫无因为在heap上, 我们要探究的是string的对象具体在heap的哪个部分(貌似探究这个没什么意义 ummmm) 况且, 所有对象都在heap上,这句话是不变的真理.\nWhenever we declare a variable or create an object, it is stored in the memory. At a high level, Java divides the memory into two blocks: stack and heap. https://www.baeldung.com/java-string-constant-pool-heap-stack\nRuntime Constant Pool(runtime constant pool)在Method Area, 然后Method Area在heap上, 那么Runtime Constant Pool的内容是什么呢? 上面我们提到Runtime Constant Pool就是class file里的constant pool table, 想知道runtime constant pool里有什么, 就要知道class file里constant pool table的格式内容, 去Java SE 18标准看看:\n找到了, 这句话不知道说的啥意思, 但感觉constant pool挺重要, Java Virtual Machine instructions do not rely on the run-time layout of classes, interfaces, class instances, or arrays. Instead, instructions refer to symbolic information in the constant_pool table:\nEach entry in the constant_pool table must begin with a 1-byte tag indicating the kind of constant denoted by the entry, 如下图, 我们去4.4.3看看哪个CONSTANT_String是什么:\nThe CONSTANT_String_info structure is used to represent constant objects of the type String:\n所以, String对象就在这——runtime constant pool.\n# 3. GC and String Before Java 7, the JVM placed the Java String Pool in the PermGen space, which has a fixed size — it can\u0026rsquo;t be expanded at runtime and is not eligible for garbage collection.\nThe risk of interning Strings in the PermGen (instead of the Heap) is that we can get an OutOfMemory error from the JVM if we intern too many Strings.\nFrom Java 7 onwards, the Java String Pool is stored in the Heap space, which is garbage collected by the JVM. The advantage of this approach is the reduced risk of OutOfMemory error because unreferenced Strings will be removed from the pool, thereby releasing memory.\n所以String对象会被GC清理, 却String存储在heap上, 可能说的有点区别, 比如string存储在string pool, 不会直接说存储在heap上.\n","date":"2023-05-14T21:50:26Z","permalink":"https://blog.yorforger.cc/p/java%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/","title":"Java内存结构"},{"content":" # 1. Define 1 2 int x; // define an integer variable named x int y, z; // define two integer variables, named y and z # 2. Variable Assignment After a variable has been defined, you can give it a value (in a separate statement) using the = operator. This process is called copy assignment (or just assignment) for short:\n1 2 int width; // define an integer variable named width width = 5; // copy assignment of value 5 into variable width # 3. Initialization Initialization in C++ is surprisingly complex, so we’ll present a simplified view here.\nThere are 6 basic ways to initialize variables in C++:\n1 2 3 4 5 6 7 8 int a; // no initializer (default initialization) int b = 5; // initializer after equals sign (copy initialization) int c(6); // initializer in parenthesis (direct initialization) // List initialization methods (C++11) (preferred) int d{7}; // initializer in braces (direct list initialization) int e = {8}; // initializer in braces after equals sign (copy list initialization) int f {}; // initializer is empty braces (value initialization) # 3.1. Default Initialization When no initialization value is provided (such as for variable a above), this is called default initialization. In most cases, default initialization leaves a variable with an indeterminate value.\n1 int a; // no initializer (default initialization) # 3.2. Copy Initialization 1 int width = 5; // copy initialization of value 5 into variable width Copy initialization is also used whenever values are implicitly copied or converted, such as when passing arguments to a function by value, returning from a function by value, or catching exceptions by value.\n# 3.3. List Initialization The modern way to initialize objects in C++ is to use a form of initialization that makes use of curly braces: list initialization (also called uniform initialization or brace initialization). List initialization comes in three forms:\n1 2 3 int width {5}; // direct list initialization of value 5 into variable width int height = {6}; // copy list initialization of value 6 into variable height int depth {}; // value initialization (see next section) List initialization has an added benefit: it disallows “narrowing conversions”. This means that if you try to brace initialize a variable using a value that the variable can not safely hold, the compiler will produce an error. For example:\n1 int width { 4.5 }; // error: a number with a fractional value can\u0026#39;t fit into an int References:\n1.4 — Variable assignment and initialization – Learn C++\n","date":"2023-05-14T18:29:32Z","permalink":"https://blog.yorforger.cc/p/c-variable-assignment-and-initialization/","title":"C++ Variable Assignment and Initialization"},{"content":" # 1. Status Switch Remember that each file in your working directory can be in one of two states: tracked or untracked. Tracked files are files that were in the last snapshot, as well as any newly staged files; they can be unmodified, modified, or staged. In short, tracked files are files that Git knows about. As you edit files, Git sees them as modified, because you’ve changed them since your last commit. As you work, you selectively stage these modified files and then commit all those staged changes, and the cycle repeats.\nSome commands are used frequently, the commands below will make a diffference on Git repository but won\u0026rsquo;t change the wok place (file system):\n1 2 3 4 # just untrack file git rm --cached file-name # just unstatge file git restore --staged file-name The commands below will change both work place and Git repository:\n1 2 3 4 # untrack file \u0026amp; rm file git rm file-name # unstatge file \u0026amp; discard uncommitted local changes git restore file-name # 2. Untrack File 1 2 git rm file-name git rm --cached file-name The \u0026ldquo;rm\u0026rdquo; command helps you to remove files from a Git repository. It allows you to not only delete a file from the repository, but also - if you wish - from the filesystem.\nDeleting a file from the filesystem can of course easily be done in many other applications, e.g. a text editor, IDE or file browser. But deleting the file from the actual Git repository is a separate task, for which git rm was made.\n--cached\nRemoves the file only from the Git repository, but not from the filesystem. By default, the git rm command deletes files both from the Git repository as well as the filesystem. Using the --cached flag, the actual file on disk will not be deleted. -r\nRecursively removes folders. When a path to a directory is specified, the -r flag allows Git to remove that folder including all its contents. # 3. Unstage File 1 2 git restore file-name git restore --staged file-name The \u0026ldquo;restore\u0026rdquo; command helps to unstage or even discard uncommitted local changes. On the one hand, the command can be used to undo the effects of git add and unstage changes you have previously added to the Staging Area.\n--staged\nRemoves the file from the Staging Area, but leaves its actual modifications untouched. By default, the git restore command will discard any local, uncommitted changes in the corresponding files and thereby restore their last committed state. With the --staged option, however, the file will only be removed from the Staging Area - but its actual modifications will remain untouched. --source \u0026lt;ref\u0026gt;\nRestores a specific revision of the file. By default, the file will be restored to its last committed state (or simply be unstaged). The --source option, however, allows you to restore the file at a specific revision. 1 2 $ git restore --source 7173808e index.html $ git restore --source master~2 index.html The first example will restore the file as it was in commit #7173808e, while the second one will restore it as it was \u0026ldquo;two commits before the current tip of the master branch\u0026rdquo;.\n# 3. Stage File 1 git add file-name It adds changes to Git\u0026rsquo;s Staging Area (index), the contents of which can then be wrapped up in a new revision with the git commit command.\n# 4. Conclusion Git repository, Work place (Filesystem, Working tree), Staging Area (Index), Stage the file: git add \u0026lt;filename\u0026gt; Unstage: git restore --staged \u0026lt;filename\u0026gt;, git restore \u0026lt;filename\u0026gt; Untrack: git rm --cached \u0026lt;filename\u0026gt;, git rm \u0026lt;filname\u0026gt; References:\nhttps://git-scm.com/book/en/v2/Git-Basics-Recording-Changes-to-the-Repository https://www.git-tower.com/learn/git/commands/git-rm https://www.git-tower.com/learn/git/commands/git-restore ","date":"2023-05-05T09:31:30Z","permalink":"https://blog.yorforger.cc/p/git-rm-git-restore/","title":"git rm \u0026 git restore"},{"content":" # 1. Some Commands You can use xxx --help , man xxx, tldr you-command to check the usage of the command.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ nohup ./server -p 8080 \u0026amp; $ zip -r root.zip root $ git log --all --decorate --oneline --graph # \u0026#34;A Dog\u0026#34; $ ls -lh xxx $ file server $ uname -a $ otool # mac only for xxd $ xxd a.class $ du -sh * # h: human-readable, *: all, s: sort $ df -lh # disk usage, h: human-readable, l: local file system $ chmod +x test.sh $ ipconfig getifaddr en0 # check your ip on Mac $ find themes/source/css -name \u0026#34;*header*\u0026#34; -type f $ grep -nr \u0026#39;ul$\u0026#39; themes/source/css # n: line number, r: recursive # 1.1. netstat The basic netstat command without any options will list all active TCP connections and listening ports. Alternatively, you can use lsof -i on Mac.\nWith the -a option, netstat can show all listening and non-listening sockets.\nWith -p option, netstat can display the process ID (PID) and the program name associated with each network connection, on Linux \u0026amp; Windows.\n1 2 3 4 5 6 7 8 9 # lsof -i on Mac $ netstat -anp | grep 127.0.0.53 ❯ lsof -n -i COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME Raycast 688 David 42u IPv6 0x91b82f8c4899 0t0 TCP *:7265 (LISTEN) redis-ser 726 David 6u IPv4 0x918862f10b09 0t0 TCP 127.0.0.1:6379 (LISTEN) WeChat 1137 David 161u IPv4 0x91bbc886151b1 0t0 TCP 192.168.2.15:51295-\u0026gt;43.130.30.247:http (ESTABLISHED) Arc\\x20He 1269 David 21u IPv4 0x91c91fd81e559 0t0 UDP 192.168.2.15:50307-\u0026gt;142.251.35.174:https e.g., you can use netstat -anp | grep 8080 to find the PID of the process, so that you can kill it by kill PID.\nYou can also use ps aux to find a specific process by name, then use netstat to find the ip and port of the process.\n# 1.2. ps The ps command, short for Process Status, is used to view information related to the processes running in a Linux system.\nps aux: a: show processes for all users, u: display the process\u0026rsquo;s user/owner, x: also show processes not attached to a terminal.\nThe key info listed by ps are:\n1 2 3 4 5 ❯ ps aux USER PID COMMAND PID: Process ID. COMMAND: The command that started this process. e.g., you can use ps aux | grep ./server to find the PID of ./server process, so that you can kill it by kill PID.\n# 1.3. brew services list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ❯ brew services list Name Status User File mongodb-community none David mysql started David ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist redis started David ~/Library/LaunchAgents/homebrew.mxcl.redis.plist unbound none ❯ brew services start mongodb-community ==\u0026gt; Successfully started `mongodb-community` (label: homebrew.mxcl.mongodb-community) ❯ brew services list Name Status User File mongodb-community started David ~/Library/LaunchAgents/homebrew.mxcl.mongodb-community.plist ... # 1.4. systemctl The systemctl command is used to manage systemd services and units, such as starting, stopping, and checking the status of units.\nStarting a Service: To start a service, you can use systemctl start \u0026lt;service-name\u0026gt;. Stopping a Service: To stop a service, you can use systemctl stop \u0026lt;service-name\u0026gt;. Checking the Status of a Service: To check the status of a service, you can use systemctl status \u0026lt;service-name\u0026gt;. The systemd is a service manager for Linux operating systems. It is responsible for initializing and managing system services, daemons, and other processes during the boot process and while the system is running.\n# 2. wget 1 2 3 4 5 6 7 8 # -O write documents to FILE: download the file and save it into install.sh wget -O install.sh https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh # -q quiet, same as above but output nothing wget -qO install.sh https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh # just output the content to stdout, no file saving wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh sh -c \u0026#34;$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; ","date":"2023-05-03T12:40:56Z","permalink":"https://blog.yorforger.cc/p/commands-commonly-used-in-linux/","title":"Commands Commonly Used in Linux"},{"content":"最初想筛选以sh结尾但前面没有.的文件, 对应的Regex为(?\u0026lt;!\\.)sh$, 但是! 刚开始不知道(?!),(?=),(?\u0026lt;!)这种写法规则, 就在想用逻辑与写出以sh结尾的字符但不以.sh结尾, 但这本身就矛盾啊, 以.sh结尾本身就是以sh结尾啊,,, 研究了俩小时, 太菜了,,,, 又快凌晨一点了,\n在StackOverflow看到一个好的回答,\nGiven the string foobarbarfoo:\n1 2 3 4 bar(?=bar) finds the 1st bar (\u0026#34;bar\u0026#34; which has \u0026#34;bar\u0026#34; after it) bar(?!bar) finds the 2nd bar (\u0026#34;bar\u0026#34; which does not have \u0026#34;bar\u0026#34; after it) (?\u0026lt;=foo)bar finds the 1st bar (\u0026#34;bar\u0026#34; which has \u0026#34;foo\u0026#34; before it) (?\u0026lt;!foo)bar finds the 2nd bar (\u0026#34;bar\u0026#34; which does not have \u0026#34;foo\u0026#34; before it) You can also combine them:\n1 (?\u0026lt;=foo)bar(?=bar) finds the 1st bar (\u0026#34;bar\u0026#34; with \u0026#34;foo\u0026#34; before it and \u0026#34;bar\u0026#34; after it) 使用这种 lookaround 句型的时候要注意上面的写法, 即 (?!bar) 就是用来指定某个单词后不含有什么的, 还有一个重要的点如下:\nIn the meantime, if there is one thing you should remember, it is this: a lookahead or a lookbehind does not \u0026ldquo;consume\u0026rdquo; any characters on the string. This means that after the lookahead or lookbehind\u0026rsquo;s closing parenthesis, the regex engine is left standing on the very same spot in the string from which it started looking: it hasn\u0026rsquo;t moved. From that position, then engine can start matching characters again.\n即这些Lookaround只是用来检查check的, 是个附加条件:\n1 $ printf \u0026#34;handle_test.go\u0026#34; | grep -P \u0026#39;test(?=.)go\u0026#39; 凭直觉 'test(?=.)go' 匹配的是含有 test 且其后紧跟着 .go, 可是 \u0026quot;handle_test.go\u0026quot; 并不满足 'test(?=.)go', 因为(?=.) 匹配到了 . 并没有消耗掉它, 要这么写才行: 'test(?=.).go' ,但这样看着很奇怪 并不是这么用的, 其实直接写 'test(?=.go)' 就好了,\n参考:\nAdvanced Regex Tutorial—Regex Syntax\nlookaround - Regex lookahead, lookbehind and atomic groups - Stack Overflow\n","date":"2023-05-02T23:30:28Z","permalink":"https://blog.yorforger.cc/p/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%B9%8B%E5%85%B3%E4%BA%8E-%E7%9A%84%E5%9B%B0%E6%83%91/","title":"正则表达式之关于'(? … )'的困惑"},{"content":"刚开始使用printf \u0026quot;123\\n24567\\n8930\\n234\\n\u0026quot; | egrep '(?\u0026lt;!2)3'提示egrep: repetition-operator operand invalid, 然后查到可以使用参数-P, 如下:\n1 $ printf \u0026#34;123\\n24567\\n8930\\n234\\n\u0026#34; | grep -P \u0026#39;(?\u0026lt;!2)3\u0026#39; 但是仍然报错和上面报错相同, 然后就考虑是不是版本问题,\n1 2 $ grep --v grep (BSD grep, GNU compatible) 2.6.0-FreeBSD 尝试更新,\n1 2 $ brew upgrade grep Error: grep not installed 然后尝试安装,\n1 2 3 4 5 6 $ brew install grep ... All commands have been installed with the prefix \u0026#34;g\u0026#34;. If you need to use these commands with their normal names, you can add a \u0026#34;gnubin\u0026#34; directory to your PATH from your bashrc like: PATH=\u0026#34;/opt/homebrew/opt/grep/libexec/gnubin:$PATH\u0026#34; 安装成功, 然后根据提示往.zshrc添加对应环境变量PATH, 注意$:PATH的含义(追加),\n查看版本,\n1 2 3 4 grep --v grep (GNU grep) 3.10 Packaged by Homebrew Copyright (C) 2023 Free Software Foundation, Inc. 现在支持上面语法grep -P '(?\u0026lt;!2)3' , 问题解决~\n","date":"2023-05-02T22:29:30Z","permalink":"https://blog.yorforger.cc/p/grep%E4%B8%8D%E6%94%AF%E6%8C%81negative-lookahead%E9%97%AE%E9%A2%98/","title":"grep不支持Negative Lookahead问题"},{"content":" # 1. ?, *, ., + + one occurrences of the preceding element * zero occurrence of the preceding element ? zero or one occurrences of the preceding element . matches any single character (exclude newlines) # 2. \\b To match \u0026ldquo;port\u0026rdquo; and \u0026ldquo;ports\u0026rdquo; but not match \u0026ldquo;export\u0026rdquo;, \u0026ldquo;portable\u0026rdquo;, \u0026ldquo;important\u0026rdquo;, use \\bports?\\b. Here, \\b indicates a boundary, where there cannot be other word characters on either side. Word characters refer to English letters, numbers and underscore \u0026ldquo;_\u0026rdquo;. So \\bports?\\b will match \u0026ldquo;port\u0026rdquo; or \u0026ldquo;ports\u0026rdquo; only if they appear as a full word by themselves, not as part of another word.\n1 2 3 $ printf \u0026#34;The port is...\\nTo export it...\\nThere are many ports...\\nportable\u0026#34; | egrep \u0026#39;\\bports?\\b\u0026#39; The port is... There are many ports... Please note that Bash generally uses \u0026ldquo;double quotes\u0026rdquo; to quote strings, in order to avoid potential misparsing, we should use \u0026lsquo;single quotes\u0026rsquo; when quoting any regular expressions.\nYou should always quote regular expressions for grep\u0026ndash;and single quotes are usually best. https://askubuntu.com/a/957504/1690738\n# 3. [] A string of characters enclosed in square brackets ([]) matches any one character in that string. If the first character in the brackets is a caret (^), it matches any character except those in the string. For example, [abc] matches a, b, or c, but not x, y, or z. However, [^abc] matches x, y, or z, but not a, b, or c.\nA minus sign (-) within square brackets indicates a range of consecutive ASCII characters. For example, [0-9] is the same as [0123456789].\nIf any special character, such as backslash (\\), asterisk (*), or plus sign (+), is immediately after the left square bracket, it doesn\u0026rsquo;t have its special meaning and is considered to be one of the characters to match.\nMatch Not match [0-9] 1, 2, 3 12, 01, 22 [a-zA-Z0-9_] a, E, 2, _ ab, 56 END[.] END. END; END DO [ \\t\\n], \u0026ldquo;任何一个空白类字符\u0026rdquo;, 注意[ \\t\\n]是前面故意有个空格, [^ \\t\\n], \u0026ldquo;任何一个非空白类字符\u0026rdquo; 计数用, 表达 「前面的样版重复出现多少次」 的 quantifier:\n{5} 重复 5 次 {3,7} 重复 3 到 7 次 {3,} 重复至少 3 次 {0,}重复出现任意次, 包含 0 次 {1,} 重复出现任意次, 至少 1 次 国内的手机号是11位, 所以要查手机号, 我们可以简单的查找大于等于11位数字的字符串, 下面用7位的举例子,\n1 2 3 $ printf \u0026#34;12345\\n12345678\\n123\\n234567\\n1234567\u0026#34; | egrep \u0026#39;[0-9]{7}\u0026#39; 12345678 1234567 如果想要查找就是准确的7位的数字呢?\n1 2 $ printf \u0026#34;12345\\n12345678\\n123\\n234567\\n1234567\u0026#34; | ggrep -E \u0026#39;\\b[0-9]{7}\\b\u0026#39; 1234567 # 4. $ \u0026amp; ^ $ The pattern hello$ will match the word \u0026ldquo;hello\u0026rdquo; only if it appears at the end of a line.\n1 2 3 $ printf \u0026#34;My name is Jack\\nHi Jack, this is John\\nYou and Jack will come..\\ndfHelloJack\u0026#34; | egrep \u0026#39;Jack$\u0026#39; My name is Jack dfHelloJack ^ Can be used to match the start of a line. For instance, the pattern ^Hello will match any line that begins with \u0026ldquo;Hello.\u0026rdquo;\nFor example, the pattern ^hello$ will only match the exact string \u0026ldquo;hello\u0026rdquo; and not \u0026ldquo;hello world\u0026rdquo; or \u0026ldquo;say hello.\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 $ printf \u0026#34;Hell 123\\n123456\\n123\\n123 hello\u0026#34; | egrep \u0026#39;^\\d+$\u0026#39; 123456 123 $ printf \u0026#34;Hell 123\\n123456\\n123\\n123 hello\u0026#34; | egrep \u0026#39;\\d{3}\u0026#39; Hell 123 123456 123 123 hello $ printf \u0026#34;Hell 123\\n123456\\n123\\n123 hello\u0026#34; | egrep \u0026#39;\\b\\d{3}\\b\u0026#39; Hell 123 123 123 hello # 5. e.g. 在一篇文章当中, 抓出所有 「看起来像是机场代码的字符串」 (例如 TPE 台北, KHH 高雄, LAX 洛杉矶, \u0026hellip; 等等): \\b[A-Z][A-Z][A-Z]\\b。 这里的 [A-Z] 是 [ABCDEFGHIJKLMNOPQRSTUVWXYZ] 的简写, 意思是 「任何一个大写字母」\n如何在一大片文本, 银行账号, 信用卡号\u0026hellip; 当中, 找出看来像是移动电话号码的字符串, 例如 0912345678 或是 0912-345678 或是 0912-345-678 之类的? 09\\d\\d-?\\d\\d\\d-?\\d\\d\\d 这里的 \\d 是 [0-9] 的简写, 这又是 [0123456789] 的简写, 意思是 「任何一个数字字符」\n想要找一组数字 ip (例如 168.95.1.1 或 163.17.9.176 之类的) 印象中在某个文件内曾看过, 但既不记得精确的数字, 也不记得在那个文件看过, 该怎么办? 可以搜索 \\d+\\.\\d+\\.\\d+\\.\\d+ 抓出所有数字 ip。 这里的 + 表示 「前面的东西, 可以重复出现 1 次, 2 次, 3 次, \u0026hellip; 任意次」。 因为 . 在 regexp 当中有特殊的意义: 「任何一个字符」; 但在这里我们就是要找 \u0026ldquo;.\u0026rdquo; 于是在前面加上 \\ 以取消它的特殊意义。\n可以把一个文本文件里面的所有空白列都删掉吗? 这个 regexp 可以抓出所有空白列: ^\\s*$。 在 regexp 最前面放一个 ^ 表示您只对 「出现在一列之首」 的样版有兴趣; 在 regexp 的最后面放一个 $ 表示您只对 「出现在一列之尾」 的样版有兴趣。 \\s 是 [\\t\\n] 的简写, 意思是 「任何一个空白字符」 (包含空格, tab, 等等)。 * 表示 「前面的东西, 可以重复出现 0 次, 1 次, 2 次, \u0026hellip; 任意次」。 这个样版翻译成中文, 就是 「从头到尾都是一片空白的那种列」。\n\\ \\ is used to escape special chars: \\*matches * [abc] any of a, b, or c [^abc] not a, b, or c [a-g] character between a \u0026amp; g [^adgf] 代表不是a, d, g或f的字符 1 2 3 4 5 6 # 忽略大小写 $ printf \u0026#34;Hello\\nHeal\\ntold\\nhello\u0026#34; | egrep \u0026#39;(?i)LL\u0026#39; Hello hello # 找出带后缀的文件 $ ls /etc/ | egrep \u0026#39;\\.\\w+$\u0026#39; References:\nRegexp 是什么东东?\nTop 15 Commonly Used Regex - Digital Fortress\nPerl 常用的 regexp 规则列表\n","date":"2023-05-01T23:14:22Z","permalink":"https://blog.yorforger.cc/p/regex-basics/","title":"Regex Basics"},{"content":"一直听说Spring Boot内嵌了Tomcat, 然后Spring Boot默认把应用打包为Jar, 这到底意味着什么呢, 为啥不直接用War包部署到外部服务器呢(外部的Tomcat)? 不了解War可以看看我的其它War相关的文章或者站内搜War等关键字.\n在StackOverflow上看到一个关于什么是内嵌Tomcat回答, 看看咋说的\n\u0026ldquo;Embedded\u0026rdquo; means that you program ships with the server within it as opposed to a web application being deployed to external server. With embedded server your application is packaged with the server of choice and responsible for server start-up and management.\nFrom the user standpoint the difference is:\nApplication with embedded server looks like a regular java program. You just launch it and that\u0026rsquo;s it. Regular web application is usually a war archive which needs to be deployed to some server Embedding a server is very useful for testing purposes where you can start or stop server at will during the test.\n现在貌似懂点了, 就是Tomcat被被集成到了我们的项目里呗(Tomcat本质也就是个实现了Servlet和JSP标准的程序), 那我们去看看我们打包好的Spring Boot项目(一个Jar文件)里是不是有Tomcat,\n1 2 3 4 5 6 7 $ pwd Downloads/SpringDemo-0.0.1-SNAPSHOT/BOOT-INF/lib/ $ ls | grep tomcat tomcat-embed-core-10.1.7.jar tomcat-embed-el-10.1.7.jar tomcat-embed-websocket-10.1.7.jar 好家伙, 果然在里面,\n然后又看到一篇文章, 说的很清晰, 这里粘贴部分, 分享一下,\nThink about what you would need to be able to deploy your application (typically) on a virtual machine.\nStep 1 : Install Java Step 2 : Install the Web/Application Server (Tomcat/Websphere/Weblogic etc) Step 3 : Deploy the application war What if we want to simplify this?\nHow about making the server a part of the application?\nYou would just need a virtual machine with Java installed and you would be able to directly deploy the application on the virtual machine. Isn’t it cool?\nThis idea is the genesis for Embedded Servers. When we create an application deployable, we would embed the server (for example, tomcat) inside the deployable.\nFor example, for a Spring Boot Application, you can generate an application jar which contains Embedded Tomcat. You can run a web application as a normal Java application!\nEmbedded server implies that our deployable unit contains the binaries for the server (example, tomcat.jar). 这也就是我们上面看到的那几个文件tomcat-embed-core-10.1.7.jar等, 了解更多Spring Boot and Embedded Servers - Tomcat\n打开项目pom.xml可以看到依赖里并没tomcat相关的东西, 但是我们项目里却有tomcat那几个jar包, 这是为啥哩, 其实是因为下面的spring-boot-starter-web依赖tomcat, 自动为我们添加类似spring-boot-starter-tomcat这种了,\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 看一下别人怎么说,\nThe single spring-boot-starter-web dependency can pull in all dependencies related to web development. It also reduces the count of build dependency. The spring-boot-starter-web mainly depends on the following:\n1 2 3 4 5 6 - org.springframework.boot:spring-boot-starter - org.springframework.boot:spring-boot-starter-tomcat - org.springframework.boot:spring-boot-starter-validation - com.fasterxml.jackson.core:jackson-databind - org.springframework:spring-web - org.springframework:spring-webmvc By default, the spring-boot-starter-web contains the below tomcat server dependency given:\n1 2 3 4 5 6 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.0.RELEASE\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;compile\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; The spring-boot-starter-web auto-configures the below things required for the web development:\nDispatcher Servlet Error Page Embedded servlet container Web JARs for managing the static dependencies Difference Between Spring Boot Starter Web and Spring Boot Starter Tomcat - GeeksforGeeks\n说了那么多, 既然是生成的项目jar包内嵌服务器, 也就是说我们只要安装了Java(比如Java8以上), 那我们就可以运行我们的Spring Boot项目了, 不用额外安装Tomcat了,\n首先得先打包我们的项目吧, 在Spring Boot根目录运行mvn clean install\n然后报错 (不出一点岔子就不正常了, 写代码嘛),\n1 [ERROR] Failed to execute goal org.springframework.boot:spring-boot-maven-plugin:2.3.5.RELEASE:repackage (repackage) on project SpringDemo: Execution repackage of goal org.springframework.boot:spring-boot-maven-plugin:2.3.5.RELEASE:repackage failed: Unsupported class file major version 61 -\u0026gt; [Help 1] 然后仔细看了一下发现上面说org.springframework.boot:spring-boot-maven-plugin:2.3.5.RELEASE:repackage failed, 好熟悉, 好像是之前刚开始初始化Spring Boot项目的时候IDEA提示pom.xml有错误(出现曲线下划线那种错, 不是运行报错), 出去强迫症想解决它,然后在StackOverflow在哪查到添加上版本信息就不会提示错误了, 于是我加了一行\u0026lt;version\u0026gt;2.3.5.RELEASE\u0026lt;/version\u0026gt;, 如下:\n1 2 3 4 5 6 7 8 9 \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.5.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 加了版本信息后确实那个红色曲线消失, 也没影响我程序的正常运行(即在IDEA点击运行按钮那个斜三角按钮), 但是使用命令行mvn clean install就出现了刚刚的错误, 所以我把版本信息去掉了然后更新pom.xml, 然后mvn clean install正常运行,\n1 2 3 4 5 6 7 8 ..... [INFO] Installing /Users/David/Codes/IDEA/SpringDemo/target/SpringDemo-0.0.1-SNAPSHOT.jar to /Users/David/.m2/repository/com/choo/SpringDemo/0.0.1-SNAPSHOT/SpringDemo-0.0.1-SNAPSHOT.jar [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 4.091 s [INFO] Finished at: 2023-04-29T16:22:17-03:00 [INFO] ------------------------------------------------------------------------ 注意上面jar包输出路径, 然后进入到生成的jar包的路径里, 运行jar包(运行之前确保你的数据库服务已经正确开启, 可以正常连接, 如你使用了MySQL服务),\n1 java -jar SpringDemo-0.0.1-SNAPSHOT.jar 取决于你项目的REST API怎么设计的, 然后去浏览器访问http://localhost:8080/find/temperature?from=2023-04-01\u0026amp;to=2023-04-02, 得到如下数据, 访问成功:\n1 [{\u0026#34;id\u0026#34;:1,\u0026#34;value\u0026#34;:27.5,\u0026#34;createdDate\u0026#34;:\u0026#34;2023-04-01T14:54:26\u0026#34;},{\u0026#34;id\u0026#34;:2,\u0026#34;value\u0026#34;:28.5,\u0026#34;createdDate\u0026#34;:\u0026#34;2023-04-01T20:42:50\u0026#34;},{\u0026#34;id\u0026#34;:3,\u0026#34;value\u0026#34;:20.6,\u0026#34;createdDate\u0026#34;:\u0026#34;2023-04-02T11:09:26\u0026#34;},{\u0026#34;id\u0026#34;:4,\u0026#34;value\u0026#34;:30.6,\u0026#34;createdDate\u0026#34;:\u0026#34;2023-04-02T15:32:39\u0026#34;}] 你也可以使用wget发送GET请求,\n1 wget \u0026#34;http://localhost:8080/find/temperature?from=2023-04-01\u0026amp;to=2023-04-02\u0026#34; 总结下, 这次讨论我们理解了Jar是什么, 也知道了什么是所谓的内嵌Tomcat, 可以看到运行Jar包并不是像之前讨论War那样, 还得把War部署到Tomcat的webapps目录下, 然后启动Tomcat, 再去访问对应url, 我们直接一个java -jar便可以运行我们的Java Web项目, 可谓是很方便, 但是需要注意如果你的应用用到了数据库, 那你仍需要在你执行该应用的机器上开启对应的数据库服务以及创建对应的表, 这跟tomcat没关系, SpringBoot只是内嵌了Tomcat并不是内嵌了你的数据库啥的,\n然后我们也就知道别人所说的那种, Spring Boot内嵌Tomcat, 然后默认项目打包成Jar而不是War到底是个什么玩意, 即就是把所有依赖和所有我们编写的源代码的字节码.class文件通过约定好的目录结构放到一起, 然后打包成一个jar, 上面说到的依赖比如pom.xml中的各种依赖以jar文件格式放到SpringDemo-0.0.1-SNAPSHOT/BOOT-INF/lib目录下, 然后.class文件都在SpringDemo-0.0.1-SNAPSHOT/BOOT-INF/classes下, 当然可能还包含其他的文件, 这你自己去探索吧,\n最后看一下项目生成的Jar的结构(依赖太多了, 删除了一部分):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . ├── BOOT-INF │ ├── classes │ │ ├── application.properties │ │ ├── com │ │ │ └── choo │ │ │ └── springdemo │ │ │ ├── SpringDemoApplication.class │ │ │ ├── Temperature.class │ │ │ └── TemperatureRepository.class │ │ └── static │ │ └── index.html │ ├── classpath.idx │ ├── layers.idx │ └── lib │ ├── HikariCP-5.0.1.jar │ ├── angus-activation-2.0.0.jar │ ├── antlr4-runtime-4.10.1.jar │ ├── spring-jcl-6.0.7.jar │ ├── spring-jdbc-6.0.7.jar │ ├── spring-orm-6.0.7.jar │ ├── spring-tx-6.0.7.jar │ ├── spring-web-6.0.7.jar │ ├── spring-webmvc-6.0.7.jar │ ├── tomcat-embed-core-10.1.7.jar │ ├── tomcat-embed-el-10.1.7.jar │ ├── tomcat-embed-websocket-10.1.7.jar 其实有时候对于一些概念区别, 读很多文章博客不如去自己实操一遍, 读多了可能觉得自己会了理解了, 但还是会云里雾里, 因为有的说的很泛, 总之别嫌麻烦, 多动手,\n","date":"2023-04-28T15:02:55Z","permalink":"https://blog.yorforger.cc/p/spring-boot%E4%B9%8B%E4%BD%95%E4%B8%BA%E5%86%85%E5%B5%8Ctomcat/","title":"Spring Boot之何为内嵌Tomcat"},{"content":"Web application resources or web application archives are commonly called WAR files. A WAR file is used to deploy a Java EE web application in an application server. Inside a WAR file, all the web components are packed into one single unit. These include JAR files, JavaServer Pages, Java servlets, Java class files, XML files, HTML files, and other resource files that we need for web applications. We can use the Maven WAR plugin to build our project as a WAR file.\n# Step 1: Add a new user with deployment rights to Tomcat To perform a Maven Tomcat deploy of a WAR file you must first set up a user in Tomcat with the appropriate rights. You can do this with an edit of the tomcat-users.xml file, which can be found in Tomcat\u0026rsquo;s conf sub-directory. Add the following entry inside the tomcat-users tag:\n1 \u0026lt;user username=\u0026#34;war-deployer\u0026#34; password=\u0026#34;maven-tomcat-plugin\u0026#34; roles=\u0026#34;manager-gui, manager-script, manager-jmx\u0026#34; /\u0026gt; Save the tomcat-users.xml file and restart the server to have the changes take effect.\n重启Tomcat就是进到Tomcat的bin目录下, 执行startup.sh, ./shutdown.sh, 其实你直接使用startup.sh命令开启Tomcat服务就会加载配置文件了, 上面说的重启是默认你的Tomcat一直处于运行状态. 现在你也应该启动Tomcat服务了, 启动后尝试访问http://localhost:8080/,\n点击后输入上面的username和对应的password, 即可进入管理页面如下:\n# 无法访问 tomcat 主页问题 我在访问Tomcat主页出现了问题, 访问的总是我以前的JSP应用, 我用IDEA开发的, 但我都没打开IDEA, 仍然可以访问到, 真是奇了怪了, 如下:\n然后我就查到了一个博客说需要将Tomcat的首页的工程部署到Tomcat服务器上，我们通过IDEA来操作, 部署步骤如下：\n选择菜单栏“Run\u0026ndash;\u0026gt;Edit Configuration\u0026hellip;\u0026ndash;\u0026gt;Deployment”, 选择右上角绿色“+”，选择“External Source\u0026hellip;”，将Apache-tomcat的webapps目录下的ROOT文件夹添加进来, 下面的Application Context 空着, 删除 ROOT 下面的那个ServletDemo:war exploded, 如下图:\n然后我IDEA上选择的Tomcat服务器不是我现在用的, 我有个旧的Tomcat服务器, 我不知道, 然后IDEA用的一直是那个旧的(但我在上面部署位置的ROOT文件夹选择的是新的Tomcat下的文件), 所以就导致就算部署项目后, 我依然无法访问Tomcat的主页. 所以检查一下你是否选择了正确的Tomcat服务器,\n这样配置好后再在IDEA点击运行, 就可以通过http://localhost:8080/访问 Tomcat 主页了, 之后你关闭IDEA, 直接进入Tomcat根目录的bin下通过执行startup.sh来启动Tomcat.\n有时候你会遇到其他情况, 比如8080端口被占用, 这时候解决办法也很简单\n1 2 3 4 # 查看PID lsof -n -i4TCP:8080 # 删除8080端口对应的PID kill -9 PID 说了那么多终于要进行下一步了,\n# Step 2: Tell Maven about the Tomcat deploy user After you add the war-deployer user to Tomcat, register that username and password in Maven, along with a named reference to the server. The Maven-Tomcat plugin will use this information when it tries to connect to the application server. Edit the settings.xml file and add the following entry within the \u0026lt;server\u0026gt; tag to create the named reference to the server:\n1 2 3 4 5 6 \u0026lt;!-- Configure the Tomcat Maven plugin user --\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;maven-tomcat-war-deployment-server\u0026gt;\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;war-deployer\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;maven-tomcat-plugin\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; 注意, 上面提到的settings.xml文件在Downloads/apache-maven-3.9.1/conf下, 根据你的maven安装目录查找,\n另外这里加的账号密码就是上面在Tomcat添加用户时候的账号密码, 这是因为你进入Tomcat管理页面的时候需要,如果你不提供(下面配置pom.xml也会说到), 那生成war文件的时候maven就会报错,\n# Step 3: Register the tomcat7-maven-plugin in the POM 把打包格式改成war, 即在pom.xml中找到\u0026lt;packaging\u0026gt;标签, 没有的话添加一个, 与\u0026lt;dependencies\u0026gt;标签并列:\n1 2 3 4 5 \u0026lt;groupId\u0026gt;com.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ServletDemo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;ServletDemo\u0026lt;/name\u0026gt; \u0026lt;packaging\u0026gt;war\u0026lt;/packaging\u0026gt; Now that Maven and Tomcat are configured, the next step is to edit the Java web application\u0026rsquo;s POM file to reference the Tomcat Maven plugin.\n1 2 3 4 5 6 7 8 9 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.tomcat.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tomcat7-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;url\u0026gt;http://localhost:8080/manager/text\u0026lt;/url\u0026gt; \u0026lt;path\u0026gt;/rps\u0026lt;/path\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; 运行mvn install tomcat7:deploy生成war的时候总是报错(如果你之前已经生成了War文件, 请记得去Tomcat根目录下的webapp目录下删除一生成的war文件, 否则也会报错, 和下面一样):\n1 2 3 4 5 6 7 [ERROR] Failed to execute goal org.apache.tomcat.maven:tomcat7-maven-plugin:2.0:deploy (default-cli) on project ServletDemo: Cannot invoke Tomcat manager: Broken pipe -\u0026gt; [Help 1] [ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException 加入我们在Tomcat Users里配置的账号密码:\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.tomcat.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tomcat7-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;url\u0026gt;http://localhost:8080/manager/text\u0026lt;/url\u0026gt; \u0026lt;path\u0026gt;/rps\u0026lt;/path\u0026gt; \u0026lt;username\u0026gt;war-deployer\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;maven-tomcat-plugin\u0026lt;/password\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; 注意: 修改pom.xml后需要更新pom.xml\n提示: 点击IDEA软件的右上角有个浮动的更新小按钮即更新, 或者你可以查查命令行maven怎么更新pom.xml文件.\n然后重新运行mvn install tomcat7:deploy, 成功:\n然后去Tomcat根目录的webapps下查看生成的War, 可以看到生成了名为rps的web应用, 即这个名字取决于上面pom.xml填的内容,\n然后我们对比一下生成的war与我们的源代码文件结构:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #源代码 . ├── pom.xml ├── src │ └── main │ ├── java │ │ ├── database │ │ └── servlet │ ├── resources │ │ └── log4j.properties │ └── webapp │ ├── WEB-INF │ ├── hello.jsp │ └── index.jsp # War . ├── META-INF │ ├── MANIFEST.MF │ ├── maven │ │ └── com.example │ │ └── ServletDemo │ └── war-tracker ├── WEB-INF │ ├── classes │ │ ├── database │ │ │ ├── Controller.class │ │ │ ├── DataEntity.class │ │ │ ├── Database.class │ │ │ └── MysqlDatabase.class │ │ ├── log4j.properties │ │ ├── servlet │ │ │ ├── GetDataServlet$1.class │ │ │ └── GetDataServlet.class │ │ └── test │ ├── lib │ │ ├── gson-2.9.0.jar │ │ ├── log4j-1.2.17.jar │ │ ├── mysql-connector-j-8.0.32.jar │ │ └── protobuf-java-3.21.9.jar │ └── web.xml ├── hello.jsp └── index.jsp 对于生成的War文件可以发现所有Java相关的文件都在WEB-INF下, 比如我们编写的Servlet字节码文件和和我们用到的依赖(gson, mysql connector, log4j). 然后仔细看源代码文件结构, 在webapp下也有个WEB-INF, 这下面放的就是我们项目的web.xml, 内容如下(所以这有什么联系呢),\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;web-app xmlns=\u0026#34;http://xmlns.jcp.org/xml/ns/javaee\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\u0026#34; version=\u0026#34;4.0\u0026#34;\u0026gt; \u0026lt;welcome-file-list\u0026gt; \u0026lt;welcome-file\u0026gt;/index.jsp\u0026lt;/welcome-file\u0026gt; \u0026lt;/welcome-file-list\u0026gt; \u0026lt;servlet\u0026gt; \u0026lt;servlet-name\u0026gt;get-data\u0026lt;/servlet-name\u0026gt; \u0026lt;servlet-class\u0026gt;servlet.GetDataServlet\u0026lt;/servlet-class\u0026gt; \u0026lt;/servlet\u0026gt; \u0026lt;servlet-mapping\u0026gt; \u0026lt;servlet-name\u0026gt;get-data\u0026lt;/servlet-name\u0026gt; \u0026lt;url-pattern\u0026gt;/temperature/*\u0026lt;/url-pattern\u0026gt; \u0026lt;url-pattern\u0026gt;/humidity/*\u0026lt;/url-pattern\u0026gt; \u0026lt;/servlet-mapping\u0026gt; \u0026lt;/web-app\u0026gt; # FInal Step: Verify 确保你已经开启Tomcat服务(即使你关闭了IDEA, IDEA和Tomcat是两个东西, IDEA是个IDE会用到Tomcat作为web服务器来部署web app), 然后访问通过http://localhost:8080/访问到Tomcat主页, 这时候你可以在链接🔗后加上/rps即http://localhost:8080/rps/就可以进入到你的那个web网页, 如下:\n# 思考总结 这时候其实我们也就知道了什么是根目录和url中神秘的路径问题, 你看我们若想访问manager页面, 这个页面的url是http://localhost:8080/manager/, 我们访问我们刚部署的页面是http://localhost:8080/rps/, 你看最后的这个路径及/manager, /rps都是tomcat的webapps目录下的文件, 如下:\n1 2 3 4 # David @ tc0db in ~/Downloads/Programs/apache-tomcat-9.0.73/webapps $ ls ROOT examples manager rps.war docs host-manager rps 所以webapp就是所谓的根目录, 我们访问什么都是根据它来的, 可以看到, webapps目录下还有examples等文件夹, 所以我们可以直接通过http://localhost:8080/example/访问. 但是又有个问题, Tomcat的主页也就是是http://localhost:8080/具体在哪呢? 按理说webapps下应该有个index.html文件呀, 可是却空空, 这是怎么回事, 怎么没有按我们上面推导的路径来呢?\n还记不记得当时学习servlet的时候有个web.xml文件, 我们在这个文件里可以配置个welcome标签, 通过这个标签我们就可以直接指定一个html文件作为我们的主页而不是根目录下的index.tml文件, 同样, Tomcat当然也有这个文件 TOMCAT_HOME/conf/web.xml, 搜索welcome找到啦(在tomcat/webapps/ROOT/index.jsp):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026lt;!-- ==================== Default Welcome File List ===================== --\u0026gt; \u0026lt;!-- When a request URI refers to a directory, the default servlet looks --\u0026gt; \u0026lt;!-- for a \u0026#34;welcome file\u0026#34; within that directory and, if present, to the --\u0026gt; \u0026lt;!-- corresponding resource URI for display. --\u0026gt; \u0026lt;!-- If no welcome files are present, the default servlet either serves a --\u0026gt; \u0026lt;!-- directory listing (see default servlet configuration on how to --\u0026gt; \u0026lt;!-- customize) or returns a 404 status, depending on the value of the --\u0026gt; \u0026lt;!-- listings setting. --\u0026gt; \u0026lt;!-- --\u0026gt; \u0026lt;!-- If you define welcome files in your own application\u0026#39;s web.xml --\u0026gt; \u0026lt;!-- deployment descriptor, that list *replaces* the list configured --\u0026gt; \u0026lt;!-- here, so be sure to include any of the default values that you wish --\u0026gt; \u0026lt;!-- to use within your application. --\u0026gt; \u0026lt;welcome-file-list\u0026gt; \u0026lt;welcome-file\u0026gt;index.html\u0026lt;/welcome-file\u0026gt; \u0026lt;welcome-file\u0026gt;index.htm\u0026lt;/welcome-file\u0026gt; \u0026lt;welcome-file\u0026gt;index.jsp\u0026lt;/welcome-file\u0026gt; \u0026lt;/welcome-file-list\u0026gt; 然后怎么覆盖这个home page呢? 刚好看到了下面这个回答, 看来和我们猜想的一样, 如下:\nIn any web application, there will be a web.xml in the WEB-INF/ folder. (别忘了我们之前学习JSP的时候可没少在这个文件夹花时间去配置servlet name和对应的jsp, 每创建一个新的servlet就要在这创建个新的servlet pattern)\nIf you dont have one in your web app, as it seems to be the case in your folder structure, the default Tomcat web.xml is under TOMCAT_HOME/conf/web.xml\nEither way, the relevant lines of the web.xml are\n1 2 3 4 5 \u0026lt;welcome-file-list\u0026gt; \u0026lt;welcome-file\u0026gt;index.html\u0026lt;/welcome-file\u0026gt; \u0026lt;welcome-file\u0026gt;index.htm\u0026lt;/welcome-file\u0026gt; \u0026lt;welcome-file\u0026gt;index.jsp\u0026lt;/welcome-file\u0026gt; \u0026lt;/welcome-file-list\u0026gt; so any file matching this pattern when found will be shown as the home page.\nIn Tomcat, a web.xml setting within your web app will override the default, if present.\nFurther Reading: How do I override the default home page loaded by Tomcat?\n参考:\nStep-by-step Maven Tomcat WAR file deploy example | TheServerSide tomcat启动成功浏览器却无法访问 - 掘金 web applications - How does Tomcat find the HOME PAGE of my Web App? - Stack Overflow Generate a WAR File in Maven | Baeldung ","date":"2023-04-27T21:30:48Z","permalink":"https://blog.yorforger.cc/p/%E6%89%8B%E5%8A%A8%E9%83%A8%E7%BD%B2war%E5%8C%85%E5%88%B0tomcat%E4%B9%8B%E4%BD%95%E4%B8%BAwar/","title":"手动部署War包到Tomcat之何为War"},{"content":" # 问题描述: 这几天弄服务器, 用ssh通过域名连接的时候总是出现超时(域名只有一个A记录即服务器IP), 问题下面是分析思路,\nI\u0026rsquo;m trying to use ssh to connect my server with my domain name ssh root@www.davidzhu.xyz, but I always get timeout. However I can connect my server when use my server ip directly ssh root@144.202.12.32, so I use dig to check the ip address of my domain on DNS, the output is:\n1 2 3 4 5 6 7 dig davidzhu.xyz +short 172.67.210.8 104.21.50.195 dig www.davidzhu.xyz +short 172.67.210.8 104.21.50.195 But I only have one A record on my domain, this is the DNS Record of my domain name (the third one is about the google search console):\n惊不惊讶, dig返回的两个IP地址竟然都与我域名的A记录不同!\n# 更新: 原因是我的域名开启了 Cloudfalre 的 reverse proxy, 通过域名获取到的是 Clouflare 的IP, 无法访问到我服务器真实IP:\nWhen you add your application to Cloudflare, we use this network to sit in between requests and your origin server.\nCloudflare does this by serving as a reverse proxy for your web traffic. All requests to and from your origin flow through Cloudflare and — as these requests pass through our network — we can apply various rules and optimizations to improve security, performance, and reliability.\n关闭 Cloudflare 反向代理就可以通过域名获取到服务器真实IP了:\n此时可以看到现实 DNS Only, 使用 dig xxx.com +short 即可查到真实绑定 ip, 或者 ping 也可以.\n另外通过Cloudflare 域名托管机制(就是现在我们操作的, 中文里都叫托管更合适, 因为他会接管通往我们域名的所有流量), 就是域名被 Cloudflare 托管(Domain Hosting)后, 若想更换域名的 DNS Record, 只需要在上图中的 edit 上修改 A record 就行了, 就是改成你的新的 VPS(服务器) 的 ipv4 地址, 不用去域名服务商那里修改 DNS Record 了, 因为在把域名托管给 Cloudflare 的时候已经在域名提供商那里把域名的 DNS 服务器设置成了 Cloudflare.\n如何让 Cloudfalre 托管我们的域名:: Enable Coudflare Reverse Proxy - David\u0026rsquo;s Blog\n另外如果使用了 cloudflare https 代理, 一般都是先修改域名的 NS 为 cloudfalre 指定的 NS, 但是有些电脑可能因为 DNS cache 问题, 通过 https 访问你的域名可能会失败, 此时应该尝试修改本地 DNS cache, 可以查一下 flush dns cache 的命令,\n注意, DNS cache 一般 chrome 保存了一份, 操作系统也会保存一份, 因此两个地方都要考虑进去,\n关于DNS: DNS Concepts (NameServer(NS), DNS Records and Caching) - David\u0026rsquo;s Blog\n了解更多: HTTPS Works on some computers but not others? - Website, Application, Performance / Security - Cloudflare Community\n其实这样有个好处, 就是别人无法通过你的域名接触到你的服务器, 因为每次请求你的域名, 都要经过 Cloudfalre 中转才能到你的服务器, 另外通过nslookup, dig也无法查到你服务器的真实IP, 返回的都是 Cloudflare 的IP.\nBecause of how Cloudflare works, all traffic to proxied DNS records pass through Cloudflare before reaching your origin server. This means that your origin server will stop receiving traffic from individual visitor IP addresses and instead receive traffic from Cloudflare IP addresses, which are shared by all proxied hostnames.\nCloudflare IPs · Getting started · Learning paths\n","date":"2023-04-26T17:30:22Z","permalink":"https://blog.yorforger.cc/p/%E5%90%AF%E5%8A%A8cloudflare%E4%BB%A3%E7%90%86%E5%AF%BC%E8%87%B4%E6%97%A0%E6%B3%95%E9%80%9A%E8%BF%87%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E5%87%BAa%E8%AE%B0%E5%BD%95%E4%B8%AD%E7%9A%84ip/","title":"启动Cloudflare代理导致无法通过域名解析出A记录中的IP"},{"content":" JAR stands for Java ARchive. It\u0026rsquo;s a file format based on the popular ZIP file format and is used for aggregating many files into one. Although JAR can be used as a general archiving tool, the primary motivation for its development was so that Java applets and their requisite components (.class files, images and sounds) can be downloaded to a browser in a single HTTP transaction, rather than opening a new connection for each piece. This greatly improves the speed with which an applet can be loaded onto a web page and begin functioning. JAR File Overview\n上面的话是在说一些Java程序(比如一些库)会有很多.class文件(因为有很多源代码.java各种类)和其他资源文件如图片, 如果我们想用这个库, 得一个一个下载这个库用到的每个.class文件(依赖之类), 这很麻烦.\n所以Java ARchive出现了, 按照Java ARchive规定的格式压缩文件后(也就是jar包), 你可以将这个jar包直接放进项目然后使用jar包中的类 (注意jar包里可能还会有jar包, 比如有的库也会依赖其他的库), 依赖一般放在jar包的BOOT-INF文件夹的lib目录下。这是我的Spring Boot Web项目形成的Jar包大致结构(我删除了一些文件, 太多了):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 . ├── BOOT-INF │ ├── classes │ │ ├── application.properties │ │ ├── com │ │ │ └── choo │ │ │ └── springdemo │ │ │ └── SpringDemoApplication.class │ │ └── static │ │ └── index.html │ ├── classpath.idx │ └── lib │ ├── jackson-annotations-2.14.2.jar │ ├── spring-web-6.0.7.jar │ ├── spring-webmvc-6.0.7.jar │ ├── tomcat-embed-core-10.1.7.jar │ └── tomcat-embed-websocket-10.1.7.jar ├── META-INF │ ├── MANIFEST.MF │ └── maven │ └── com.choo │ └── SpringDemo │ ├── pom.properties │ └── pom.xml └── org └── springframework └── boot └── loader ├── ClassPathIndexFile.class ├── ExecutableArchiveLauncher.class ├── JarLauncher.class 如果我们要执行jar包的里面的某个class，就可以把jar包放到classpath中：\n1 java -cp ./hello.jar abc.xyz.Hello 这样JVM会自动在hello.jar文件里去搜索某个类。\n因为jar包就是zip包，所以，直接在资源管理器中，找到正确的目录，点击右键，在弹出的快捷菜单中选择“发送到”，“压缩(zipped)文件夹”，就制作了一个zip文件。然后，把后缀从.zip改为.jar，一个jar包就创建成功。\n下面是简单项目编译输出的目录：\n1 2 3 4 5 6 7 8 9 package_sample └─ bin ├─ hong │ └─ Person.class │ ming │ └─ Person.class └─ mr └─ jun └─ Arrays.class 这里需要注意 遇上面输出相比, 压缩后的jar包内第一层目录并不是 bin, 而是package, 在Windows的资源管理器中看，应该长这样：\n就说明打包打得有问题, 因此JVM无法从jar包中查找正确的class, 原因是hong.Person必须按hong/Person.class存放，而不是bin/hong/Person.class。\njar包还可以包含一个特殊的/META-INF/MANIFEST.MF文件，MANIFEST.MF是纯文本，可以指定Main-Class和其它信息。JVM会自动读取这个MANIFEST.MF文件，如果存在Main-Class，我们就不必在命令行指定启动的类名，而是用更方便的命令：\n1 java -jar hello.jar 在大型项目中，不可能手动编写MANIFEST.MF文件，再手动创建zip包。Java社区提供了大量的开源构建工具，例如Maven，可以非常方便地创建jar包\n推荐阅读: How Classes are Found\n参考:\nhttps://docs.oracle.com/javase/8/docs/technotes/tools/findingclasses.html https://www.liaoxuefeng.com/wiki/1252599548343744/1260466914339296 ","date":"2023-04-25T15:04:48Z","permalink":"https://blog.yorforger.cc/p/what-is-jar-package-in-java/","title":"What is Jar Package in Java"},{"content":" # PATH You should set the PATH variable if you want to be able to run the executables (javac, java, javadoc, and so on) from any directory without having to type the full path of the command. If you do not set the PATH variable, you need to specify the full path to the executable every time you run it, such as:\n1 /usr/local/jdk1.7.0/bin/javac MyClass.java 怎么设置PATH呢, 也就是怎么添加环境变量呢? 编辑~/.bashrc或者.zshrc, 看你用的什么shell, 默认的是bash, 我用的是zsh\n1 export PATH=$PATH:/place/with/the/file 其实这个很容易看出什么意思, PATH = $PATH + /place/with/the/file, 句子里面的分号就是个separator而已. 比如下面的输出, 一看就明白了:\n1 2 echo $PATH /opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin On most systems (Linux, Mac OS, UNIX, etc) the colon character (:) is the classpath separator. In windowsm the separator is the semicolon (;)\n其实PATH就是告诉terminal可执行指令的位置信息, 而CLASSPATH是用来告诉JRE相关程序 用户自定义类 的位置信息\n# CLASSPATH 阅读此节前, 可先参考: 手动编译运行Java程序之JVM加载类的顺序\nSimilar to the classic dynamic loading behavior, when executing Java programs, the Java Virtual Machine finds and loads classes lazily (it loads the bytecode of a class only when the class is first used). The classpath tells Java where to look in the filesystem for files defining these classes. - Wiki\n注意: 从 Java5 开始 CLASSPATH 默认就是当前路径, 一般情况下就不需要再设定了,\n下面我们就介绍一下 CLASSPATH 的用, 以便对CLASSPATH有更加深入的理解: 因为Java是编译型语言，源码文件是.java，而编译后的.class文件才是真正可以被JVM执行的字节码。因此，如果要加载一个abc.xyz.Hello的类，JVM需要知道应该去哪搜索对应的Hello.class文件。所以，classpath就是一组目录的集合，它设置的搜索路径与操作系统相关。例如，在Windows系统上，用;分隔，带空格的目录用\u0026quot;\u0026quot;括起来，可能长这样：\n1 C:\\work\\project1\\bin;C:\\shared;\u0026#34;D:\\My Documents\\project1\\bin\u0026#34; 在Linux系统上，用:分隔，可能长这样：\n1 /usr/shared:/usr/local/bin:/home/liaoxuefeng/bin 现在我们假设classpath是.;C:\\work\\project1\\bin;C:\\shared，当JVM在加载abc.xyz.Hello这个类时，会依次查找：\n\u0026lt;当前目录\u0026gt;\\abc\\xyz\\Hello.class C:\\work\\project1\\bin\\abc\\xyz\\Hello.class C:\\shared\\abc\\xyz\\Hello.class 注意到.代表当前目录。如果JVM在某个路径下找到了对应的class文件，就不再往后继续搜索。如果所有路径下都没有找到，就报错。\n我们强烈不推荐在系统环境变量中设置classpath，那样会污染整个系统环境。在启动JVM时设置classpath才是推荐的做法。实际上就是给java命令传入-classpath或-cp参数：\n1 java -cp .;C:\\work\\project1\\bin;C:\\shared abc.xyz.Hello 没有设置系统环境变量classpath也没有传入-cp参数时，那么JVM默认的classpath为.，即当前目录.\n1 java abc.xyz.Hello 上述命令告诉JVM搜索class文件路径为:./abc/xyz/Hello.class.\n例子, 比如我写个Cat类, 在包com.zhu.servlet, 编译运行指令如下:\n1 2 3 4 5 6 7 package com.zhu.servlet; public class Cat { public static void main(String[] args) { System.out.println(\u0026#34;hello world!\u0026#34;); } } 1 2 3 4 5 6 7 8 javac com/zhu/servlet/Cat.java java Cat Error: Could not find or load main class Cat Caused by: java.lang.ClassNotFoundException: Cat java com.zhu.servlet.Cat hello world! 所以这也说明了类的全名应该是包名+类名, 而不只是一个简单的类名.\nYou can check value of classpath in java inside your application by looking at following system property “java.class.path”:\n1 System.getProperty(\u0026#34;java.class.path\u0026#34;) # Set Classpath 1 export CLASSPATH=$PATH:/home/myaccount/myproject/lib/CoolFramework.jar:/home/myaccount/myproject/output/ 这只是举个例子, 更好的做法就是用第二种方法, 就是启动JVM的通过-cp来指定classpath, 不要在环境变量中设置classpath！默认的当前目录.对于绝大多数情况都够用了\n1 C:\\work\u0026gt; java -cp . com.example.Hello 参考:\nclasspath和jar - 廖雪峰的官方网站 ","date":"2023-04-25T14:53:30Z","permalink":"https://blog.yorforger.cc/p/classpath-path/","title":"CLASSPATH \u0026 PATH"},{"content":"在IDEA把webapp deploy到tomcat上的时候会看到以下设置:\n上图中选择war或者war exploded的时候, 如果选择前者, on frame deactivation diaglog就不会有update resources选项:\n有很多疑问比如war和war exploded是什么, update resources和update classes and resources的区别是什么, 这都是干啥的?\n# War \u0026amp; War exploded 在使用 IDEA 开发Java Web项目部署 Tomcat 的时候通常会出现下面的情况:\n是选择 war 还是 war exploded呢?\n看一下他们两个的区别:\nwar：将Web Application以包的形式上传到服务器 war exploded：将Web Application以当前文件夹的位置关系上传到服务器, 因此这种方式支持hot deployment，一般在开发的时候也是用这种方式. Hot deployment is the process of adding new components (such as WAR files, EJB Jar files, enterprise Java beans, servlets, and JSP files) to a running server without having to stop the application server process and start it again.\n所以现在知道上面为啥选择war的话, 在on frame deactivation diaglog就不会有update resources选项了吧.\n# Update Resources \u0026amp; Update Classes and Resources On Upate Action : update classes and resources 更新代码和资源 On Frame Deactivation : update classes and resources在IDE失去焦点时(你点开浏览器离开IDE的时候)更新并发布代码 如果On Upate Action选择了update classes and resources，然后On Frame Deactivation 选择了do nothing, 那你无论是修改了servlet, doGet等动态代码还是jsp，h5等静态资源代码，需要手动更新, 就是你得自己点击那个更新按钮, 然后再刷新浏览器页面, 你的修改才能生效:\n如果On Upate Action和On Frame Deactivation都选择了update classes and resources，那就是每次修改了servlet代码或者jsp等静态代码后你都不用点击那个更新按钮了, 直接进入浏览器刷新页面就行,这样显然会浪费电脑资源,如果你不心疼cpu, 那就这样最好, 我是心疼, 所以我选择的如下:\n这样每次修改了servlet之后我点击更新按钮, 修改了jsp之后我就不用点击了, 直接进入浏览器刷新页面就可以了.\n参考:\nhttps://blog.csdn.net/u013626215/article/details/103685304 ","date":"2023-04-25T14:27:35Z","permalink":"https://blog.yorforger.cc/p/idea%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%9A%84war%E5%92%8Cwar-exploded%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"IDEA项目中的War和War Exploded的区别"},{"content":" # 什么是Spring Boot Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can \u0026ldquo;just run\u0026rdquo;. Spring Boot\nSpring Boot是一个基于Spring的套件，它帮我们预组装了Spring的一系列组件，以便以尽可能少的代码和配置来开发基于Spring的Java应用程序。原文\n即 Spring Boot 是用来方便管理 Spring 相关组件的一个东西, 所以并不是说学了Spring Boot就不用学Spring, Spring Boot里面的东西就是Spring的一个个部件, 学Spring Boot的时候也是在学Spring。\n可以看下 Spring Boot 的maven配置文件(pom.xml)的内容，可能会帮助理解Spring Boot负责组装部件的本质：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.5\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.choo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;SpringDemo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;SpringDemo\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;SpringDemo\u0026lt;/description\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;17\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.32\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.5.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; # 基于 Spring Framework 的 Spring MVC 项目搭建 由于 Spring 主要应用于 Web 开发，下面看下 Spring Boot 出现前是如何搭建 Spring MVC 项目的。\nSpring MVC 项目需要引入 spring-webmvc 模块的依赖，因此首先要找的就是 spring-webmvc 的坐标，对于新手来说一般就是在网上找一些 Spring MVC 的入门文章，直接复制 spring-webmvc 的坐标了，此外就是在 maven 仓库 中根据关键字搜索。不管怎样找坐标吧，最终我们配置最简单的 pom.xml 内容如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.zzuhkp\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mvc-demo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;war\u0026lt;/packaging\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-webmvc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.2.6.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;javax.servlet-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.0.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; 除了引入 spring-webmvc 的依赖，由于我们还可能会使用到一些 Servlet 规范中的一些类，我们还引入了 Servlet 的依赖。而依赖引入只是万里长征的第一步，由于 Java Web 开发中的接口都是 Servlet 提供的，我们还需要配置 spring-webmvc 模块提供的 Servlet 接口的实现 DispatcherServlet。这又是什么东西？当时作为新手我的也是一脸懵逼，配置时我还得找到这个类的全限定名，这对于新手来说也太不友好了，又是一波复制粘贴。最终配置出来的 web.xml 文件内容如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;web-app xmlns=\u0026#34;http://xmlns.jcp.org/xml/ns/javaee\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\u0026#34; version=\u0026#34;4.0\u0026#34;\u0026gt; \u0026lt;servlet\u0026gt; \u0026lt;servlet-name\u0026gt;dispatcher\u0026lt;/servlet-name\u0026gt; \u0026lt;servlet-class\u0026gt;org.springframework.web.servlet.DispatcherServlet\u0026lt;/servlet-class\u0026gt; \u0026lt;/servlet\u0026gt; \u0026lt;servlet-mapping\u0026gt; \u0026lt;servlet-name\u0026gt;dispatcher\u0026lt;/servlet-name\u0026gt; \u0026lt;url-pattern\u0026gt;/\u0026lt;/url-pattern\u0026gt; \u0026lt;/servlet-mapping\u0026gt; \u0026lt;/web-app\u0026gt; 到这里就完了吗？显然不是，我们还没有为 Spring 配置 bean。对于 Spring MVC 来说，我们需要把 Spring 的配置文件放在 /WEB-INF/${servlet-name}-servlet.xml 中，其中 ${servlet-name} 为 Servelt 的名称，我们为 DispatcherServlet 取的名字是 dispatcher，因此我们需要创建 /WEB-INF/dispatcher-servlet.xml 作为配置文件。这对新手又是一个挑战，还得记住命名规范，那能不能自己指定配置文件位置呢？可以，配置一个 Servlet 的初始化参数 configurationLocation 指定配置文件，好吧，还得记住参数名称。真是令人崩溃，最后看下配置文件的内容吧。\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:mvc=\u0026#34;http://www.springframework.org/schema/mvc\u0026#34; xmlns:context=\u0026#34;http://www.springframework.org/schema/context\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc https://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd\u0026#34;\u0026gt; \u0026lt;bean class=\u0026#34;org.springframework.web.servlet.view.InternalResourceViewResolver\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;prefix\u0026#34; value=\u0026#34;/WEB-INF/page\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;suffix\u0026#34; value=\u0026#34;.jsp\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;context:component-scan base-package=\u0026#34;com.zzuhkp.mvc\u0026#34;/\u0026gt; \u0026lt;/beans\u0026gt; 最初的 Java Web 开发可没有前后端分离，为了根据视图名称查找到对应的 JSP 文件，我们配置一个 InternalResourceViewResolver 类型的视图解析器 bean，这是 Spring MVC 特有的一个 bean，又是一个新概念，视图解析器又是什么？此外，我们为了启用注解支持，我们添加了 context:compent-scan 标签指定了要 Spring 要扫描的包。bean 的配置还好，最要名的是 xml 配置的命名空间，就是最上面那一坨，除了复制粘贴谁能自己写出来呢？\n写到这里，我已经近乎崩溃了，简直又经历了一次那段痛苦的历史。来个 Controller 测试下请求是否正常。\n1 2 3 4 5 6 7 8 @Controller public class HelloController { @GetMapping(\u0026#34;/hello\u0026#34;) public String hello() { return \u0026#34;/hello\u0026#34;; } } Controller 方法 String 类型的返回值将作为视图名，这里我们指定的是 /hello，也就是说有一个 /hello 对应的 jsp 文件，当请求 /hello 时我们将这个文件的内容返回给前端，结合我们前面配置的 InternalResourceViewResolver，它的位置应该是 /WEB-INF/page/hello.jsp，我们定义的文件内容如下:\n1 2 3 4 5 6 7 8 9 \u0026lt;%@ page contentType=\u0026#34;text/html;charset=UTF-8\u0026#34; language=\u0026#34;java\u0026#34; %\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;hello\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Hello,Spring MVC \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 最后整个项目结构如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 . ├── pom.xml └── src └── main ├── java │ └── com │ └── zzuhkp │ └── mvc │ └── HelloController.java └── webapp ├── WEB-INF │ ├── dispatcher-servlet.xml │ └── web.xml └── page └── hello.jsp 原文:\nhttps://blog.csdn.net/zzuhkp/article/details/123518033 # 基于 Spring Boot 的 Spring MVC 项目搭建 总结基于 Spring Framework 的 Spring MVC 项目搭建有哪些问题呢？\n首先概念过多，新人需要关注 spring-webmvc 中的众多概念，如 DispatcherServlet、视图解析器 InternalResourceViewResolver。其次配置过多，新人需要关注配置文件命名规范、xml 配置文件命名空间 等。\n为了解决解决上述的问题，Spring Boot 遵循约定大于配置的开发原则，大大简化的 Spring 的配置。首先进行自动化配置，只要引入相关依赖就会自动进行一些默认的配置，其次如果默认的配置不满足要求还可以自定义配置覆盖默认的配置，大大降低了 Spring 应用上手的门槛。\n将上述示例改造成基于 Spring Boot 的 Spring MVC 项目，首先看下 pom 文件内容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.7.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.zzuhkp\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mvc-demo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.tomcat.embed\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tomcat-embed-jasper\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; 和上述基于 Spring Framwork 的 pom 文件相比，主要有4处不同。\n引入了一个名为 spring-boot-starter-parent 的 parent，这是 Spring Boot 为简化 pom 文件配置提供的一个模块，内部管理了很多依赖，我们的 pom 继承这个 parent 之后很多依赖就可以省略版本号，如下面的 spring-boot-starter-web。 打包方式由 war 改成 jar，Spring Boot 可内嵌 Servlet 容器，可直接使用 jar 包启动，因此无需打包为 war 再部署。 引入了 spring-boot-starter-web 依赖，这个依赖被称为 starter，spring-boot-starter 会引入一些本模块相关的依赖和自动化配置，spring-boot-starter-web 就内嵌了 Tomcat，并自动进行 Spring MVC 的配置，如 DispatcherServlet。 引入了 tomcat-embed-jasper 依赖，这个依赖的作用在于支持内嵌 Tomcat 解析 jsp。 Spring Boot 项目由于使用 jar 包启动，因此需要提供一个主类，我们定义的主类如下。 jar和war的区别:\nThese files are simply zipped files using the java jar tool. These files are created for different purposes. Here is the description of these files:\n.jar files: The .jar files contain libraries, resources and accessories files like property files. .war files: The war file contains the web application that can be deployed on any servlet/jsp container. The .war file contains jsp, html, javascript and other files necessary for the development of web applications. https://stackoverflow.com/a/5871102/16317008 Spring Boot 项目由于使用 jar 包启动，因此需要提供一个主类，我们定义的主类如下:\n1 2 3 4 5 6 @SpringBootApplication public class MvcApplication { public static void main(String[] args) { SpringApplication.run(MvcApplication.class, args); } } @SpringBootApplication 注解主要用于开启自动化配置，main 方法则用于启动 Spring 容器。至此一个 Spring Boot 项目其实已经搭建完成了，不再需要进行繁杂的 web.xml 配置及 Spring 配置。\n虽然引入 spring-boot-starter-web 之后自动进行了 Web 开发相关的配置，不过由于我们需要自定义 InternalResourceViewResolver 的使用的视图前缀和后缀，我们还需要进一步的配置。Spring Boot 支持将相关配置直接添加到 /application.properties，看下我们的配置内容。\n1 2 spring.mvc.view.prefix=/WEB-INF/page spring.mvc.view.suffix=.jsp 注意, 我们配置数据库连接也是在SpringDemo/src/mian/resources/application.properties文件\n1 2 3 4 spring.datasource.url=jdbc:mysql://${MYSQL_HOST:localhost}:3306/greenhouse spring.datasource.username=root spring.datasource.password=778899 spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver 是不是很简单，JSP 文件和 Controller 未做变动，仍使用前面示例的代码。看下现在的项目结构:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 . ├── pom.xml └── src └── main ├── java │ └── com │ └── zzuhkp │ └── mvc │ ├── HelloController.java │ └── MvcApplication.java ├── resources │ └── application.properties └── webapp └── WEB-INF └── page └── hello.jsp 总结 Spring Boot 简化应用创建的方式为：使用 spring-boot-starter-parent 管理依赖版本、使用 spring-boot-starter 自动化配置、支持用户自定义配置覆盖默认配置。\n# Spring Boot 是如何简化应用运行的？ 对于应用运行的简化，主要提现在内嵌 Servlet 容器，能够将我们的应用自动打成 jar 包启动。上面的示例是我们在 IDE 中运行的，为了打成 jar 包，我们需要引入一个 Spring Boot 专有的插件。\n1 2 3 4 5 6 7 8 \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 这个插件可以将 Spring Boot 项目依赖的所有 jar 包打包到一个 jar 包中，这个 jar 也被称为 fat jar。\n# 总结 Spring 官网将 Spring Boot 的核心特性总结为 6 点，在我们上述的例子中也基本有体现：\nCreate stand-alone Spring applications Embed Tomcat, Jetty or Undertow directly (no need to deploy WAR files) Provide opinionated \u0026lsquo;starter\u0026rsquo; dependencies to simplify your build configuration Automatically configure Spring and 3rd party libraries whenever possible Provide production-ready features such as metrics, health checks, and externalized configuration Absolutely no code generation and no requirement for XML configuration 原文:\nhttps://blog.csdn.net/zzuhkp/article/details/123518033 ","date":"2023-04-25T12:30:22Z","permalink":"https://blog.yorforger.cc/p/spring-boot%E4%B8%8Espring%E6%A1%86%E6%9E%B6%E7%9A%84%E5%AF%B9%E6%AF%94-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"Spring Boot与Spring框架的对比-阅读笔记"},{"content":" 1 2 ssh root@144.202.12.32 ssh: connect to host 144.202.12.32 port 22: Connection refused 查的一些博客说修改mac设置成允许远程连接(这其实是允许别人连接你的mac电脑比如ssh localhost), 还有修改服务器里的配置文件/etc/ssh/ssh_config, 22端口取消注释, 都没用,\n直接去服务器卸载ssh再重装就好了\n1 2 3 4 5 6 7 sudo yum remove openssh-server sudo yum install openssh-server # sudo systemctl stop sshd sudo systemctl start sshd # 查看状态 service sshd status 注意上面我们检查ssh服务是否打开输入的是 service sshd status, 并不是上面的ssh status, 在我的服务器上输入ssh status会显示Unit ssh.service could not be found.所以根据实际情况而定.\n","date":"2023-04-23T20:30:22Z","permalink":"https://blog.yorforger.cc/p/ssh%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E4%B8%BB%E6%9C%BA%E5%87%BA%E7%8E%B0%E7%9A%84connection-refused%E9%97%AE%E9%A2%98/","title":"SSH连接远程主机出现的Connection Refused问题"},{"content":" # 1. 安装Nginx和git (服务器端) # 1.1. Installing the Nginx Web Server 1 2 3 4 sudo yum install nginx sudo systemctl enable nginx sudo systemctl start nginx # 1.2. Adjusting Firewall Rules Run the following command to permanently enable HTTP connections on port 80:\n1 sudo firewall-cmd --permanent --add-service=http To verify that the http firewall service was added correctly, you can run:\n1 sudo firewall-cmd --permanent --list-all You’ll see output like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 public target: default icmp-block-inversion: no interfaces: sources: services: cockpit dhcpv6-client http ssh ports: protocols: masquerade: no forward-ports: source-ports: icmp-blocks: rich rules: To apply the changes, you’ll need to reload the firewall service:\n1 sudo firewall-cmd --reload # 2. 创建新的用户 git (服务端) 1 2 3 4 5 6 7 8 9 10 useradd git passwd git # 给git用户配置sudo权限 chmod 740 /etc/sudoers vim /etc/sudoers # 找到root ALL=(ALL) ALL，在它下方加入一行 git ALL=(ALL) ALL chmod 400 /etc/sudoers # 2.1. 添加公钥认证 1 2 3 4 5 6 7 8 9 10 su git cd mkdir -p ~/.ssh touch ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys chmod 700 ~/.ssh #把本地电脑ssh的公钥加进去 ~/.ssh/id_rsa.pub vim ~/.ssh/authorized_keys # 2.2. git-hooks 自动部署 注意, 现在服务器有两个用户git和root, 然后他们只有自己的home目录是独立的, 而根目录下的比如/etc/, /usr这都是他们两个用户共享的, 看下面的输出你就知道什么意思了,\n1 2 3 4 5 6 7 ls -a /home/git/ . .. .bash_logout .bash_profile .bashrc .ssh .viminfo # 因为我们是在用户git下访问的, 所以得用sudo才能访问root用户的文件 sudo ls -a /root/ . .bash_logout .bashrc .cshrc .ssh .viminfo .. .bash_profile .cache .pki .tcshrc 好了不解释了, 开始设置git-hook以及服务器文件相关配置:\n1 2 3 4 5 6 7 8 9 # 新建目录，这是git仓库的位置 sudo mkdir -p /var/repo # 这里是我们网站网页的位置 sudo mkdir -p /var/www/hexo # 创建个bare repository cd /var/repo sudo git init --bare blog.git sudo vim /var/repo/blog.git/hooks/post-update post-update的内容如下:\n1 2 #!/bin/bash git --work-tree=/var/www/hexo --git-dir=/var/repo/blog.git checkout -f 给post-update授权:\n1 2 3 4 5 6 cd /var/repo/blog.git/hooks/ sudo chown -R git:git /var/repo/ sudo chown -R git:git /var/www/hexo # 赋予其可执行权限 sudo chmod +x post-update # 3. 配置Nginx 1 2 cd /etc/nginx/conf.d/ sudo vim blog.conf blog.conf的内容如下(注意因为我为域名添加个A记录然后www作为HOSTNAME, 如果你的域名只有一个A记录然后HOSTNAME还是空, 那你就填二级域名就好了davidzhu.xyz这样, 不懂的话请看我的域名分级和DNS记录之何为www：\n1 2 3 4 5 6 server { listen 80 default_server; listen [::] default_server; server_name www.davidzhu.xyz; root /var/www/hexo; } 1 2 3 4 5 nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful nginx -s reload # 4. 在本地安装Hexo Installing Hexo is quite easy and only requires the following beforehand:\nNode.js (Should be at least Node.js 10.13, recommends 12.0 or higher) Git node -v查看是否安装了node # 4.1 Install Node.js # 4.1.1. Install nvm 1 2 3 4 5 wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.3/install.sh | bash source ~/.bashrc source ~/.zshrc bash is an sh-compatible command language interpreter that executes commands read from the standard input or from a file.\nWe strongly recommend using a Node version manager like nvm to install Node.js and npm. We do not recommend using a Node installer, since the Node installation process installs npm in a directory with local permissions and can cause permissions errors when you run npm packages globally.\n# 4.1.2. Install node 1 2 3 4 5 6 7 nvm install node # \u0026#34;node\u0026#34; is an alias for the latest version # uninstall, 19.8.1是node的版本 nvm uninstall 19.8.1 # 尽量使用16.0.0版本的, 不然会出现问题: node: /lib64/libm.so.6: version `GLIBC_2.27\u0026#39; not found (required by node) nvm install 16.0.0 install nvm Downloading and installing Node.js and npm # 4.2. 安装Hexo 安装hexo以及相关插件,\n1 sudo npm install hexo-cli hexo-server hexo-deployer-git -g # 5. 在本地配置hexo # 5.1. 初始化hexo 1 hexo init ~/blog # 5.2. 配置hexo # 5.2.1 设置主题和deploy 1 2 3 cd ~/blog git clone https://github.com/next-theme/hexo-theme-next themes/next 编辑_config.yml\n1 vi _config.yml 找到theme, 改为next, 顺便也改一下deploy设置, 找到对应内容改为如下:\n1 2 3 4 5 6 7 8 9 10 11 12 # Extensions ## Plugins: https://hexo.io/plugins/ ## Themes: https://hexo.io/themes/ theme: next # Deployment ## Docs: https://hexo.io/docs/one-command-deployment # 必须填ip地址 deploy: type: git repo: root@144.202.12.32:/var/repo/blog.git branch: master 在deploy之前需要在服务器的root用户加上本地电脑的公钥(~/.ssh/id_rsa.pub), 否则你没权限提交git, 和github同理, 我猜的,\n1 vi /root/.ssh/authorized_keys 1 2 3 4 5 6 7 8 # 清除缓存 hexo clean # 生成静态页面 hexo generate # 注意要使用这个命令 否则使用hexo deploy的时候会出现找不到git报错 npm install hexo-deployer-git --save # 将本地静态页面目录部署到云服务器 hexo deploy # 6. 修改git用户默认shell环境 1 2 3 vim /etc/passwd # 修改最后一行 # 将/bin/bash修改为/usr/bin/git-shell # 7. bug总结 首先, /Users/shaowen/blog/_config.yml里的内容, delopy那部分, 不可以填错, master分支是master, 不是main, 也可以是main但是现在没空研究具体怎么操作,\n其次浏览器出错说403 forbidden, 这种情况就去看看网页目录下有没有文件, 即我们在本地电脑的部署的hexo可能并没有成功的部署到服务器, 因为权限问题或者服务器的那个hooks脚本没有执行权限导致无法自动更新,\n你可以把hooks也就是/var/repo/blog.git/hooks/post-update里命令的复制出来在服务器执行一下,看看会不会有效果,\nhttps://blog.kisnows.com/2016/03/10/Hexo部署到VPS并启用HTTPS/ ","date":"2023-04-23T19:54:52Z","permalink":"https://blog.yorforger.cc/p/%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2%E5%88%B0%E6%9C%8D%E5%8A%A1%E5%99%A8-hexo/","title":"部署博客到服务器 - Hexo"},{"content":" # 1. Commands used in branch You can use git branch -h to check these commands\u0026rsquo; explanation.\nList branches:\ngit branch, local, equals to ls ./.git/refs/heads/ git branch -r, remote git branch -a, all you can add -v option, git branch -v -a Check which branch you are on:\ngit status Create branch:\ngit branch \u0026lt;name\u0026gt; Switch branch:\ngit switch \u0026lt;name\u0026gt;, only can switch to the branches on your local repository git checkout \u0026lt;name\u0026gt; Delete branch:\ndelete fully merged branch: git branch -d \u0026lt;name\u0026gt; delete branch (even if not merged): git branch -D \u0026lt;name\u0026gt; delete a fetched branch locally: git branch -r -d \u0026lt;name\u0026gt;, e.g., git branch -r -d origin/master Merge benach issue003 into current branch：\ngit merge \u0026lt;issue003\u0026gt; Rename Current Branch\nmove/rename a branch and its reflog: git branch -m \u0026lt;branch-name\u0026gt; move/rename a branch, even if target exists: git branch -M \u0026lt;branch-name\u0026gt; # 2. 切换分支需要注意的事 # 2.1. 在本地修改远程对应分支 想要 push 到远程某个分支, 就必须在本地对应的分支修改文件, 否则会导致 push 到错误的分支,\n错误例子: 有两个分支main和backup, 我想要把文件 push 到 backup 分支上以用于备份, 可是我却每次在本地的 main 分支编辑博客, 导致往远程分支 origin/backup push 的时候说 everything is up-to-date. 然后到github看是否备份, 发现并没有备份, 就出现了这种摸不清头绪的问题.\n# 2.2. 新建分支必须做一次 commit 创建的分支后必须在该分支下做一次commit, 分支创建才会生效,\n如创建并转到分支 backup\n1 git switch -c backup 若没做任何 commit 就转到分支 master, 则分支 branch 并没有成功创建, 此时从 master 分支转到 backup 分支, 会报错fatal: invalid reference: backup,\n# 2.3. 切换分支前确保已经做了 commit 正常情况下在一个分支 A 做一些修改或者新建文件, commit 后再切换到分支B, 在分支 B 无法看到刚在分支 A 做的修改,\n如果在切换分之前没有commit, 即使你在某分支新建文件或者修改(但没commit), 跳到其他分支后仍可以看到你在那个分支做的修改, 所以 在切换分支前一定要确保你已经做了commit.\n还有一种特殊情况, 比如在master分支创建文件a.txt, 然后commit, 此时若新建一个分支 B, 则在分支 B 可以看到 master 分支所有的文件, 这是因为 B 分支还没被创建 文件 a.txt 就已经存在了, 即新建分支的内容是在原有分支内容的基础上产生的, 如果现在 在master分支创建文件 test.txt, 然后commit, 再新建个分 支dev, 这时候 dev 可以看到master分支所有的文件 a.txt, test.txt, 但已存在的分支 B 看不到 test.txt.\n有时切换分支会出现错误:\n1 2 3 4 5 6 7 8 9 10 11 $ git switch main error: Your local changes to the following files would be overwritten by checkout: content/posts/Merge\u0026amp;Rebase.md Please commit your changes or stash them before you switch branches. Aborting #-------------------------# $ git switch master error: The following untracked working tree files would be overwritten by checkout: .DS_Store Please move or remove them before you switch branches. Aborting 所以结论是, 切换分支前, 一定要记得commit, 别在A分支修改你想提交到B分支的文件.\n了解 stash: git-stash Documentation\n","date":"2023-04-22T00:47:50Z","permalink":"https://blog.yorforger.cc/p/git-branch/","title":"git branch"},{"content":" # 1. git fetch The git fetch command downloads commits, files, and refs from a remote repository into your local repo. Fetching is what you do when you want to see what everybody else has been working on. Git isolates fetched content from existing local content; it has absolutely no effect on your local development work.\nFetch all of the branches from the repository. This also downloads all of the required commits and files from the other repository.\n1 $ git fetch \u0026lt;remote\u0026gt; Same as the above command, but only fetch the specified branch.\n1 2 # e.g., git fetch origin master $ git fetch \u0026lt;remote\u0026gt; \u0026lt;branch\u0026gt; # 2. What is origin \u0026lt;remote\u0026gt; here is the url of the remote repository, we usually set an alias (often set as origin) so that we can refer it conveniently. Check here, we set the alias when specify the remote repository to local:\n1 2 # means: origin=git@github.com:shwezhu/MyProject.git git remote add origin git@github.com:shwezhu/your-repo.git You can set the alias to other name like this:\n1 git remote add my-repo-github git@github.com:shwezhu/your-repo.git But you have to use my-repo-github when you want to refer that url:\n1 2 3 # equals to: git fetch git@github.com:shwezhu/your-repo.git your-branch git push my-repo-github your-branch git fetch my-repo-github your-branch You can change the alias:\n1 git remote rename repo origin # 3. git remote command The git remote command is also a convenience or \u0026lsquo;helper\u0026rsquo; method for modifying a repo\u0026rsquo;s ./.git/config file. The commands presented below let you manage connections with other repositories. The following commands will modify the repo\u0026rsquo;s /.git/config file. The result of the following commands can also be achieved by directly editing the ./.git/config file with a text editor.\n1 git remote add \u0026lt;name\u0026gt; \u0026lt;url\u0026gt; Create a new connection to a remote repository. After adding a remote, you’ll be able to use ＜name＞ as a convenient shortcut for ＜url＞ in other Git commands.\nReferences:\nGit Remote | Atlassian Git Tutorial Git Fetch | Atlassian Git Tutorial ","date":"2023-04-21T21:46:44Z","permalink":"https://blog.yorforger.cc/p/git-fetch-the-essence-of-origin/","title":"git fetch - the Essence of \"origin\""}]